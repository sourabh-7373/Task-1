interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.25.2
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100
  response:
    content: '{"offset": 0, "next": 100, "data": [{"contexts": [], "intents": [],
      "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "598b65b8f514304abd34b7e14296559388b20f25", "externalIds": {"DBLP": "journals/corr/abs-2208-04425",
      "ArXiv": "2208.04425", "DOI": "10.48550/arXiv.2208.04425", "CorpusId": 251442494},
      "corpusId": 251442494, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/598b65b8f514304abd34b7e14296559388b20f25",
      "title": "Controlled Sparsity via Constrained Optimization or: How I Learned
      to Stop Tuning Penalties and Love Constraints", "abstract": "The performance
      of trained neural networks is robust to harsh levels of pruning. Coupled with
      the ever-growing size of deep learning models, this observation has motivated
      extensive research on learning sparse models. In this work, we focus on the
      task of controlling the level of sparsity when performing sparse learning. Existing
      methods based on sparsity-inducing penalties involve expensive trial-and-error
      tuning of the penalty factor, thus lacking direct control of the resulting model
      sparsity. In response, we adopt a constrained formulation: using the gate mechanism
      proposed by Louizos et al. (2018), we formulate a constrained optimization problem
      where sparsification is guided by the training objective and the desired sparsity
      target in an end-to-end fashion. Experiments on CIFAR-{10, 100}, TinyImageNet,
      and ImageNet using WideResNet and ResNet{18, 50} models validate the effectiveness
      of our proposal and demonstrate that we can reliably achieve pre-determined
      sparsity targets without compromising on predictive performance.", "venue":
      "Neural Information Processing Systems", "year": 2022, "referenceCount": 51,
      "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/2208.04425", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2022-08-08", "journal": {"volume": "abs/2208.04425",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Gallego-Posada2022ControlledSV,\n
      author = {Jose Gallego-Posada and Juan Ramirez and Akram Erraqabi and Y. Bengio
      and Simon Lacoste-Julien},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {Controlled Sparsity via Constrained Optimization
      or: How I Learned to Stop Tuning Penalties and Love Constraints},\n volume =
      {abs/2208.04425},\n year = {2022}\n}\n"}, "authors": [{"authorId": "1410596066",
      "name": "Jose Gallego-Posada"}, {"authorId": "2111835126", "name": "Juan Ramirez"},
      {"authorId": "3429013", "name": "Akram Erraqabi"}, {"authorId": "1865800402",
      "name": "Y. Bengio"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "6f9d72dd797a67d0a477502a509ab29d215761ae", "externalIds":
      {"DBLP": "conf/nips/OrvietoLL22", "ArXiv": "2205.04583", "CorpusId": 248665756},
      "corpusId": 248665756, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/6f9d72dd797a67d0a477502a509ab29d215761ae",
      "title": "Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants
      and Convergence to Exact Solution", "abstract": "Recently, Loizou et al. (2021),
      proposed and analyzed stochastic gradient descent (SGD) with stochastic Polyak
      stepsize (SPS). The proposed SPS comes with strong convergence guarantees and
      competitive performance; however, it has two main drawbacks when it is used
      in non-over-parameterized regimes: (i) It requires a priori knowledge of the
      optimal mini-batch losses, which are not available when the interpolation condition
      is not satisfied (e.g., regularized objectives), and (ii) it guarantees convergence
      only to a neighborhood of the solution. In this work, we study the dynamics
      and the convergence properties of SGD equipped with new variants of the stochastic
      Polyak stepsize and provide solutions to both drawbacks of the original SPS.
      We first show that a simple modification of the original SPS that uses lower
      bounds instead of the optimal function values can directly solve issue (i).
      On the other hand, solving issue (ii) turns out to be more challenging and leads
      us to valuable insights into the method''s behavior. We show that if interpolation
      is not satisfied, the correlation between SPS and stochastic gradients introduces
      a bias, which effectively distorts the expectation of the gradient signal near
      minimizers, leading to non-convergence - even if the stepsize is scaled down
      during training. To fix this issue, we propose DecSPS, a novel modification
      of SPS, which guarantees convergence to the exact minimizer - without a priori
      knowledge of the problem parameters. For strongly-convex optimization problems,
      DecSPS is the first stochastic adaptive optimization method that converges to
      the exact solution without restrictive assumptions like bounded iterates/gradients.",
      "venue": "Neural Information Processing Systems", "year": 2022, "referenceCount":
      40, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-09", "journal":
      null, "citationStyles": {"bibtex": "@Article{Orvieto2022DynamicsOS,\n author
      = {Antonio Orvieto and Simon Lacoste-Julien and Nicolas Loizou},\n booktitle
      = {Neural Information Processing Systems},\n title = {Dynamics of SGD with Stochastic
      Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution},\n
      year = {2022}\n}\n"}, "authors": [{"authorId": "51931942", "name": "Antonio
      Orvieto"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "1941365", "name": "Nicolas Loizou"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "56559a76e9804bfe907297299f8aee88a8a14d64",
      "externalIds": {"DBLP": "journals/corr/abs-2203-04940", "ArXiv": "2203.04940",
      "DOI": "10.48550/arXiv.2203.04940", "CorpusId": 247318780}, "corpusId": 247318780,
      "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural
      Information Processing Systems", "type": "conference", "alternate_names": ["Neural
      Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url":
      "https://www.semanticscholar.org/paper/56559a76e9804bfe907297299f8aee88a8a14d64",
      "title": "Data-Efficient Structured Pruning via Submodular Optimization", "abstract":
      "Structured pruning is an effective approach for compressing large pre-trained
      neural networks without significantly affecting their performance. However,
      most current structured pruning methods do not provide any performance guarantees,
      and often require fine-tuning, which makes them inapplicable in the limited-data
      regime. We propose a principled data-efficient structured pruning method based
      on submodular optimization. In particular, for a given layer, we select neurons/channels
      to prune and corresponding new weights for the next layer, that minimize the
      change in the next layer''s input induced by pruning. We show that this selection
      problem is a weakly submodular maximization problem, thus it can be provably
      approximated using an efficient greedy algorithm. Our method is guaranteed to
      have an exponentially decreasing error between the original model and the pruned
      model outputs w.r.t the pruned size, under reasonable assumptions. It is also
      one of the few methods in the literature that uses only a limited-number of
      training data and no labels. Our experimental results demonstrate that our method
      outperforms state-of-the-art methods in the limited-data regime.", "venue":
      "Neural Information Processing Systems", "year": 2022, "referenceCount": 58,
      "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf":
      {"url": "http://arxiv.org/pdf/2203.04940", "status": null}, "fieldsOfStudy":
      ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2022-03-09", "journal": {"volume": "abs/2203.04940",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Halabi2022DataEfficientSP,\n
      author = {Marwa El Halabi and Suraj Srinivas and Simon Lacoste-Julien},\n booktitle
      = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Data-Efficient
      Structured Pruning via Submodular Optimization},\n volume = {abs/2203.04940},\n
      year = {2022}\n}\n"}, "authors": [{"authorId": "2845064", "name": "Marwa El
      Halabi"}, {"authorId": "2822290", "name": "Suraj Srinivas"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "cdf4a982bf6dc373eb6463263ab5fd147c61c8ca",
      "externalIds": {"ArXiv": "2202.13903", "DBLP": "journals/corr/abs-2202-13903",
      "CorpusId": 247158659}, "corpusId": 247158659, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
      "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf Uncertain
      Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"}, "url":
      "https://www.semanticscholar.org/paper/cdf4a982bf6dc373eb6463263ab5fd147c61c8ca",
      "title": "Bayesian Structure Learning with Generative Flow Networks", "abstract":
      "In Bayesian structure learning, we are interested in inferring a distribution
      over the directed acyclic graph (DAG) structure of Bayesian networks, from data.
      Defining such a distribution is very challenging, due to the combinatorially
      large sample space, and approximations based on MCMC are often required. Recently,
      a novel class of probabilistic models, called Generative Flow Networks (GFlowNets),
      have been introduced as a general framework for generative modeling of discrete
      and composite objects, such as graphs. In this work, we propose to use a GFlowNet
      as an alternative to MCMC for approximating the posterior distribution over
      the structure of Bayesian networks, given a dataset of observations. Generating
      a sample DAG from this approximate distribution is viewed as a sequential decision
      problem, where the graph is constructed one edge at a time, based on learned
      transition probabilities. Through evaluation on both simulated and real data,
      we show that our approach, called DAG-GFlowNet, provides an accurate approximation
      of the posterior over DAGs, and it compares favorably against other methods
      based on MCMC or variational inference.", "venue": "Conference on Uncertainty
      in Artificial Intelligence", "year": 2022, "referenceCount": 62, "citationCount":
      73, "influentialCitationCount": 6, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2022-02-28", "journal": {"volume": "abs/2202.13903",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Deleu2022BayesianSL,\n
      author = {T. Deleu and Ant''onio G''ois and Chris C. Emezue and M. Rankawat
      and Simon Lacoste-Julien and Stefan Bauer and Y. Bengio},\n booktitle = {Conference
      on Uncertainty in Artificial Intelligence},\n journal = {ArXiv},\n title = {Bayesian
      Structure Learning with Generative Flow Networks},\n volume = {abs/2202.13903},\n
      year = {2022}\n}\n"}, "authors": [{"authorId": "7636193", "name": "T. Deleu"},
      {"authorId": "2060583060", "name": "Ant''onio G''ois"}, {"authorId": "1591176064",
      "name": "Chris C. Emezue"}, {"authorId": "9166330", "name": "M. Rankawat"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "153125952",
      "name": "Stefan Bauer"}, {"authorId": "1865800402", "name": "Y. Bengio"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "7a8493a6d744f621945457bd0f5428a399ba5333", "externalIds":
      {"DBLP": "conf/iclr/ZhangZLBS22", "ArXiv": "2111.12193", "CorpusId": 244527640},
      "corpusId": 244527640, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/7a8493a6d744f621945457bd0f5428a399ba5333",
      "title": "Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation",
      "abstract": "Most set prediction models in deep learning use set-equivariant
      operations, but they actually operate on multisets. We show that set-equivariant
      functions cannot represent certain functions on multisets, so we introduce the
      more appropriate notion of multiset-equivariance. We identify that the existing
      Deep Set Prediction Network (DSPN) can be multiset-equivariant without being
      hindered by set-equivariance and improve it with approximate implicit differentiation,
      allowing for better optimization while being faster and saving memory. In a
      range of toy experiments, we show that the perspective of multiset-equivariance
      is beneficial and that our changes to DSPN achieve better results in most cases.
      On CLEVR object property prediction, we substantially improve over the state-of-the-art
      Slot Attention from 8% to 77% in one of the strictest evaluation metrics because
      of the benefits made possible by implicit differentiation.", "venue": "International
      Conference on Learning Representations", "year": 2021, "referenceCount": 63,
      "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-11-23", "journal": {"volume": "abs/2111.12193",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Zhang2021MultisetEquivariantSP,\n
      author = {Yan Zhang and David W. Zhang and Simon Lacoste-Julien and G. Burghouts
      and Cees G. M. Snoek},\n booktitle = {International Conference on Learning Representations},\n
      journal = {ArXiv},\n title = {Multiset-Equivariant Set Prediction with Approximate
      Implicit Differentiation},\n volume = {abs/2111.12193},\n year = {2021}\n}\n"},
      "authors": [{"authorId": "2152821125", "name": "Yan Zhang"}, {"authorId": "2127383427",
      "name": "David W. Zhang"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "1909303", "name": "G. Burghouts"}, {"authorId": "145404204", "name":
      "Cees G. M. Snoek"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "6ad3586b9b1364de329101cae89f09712f510efc",
      "externalIds": {"DBLP": "conf/clear2/LachapelleRSEPL22", "ArXiv": "2107.10098",
      "CorpusId": 244400763}, "corpusId": 244400763, "publicationVenue": {"id": "3d07319c-4f2a-4f30-b619-c295ccd29367",
      "name": "CLEaR", "type": "conference", "alternate_names": ["Classification of
      Events, Activities and Relationships", "CLEAR", "CLeaR", "Conf Causal Learn
      Reason", "Classif Event Act Relatsh", "Conference on Causal Learning and Reasoning"],
      "issn": "2453-7128", "url": "http://www.jolace.com/publications/clear/", "alternate_urls":
      ["https://www.cclear.cc/"]}, "url": "https://www.semanticscholar.org/paper/6ad3586b9b1364de329101cae89f09712f510efc",
      "title": "Disentanglement via Mechanism Sparsity Regularization: A New Principle
      for Nonlinear ICA", "abstract": "This work introduces a novel principle we call
      disentanglement via mechanism sparsity regularization, which can be applied
      when the latent factors of interest depend sparsely on past latent factors and/or
      observed auxiliary variables. We propose a representation learning method that
      induces disentanglement by simultaneously learning the latent factors and the
      sparse causal graphical model that relates them. We develop a rigorous identifiability
      theory, building on recent nonlinear independent component analysis (ICA) results,
      that formalizes this principle and shows how the latent variables can be recovered
      up to permutation if one regularizes the latent mechanisms to be sparse and
      if some graph connectivity criterion is satisfied by the data generating process.
      As a special case of our framework, we show how one can leverage unknown-target
      interventions on the latent factors to disentangle them, thereby drawing further
      connections between ICA and causality. We propose a VAE-based method in which
      the latent mechanisms are learned and regularized via binary masks, and validate
      our theory by showing it learns disentangled representations in simulations.",
      "venue": "CLEaR", "year": 2021, "referenceCount": 52, "citationCount": 77, "influentialCitationCount":
      13, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2021-07-21", "journal": {"pages": "428-484"}, "citationStyles": {"bibtex":
      "@Article{Lachapelle2021DisentanglementVM,\n author = {S\u00e9bastien Lachapelle
      and Pau Rodr''iguez L''opez and Yash Sharma and K. Everett and R\u00e9mi Le
      Priol and Alexandre Lacoste and Simon Lacoste-Julien},\n booktitle = {CLEaR},\n
      pages = {428-484},\n title = {Disentanglement via Mechanism Sparsity Regularization:
      A New Principle for Nonlinear ICA},\n year = {2021}\n}\n"}, "authors": [{"authorId":
      "134730235", "name": "S\u00e9bastien Lachapelle"}, {"authorId": "2121428093",
      "name": "Pau Rodr''iguez L''opez"}, {"authorId": "49738125", "name": "Yash Sharma"},
      {"authorId": "98029815", "name": "K. Everett"}, {"authorId": "10712297", "name":
      "R\u00e9mi Le Priol"}, {"authorId": "8651990", "name": "Alexandre Lacoste"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "f91ca1bfe2c3f857b018948bb152c1d9a74b4018", "externalIds": {"DBLP":
      "journals/corr/abs-2107-00052", "ArXiv": "2107.00052", "CorpusId": 235694268},
      "corpusId": 235694268, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/f91ca1bfe2c3f857b018948bb152c1d9a74b4018",
      "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for
      Smooth Games: Convergence Analysis under Expected Co-coercivity", "abstract":
      "Two of the most prominent algorithms for solving unconstrained smooth games
      are the classical stochastic gradient descent-ascent (SGDA) and the recently
      introduced stochastic consensus optimization (SCO) [Mescheder et al., 2017].
      SGDA is known to converge to a stationary point for specific classes of games,
      but current convergence analyses require a bounded variance assumption. SCO
      is used successfully for solving large-scale adversarial problems, but its convergence
      guarantees are limited to its deterministic variant. In this work, we introduce
      the expected co-coercivity condition, explain its benefits, and provide the
      first last-iterate convergence guarantees of SGDA and SCO under this condition
      for solving a class of stochastic variational inequality problems that are potentially
      non-monotone. We prove linear convergence of both methods to a neighborhood
      of the solution when they use constant step-size, and we propose insightful
      stepsize-switching rules to guarantee convergence to the exact solution. In
      addition, our convergence guarantees hold under the arbitrary sampling paradigm,
      and as such, we give insights into the complexity of minibatching.", "venue":
      "Neural Information Processing Systems", "year": 2021, "referenceCount": 82,
      "citationCount": 34, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-06-30", "journal": {"pages": "19095-19108"},
      "citationStyles": {"bibtex": "@Article{Loizou2021StochasticGD,\n author = {Nicolas
      Loizou and Hugo Berard and Gauthier Gidel and Ioannis Mitliagkas and Simon Lacoste-Julien},\n
      booktitle = {Neural Information Processing Systems},\n pages = {19095-19108},\n
      title = {Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth
      Games: Convergence Analysis under Expected Co-coercivity},\n year = {2021}\n}\n"},
      "authors": [{"authorId": "1941365", "name": "Nicolas Loizou"}, {"authorId":
      "40201329", "name": "Hugo Berard"}, {"authorId": "8150760", "name": "Gauthier
      Gidel"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "90fb86330c9ad808c6b14b0fb05e82beefaa1701", "externalIds": {"ArXiv": "2105.11646",
      "DBLP": "journals/corr/abs-2105-11646", "CorpusId": 235187166}, "corpusId":
      235187166, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/90fb86330c9ad808c6b14b0fb05e82beefaa1701",
      "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling",
      "abstract": "Motivated by the needs from an airline crew scheduling application,
      we introduce structured convolutional kernel networks (Struct-CKN), which combine
      CKNs from Mairal et al. (2014) in a structured prediction framework that supports
      constraints on the outputs. CKNs are a particular kind of convolutional neural
      networks that approximate a kernel feature map on training data, thus combining
      properties of deep learning with the non-parametric flexibility of kernel methods.
      Extending CKNs to structured outputs allows us to obtain useful initial solutions
      on a flight-connection dataset that can be further refined by an airline crew
      scheduling solver. More specifically, we use a flight-based network modeled
      as a general conditional random field capable of incorporating local constraints
      in the learning process. Our experiments demonstrate that this approach yields
      significant improvements for the large-scale crew pairing problem (50,000 flights
      per month) over standard approaches, reducing the solution cost by 17% (a gain
      of millions of dollars) and the cost of global constraints by 97%.", "venue":
      "International Conference on Machine Learning", "year": 2021, "referenceCount":
      49, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2021-05-25", "journal": {"volume": "abs/2105.11646",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Yaakoubi2021StructuredCK,\n
      author = {Yassine Yaakoubi and F. Soumis and Simon Lacoste-Julien},\n booktitle
      = {International Conference on Machine Learning},\n journal = {ArXiv},\n title
      = {Structured Convolutional Kernel Networks for Airline Crew Scheduling},\n
      volume = {abs/2105.11646},\n year = {2021}\n}\n"}, "authors": [{"authorId":
      "2105163656", "name": "Yassine Yaakoubi"}, {"authorId": "1447920098", "name":
      "F. Soumis"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "b6c4a3f913ee819afa49e115f9739fe31ecf13f5", "externalIds":
      {"DBLP": "journals/corr/abs-2103-09027", "MAG": "3129012083", "ArXiv": "2103.09027",
      "CorpusId": 232240622}, "corpusId": 232240622, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/b6c4a3f913ee819afa49e115f9739fe31ecf13f5",
      "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning",
      "abstract": "Model-agnostic meta-learning (MAML) is a popular method for few-shot
      learning but assumes that we have access to the meta-training set. In practice,
      training on the meta-training set may not always be an option due to data privacy
      concerns, intellectual property issues, or merely lack of computing resources.
      In this paper, we consider the novel problem of repurposing pretrained MAML
      checkpoints to solve new few-shot classification tasks. Because of the potential
      distribution mismatch, the original MAML steps may no longer be optimal. Therefore
      we propose an alternative meta-testing procedure and combine MAML gradient steps
      with adversarial training and uncertainty-based stepsize adaptation. Our method
      outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using
      both SGD and Adam optimizers and shows improved robustness to the choice of
      base stepsize.", "venue": "International Conference on Learning Representations",
      "year": 2021, "referenceCount": 38, "citationCount": 4, "influentialCitationCount":
      0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-03-16", "journal": {"volume": "abs/2103.09027",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Kwon2021RepurposingPM,\n
      author = {Namyeong Kwon and Hwidong Na and Gabriel Huang and Simon Lacoste-Julien},\n
      booktitle = {International Conference on Learning Representations},\n journal
      = {ArXiv},\n title = {Repurposing Pretrained Models for Robust Out-of-domain
      Few-Shot Learning},\n volume = {abs/2103.09027},\n year = {2021}\n}\n"}, "authors":
      [{"authorId": "2060921177", "name": "Namyeong Kwon"}, {"authorId": "2534108",
      "name": "Hwidong Na"}, {"authorId": "24040986", "name": "Gabriel Huang"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "c02ade047261fb3a199219ae1274d70529d0a26c", "externalIds": {"DBLP": "conf/iclr/MladenovicBBHLV22",
      "ArXiv": "2103.02014", "CorpusId": 232105154}, "corpusId": 232105154, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/c02ade047261fb3a199219ae1274d70529d0a26c",
      "title": "Online Adversarial Attacks", "abstract": "Adversarial attacks expose
      important vulnerabilities of deep learning models, yet little attention has
      been paid to settings where data arrives as a stream. In this paper, we formalize
      the online adversarial attack problem, emphasizing two key elements found in
      real-world use-cases: attackers must operate under partial knowledge of the
      target model, and the decisions made by the attacker are irrevocable since they
      operate on a transient data stream. We first rigorously analyze a deterministic
      variant of the online threat model by drawing parallels to the well-studied
      $k$-secretary problem in theoretical computer science and propose Virtual+,
      a simple yet practical online algorithm. Our main theoretical result shows Virtual+
      yields provably the best competitive ratio over all single-threshold algorithms
      for $k<5$ -- extending the previous analysis of the $k$-secretary problem. We
      also introduce the \\textit{stochastic $k$-secretary} -- effectively reducing
      online blackbox transfer attacks to a $k$-secretary problem under noise -- and
      prove theoretical bounds on the performance of Virtual+ adapted to this setting.
      Finally, we complement our theoretical results by conducting experiments on
      MNIST, CIFAR-10, and Imagenet classifiers, revealing the necessity of online
      algorithms in achieving near-optimal performance and also the rich interplay
      between attack strategies and online attack selection, enabling simple strategies
      like FGSM to outperform stronger adversaries.", "venue": "International Conference
      on Learning Representations", "year": 2021, "referenceCount": 47, "citationCount":
      9, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2021-03-02", "journal": {"volume": "abs/2103.02014", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Mladenovic2021OnlineAA,\n author = {Andjela Mladenovic
      and A. Bose and Hugo Berard and William L. Hamilton and Simon Lacoste-Julien
      and Pascal Vincent and Gauthier Gidel},\n booktitle = {International Conference
      on Learning Representations},\n journal = {ArXiv},\n title = {Online Adversarial
      Attacks},\n volume = {abs/2103.02014},\n year = {2021}\n}\n"}, "authors": [{"authorId":
      "1643966780", "name": "Andjela Mladenovic"}, {"authorId": "26418299", "name":
      "A. Bose"}, {"authorId": "40201329", "name": "Hugo Berard"}, {"authorId": "49437682",
      "name": "William L. Hamilton"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "120247189", "name": "Pascal Vincent"}, {"authorId": "8150760",
      "name": "Gauthier Gidel"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "96e2e3f54b76992e95a447d50c0ac082e6f690b7",
      "externalIds": {"DBLP": "conf/icml/KerdreuxLLS21", "MAG": "3102110737", "ArXiv":
      "2011.03351", "CorpusId": 226278300}, "corpusId": 226278300, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/96e2e3f54b76992e95a447d50c0ac082e6f690b7",
      "title": "Affine Invariant Analysis of Frank-Wolfe on Strongly Convex Sets",
      "abstract": "It is known that the Frank-Wolfe (FW) algorithm, which is affine-covariant,
      enjoys accelerated convergence rates when the constraint set is strongly convex.
      However, these results rely on norm-dependent assumptions, usually incurring
      non-affine invariant bounds, in contradiction with FW''s affine-covariant property.
      In this work, we introduce new structural assumptions on the problem (such as
      the directional smoothness) and derive an affine invariant, norm-independent
      analysis of Frank-Wolfe. Based on our analysis, we propose an affine invariant
      backtracking line-search. Interestingly, we show that typical backtracking line-searches
      using smoothness of the objective function surprisingly converge to an affine
      invariant step size, despite using affine-dependent norms in the step size''s
      computation. This indicates that we do not necessarily need to know the set''s
      structure in advance to enjoy the affine-invariant accelerated rate.", "venue":
      "International Conference on Machine Learning", "year": 2020, "referenceCount":
      56, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-06",
      "journal": {"pages": "5398-5408"}, "citationStyles": {"bibtex": "@Article{Kerdreux2020AffineIA,\n
      author = {Thomas Kerdreux and Lewis Liu and Simon Lacoste-Julien and Damien
      Scieur},\n booktitle = {International Conference on Machine Learning},\n pages
      = {5398-5408},\n title = {Affine Invariant Analysis of Frank-Wolfe on Strongly
      Convex Sets},\n year = {2020}\n}\n"}, "authors": [{"authorId": "40900821", "name":
      "Thomas Kerdreux"}, {"authorId": "2145377749", "name": "Lewis Liu"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "7665349", "name":
      "Damien Scieur"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "de41c249161092e5f556ccf977611159f085670e",
      "externalIds": {"DBLP": "conf/aistats/BaratinGLHLVL21", "MAG": "3095344139",
      "CorpusId": 225197372}, "corpusId": 225197372, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/de41c249161092e5f556ccf977611159f085670e",
      "title": "Implicit Regularization via Neural Feature Alignment", "abstract":
      "We approach the problem of implicit regularization in deep learning from a
      geometrical viewpoint. We highlight a regularization effect induced by a dynamical
      alignment of the neural tangent features introduced by Jacot et al, along a
      small number of task-relevant directions. This can be interpreted as a combined
      mechanism of feature selection and model compression. By extrapolating a new
      analysis of Rademacher complexity bounds for linear models, we motivate and
      study a heuristic complexity measure that captures this phenomenon, in terms
      of sequences of tangent kernel classes along the optimization paths.", "venue":
      "International Conference on Artificial Intelligence and Statistics", "year":
      2020, "referenceCount": 68, "citationCount": 41, "influentialCitationCount":
      8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-08-03", "journal": {"pages": "2269-2277"}, "citationStyles": {"bibtex":
      "@Article{Baratin2020ImplicitRV,\n author = {A. Baratin and Thomas George and
      C\u00e9sar Laurent and R. Devon Hjelm and Guillaume Lajoie and Pascal Vincent
      and Simon Lacoste-Julien},\n booktitle = {International Conference on Artificial
      Intelligence and Statistics},\n pages = {2269-2277},\n title = {Implicit Regularization
      via Neural Feature Alignment},\n year = {2020}\n}\n"}, "authors": [{"authorId":
      "14398916", "name": "A. Baratin"}, {"authorId": "49917441", "name": "Thomas
      George"}, {"authorId": "40201308", "name": "C\u00e9sar Laurent"}, {"authorId":
      "40482726", "name": "R. Devon Hjelm"}, {"authorId": "49921594", "name": "Guillaume
      Lajoie"}, {"authorId": "120247189", "name": "Pascal Vincent"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "97faf5e185a7c28e48573212ecfa06f8f83497f3", "externalIds": {"MAG": "3034975784",
      "DBLP": "conf/icml/LoizouBJVLM20", "ArXiv": "2007.04202", "CorpusId": 220403670},
      "corpusId": 220403670, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/97faf5e185a7c28e48573212ecfa06f8f83497f3",
      "title": "Stochastic Hamiltonian Gradient Methods for Smooth Games", "abstract":
      "The success of adversarial formulations in machine learning has brought renewed
      motivation for smooth games. In this work, we focus on the class of stochastic
      Hamiltonian methods and provide the first convergence guarantees for certain
      classes of stochastic smooth games. We propose a novel unbiased estimator for
      the stochastic Hamiltonian gradient descent (SHGD) and highlight its benefits.
      Using tools from the optimization literature we show that SHGD converges linearly
      to the neighbourhood of a stationary point. To guarantee convergence to the
      exact solution, we analyze SHGD with a decreasing step-size and we also present
      the first stochastic variance reduced Hamiltonian method. Our results provide
      the first global non-asymptotic last-iterate convergence guarantees for the
      class of stochastic unconstrained bilinear games and for the more general class
      of stochastic games that satisfy a \"sufficiently bilinear\" condition, notably
      including some non-convex non-concave problems. We supplement our analysis with
      experiments on stochastic bilinear and sufficiently bilinear games, where our
      theory is shown to be tight, and on simple adversarial machine learning formulations.",
      "venue": "International Conference on Machine Learning", "year": 2020, "referenceCount":
      77, "citationCount": 43, "influentialCitationCount": 3, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-08",
      "journal": {"volume": "abs/2007.04202", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Loizou2020StochasticHG,\n author = {Nicolas Loizou and
      Hugo Berard and Alexia Jolicoeur-Martineau and Pascal Vincent and Simon Lacoste-Julien
      and Ioannis Mitliagkas},\n booktitle = {International Conference on Machine
      Learning},\n journal = {ArXiv},\n title = {Stochastic Hamiltonian Gradient Methods
      for Smooth Games},\n volume = {abs/2007.04202},\n year = {2020}\n}\n"}, "authors":
      [{"authorId": "1941365", "name": "Nicolas Loizou"}, {"authorId": "40201329",
      "name": "Hugo Berard"}, {"authorId": "1401723615", "name": "Alexia Jolicoeur-Martineau"},
      {"authorId": "120247189", "name": "Pascal Vincent"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "0da0af5ed59661fe4f901cf330a851e95eac7ea4", "externalIds":
      {"MAG": "3103069071", "DBLP": "conf/nips/BrouillardLLLD20", "ArXiv": "2007.01754",
      "CorpusId": 220347136}, "corpusId": 220347136, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/0da0af5ed59661fe4f901cf330a851e95eac7ea4",
      "title": "Differentiable Causal Discovery from Interventional Data", "abstract":
      "Discovering causal relationships in data is a challenging task that involves
      solving a combinatorial problem for which the solution is not always identifiable.
      A new line of work reformulates the combinatorial problem as a continuous constrained
      optimization one, enabling the use of different powerful optimization techniques.
      However, methods based on this idea do not yet make use of interventional data,
      which can significantly alleviate identifiability issues. In this work, we propose
      a neural network-based method for this task that can leverage interventional
      data. We illustrate the flexibility of the continuous-constrained framework
      by taking advantage of expressive neural architectures such as normalizing flows.
      We show that our approach compares favorably to the state of the art in a variety
      of settings, including perfect and imperfect interventions for which the targeted
      nodes may even be unknown.", "venue": "Neural Information Processing Systems",
      "year": 2020, "referenceCount": 58, "citationCount": 113, "influentialCitationCount":
      31, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-07-03", "journal": {"volume": "abs/2007.01754", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Brouillard2020DifferentiableCD,\n author = {P. Brouillard
      and S\u00e9bastien Lachapelle and Alexandre Lacoste and Simon Lacoste-Julien
      and Alexandre Drouin},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {Differentiable Causal Discovery from Interventional
      Data},\n volume = {abs/2007.01754},\n year = {2020}\n}\n"}, "authors": [{"authorId":
      "23138044", "name": "P. Brouillard"}, {"authorId": "134730235", "name": "S\u00e9bastien
      Lachapelle"}, {"authorId": "8651990", "name": "Alexandre Lacoste"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "14757354", "name":
      "Alexandre Drouin"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "2a799582775ff5527de1c24167aef0a9a6bf614b",
      "externalIds": {"MAG": "3039130562", "DBLP": "conf/nips/BoseGBCVLH20", "ArXiv":
      "2007.00720", "CorpusId": 220301567}, "corpusId": 220301567, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/2a799582775ff5527de1c24167aef0a9a6bf614b",
      "title": "Adversarial Example Games", "abstract": "The existence of adversarial
      examples capable of fooling trained neural network classifiers calls for a much
      better understanding of possible attacks to guide the development of safeguards
      against them. This includes attack methods in the challenging non-interactive
      blackbox setting, where adversarial attacks are generated without any access,
      including queries, to the target model. Prior attacks in this setting have relied
      mainly on algorithmic innovations derived from empirical observations (e.g.,
      that momentum helps), lacking principled transferability guarantees. In this
      work, we provide a theoretical foundation for crafting transferable adversarial
      examples to entire hypothesis classes. We introduce Adversarial Example Games
      (AEG), a framework that models the crafting of adversarial examples as a min-max
      game between a generator of attacks and a classifier. AEG provides a new way
      to design adversarial examples by adversarially training a generator and a classifier
      from a given hypothesis class (e.g., architecture). We prove that this game
      has an equilibrium, and that the optimal generator is able to craft adversarial
      examples that can attack any classifier from the corresponding hypothesis class.
      We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming
      prior state-of-the-art approaches with an average relative improvement of $29.9\\%$
      and $47.2\\%$ against undefended and robust models (Table 2 & 3) respectively.",
      "venue": "Neural Information Processing Systems", "year": 2020, "referenceCount":
      81, "citationCount": 39, "influentialCitationCount": 8, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-07-01", "journal": {"volume": "abs/2007.00720", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Bose2020AdversarialEG,\n author = {A. Bose and Gauthier
      Gidel and Hugo Berrard and Andre Cianflone and Pascal Vincent and Simon Lacoste-Julien
      and William L. Hamilton},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {Adversarial Example Games},\n volume = {abs/2007.00720},\n
      year = {2020}\n}\n"}, "authors": [{"authorId": "26418299", "name": "A. Bose"},
      {"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId": "1784615280",
      "name": "Hugo Berrard"}, {"authorId": "35090018", "name": "Andre Cianflone"},
      {"authorId": "120247189", "name": "Pascal Vincent"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "49437682", "name": "William L.
      Hamilton"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "0c604c8a025ccc29f1a8cadbd58e3dacc041ed1c",
      "externalIds": {"DBLP": "conf/aistats/PriolBBL21", "ArXiv": "2005.09136", "MAG":
      "3026172438", "CorpusId": 218684433}, "corpusId": 218684433, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/0c604c8a025ccc29f1a8cadbd58e3dacc041ed1c",
      "title": "An Analysis of the Adaptation Speed of Causal Models", "abstract":
      "We consider the problem of discovering the causal process that generated a
      collection of datasets. We assume that all these datasets were generated by
      unknown sparse interventions on a structural causal model (SCM) $G$, that we
      want to identify. Recently, Bengio et al. (2020) argued that among all SCMs,
      $G$ is the fastest to adapt from one dataset to another, and proposed a meta-learning
      criterion to identify the causal direction in a two-variable SCM. While the
      experiments were promising, the theoretical justification was incomplete. Our
      contribution is a theoretical investigation of the adaptation speed of simple
      two-variable SCMs. We use convergence rates from stochastic optimization to
      justify that a relevant proxy for adaptation speed is distance in parameter
      space after intervention. Using this proxy, we show that the SCM with the correct
      causal direction is advantaged for categorical and normal cause-effect datasets
      when the intervention is on the cause variable. When the intervention is on
      the effect variable, we provide a more nuanced picture which highlights that
      the fastest-to-adapt heuristic is not always valid. Code to reproduce experiments
      is available at this https URL", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2020, "referenceCount": 40, "citationCount":
      12, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-05-18", "journal": {"pages": "775-783"},
      "citationStyles": {"bibtex": "@Article{Priol2020AnAO,\n author = {R\u00e9mi
      Le Priol and Reza Babanezhad Harikandeh and Yoshua Bengio and Simon Lacoste-Julien},\n
      booktitle = {International Conference on Artificial Intelligence and Statistics},\n
      pages = {775-783},\n title = {An Analysis of the Adaptation Speed of Causal
      Models},\n year = {2020}\n}\n"}, "authors": [{"authorId": "10712297", "name":
      "R\u00e9mi Le Priol"}, {"authorId": "101340781", "name": "Reza Babanezhad Harikandeh"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "bc2fc5f394e709c78226ebac91f6e956781d9ef9",
      "externalIds": {"DBLP": "conf/aistats/LoizouVLL21", "ArXiv": "2002.10542", "MAG":
      "3007118755", "CorpusId": 211296607}, "corpusId": 211296607, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/bc2fc5f394e709c78226ebac91f6e956781d9ef9",
      "title": "Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for
      Fast Convergence", "abstract": "We propose a stochastic variant of the classical
      Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although
      computing the Polyak step-size requires knowledge of the optimal function values,
      this information is readily available for typical modern machine learning applications.
      Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive
      choice for setting the learning rate for stochastic gradient descent (SGD).
      We provide theoretical convergence guarantees for SGD equipped with SPS in different
      settings, including strongly convex, convex and non-convex functions. Furthermore,
      our analysis results in novel convergence guarantees for SGD with a constant
      step-size. We show that SPS is particularly effective when training over-parameterized
      models capable of interpolating the training data. In this setting, we prove
      that SPS enables SGD to converge to the true solution at a fast rate without
      requiring the knowledge of any problem-dependent constants or additional computational
      overhead. We experimentally validate our theoretical results via extensive experiments
      on synthetic and real datasets. We demonstrate the strong performance of SGD
      with SPS compared to state-of-the-art optimization methods when training over-parameterized
      models.", "venue": "International Conference on Artificial Intelligence and
      Statistics", "year": 2020, "referenceCount": 71, "citationCount": 118, "influentialCitationCount":
      22, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-02-24", "journal": {"pages": "1306-1314"}, "citationStyles": {"bibtex":
      "@Article{Loizou2020StochasticPS,\n author = {Nicolas Loizou and Sharan Vaswani
      and I. Laradji and Simon Lacoste-Julien},\n booktitle = {International Conference
      on Artificial Intelligence and Statistics},\n pages = {1306-1314},\n title =
      {Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence},\n
      year = {2020}\n}\n"}, "authors": [{"authorId": "1941365", "name": "Nicolas Loizou"},
      {"authorId": "1711940", "name": "Sharan Vaswani"}, {"authorId": "3266173", "name":
      "I. Laradji"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "f8865cd38bb8a35f9c9cd965753518f609b2a8ff", "externalIds":
      {"DBLP": "journals/ejtl/YaakoubiSL20", "MAG": "3103502522", "ArXiv": "2010.00134",
      "DOI": "10.1016/j.ejtl.2020.100020", "CorpusId": 214631090}, "corpusId": 214631090,
      "publicationVenue": {"id": "05d542af-bda7-4c57-ab4c-66a64dae8618", "name": "EURO
      Journal on Transportation and Logistics", "type": "journal", "alternate_names":
      ["EURO J Transp Logist"], "issn": "2192-4376", "url": "https://www.springer.com/business+&+management/operations+research/journal/13676",
      "alternate_urls": ["https://link.springer.com/journal/13676"]}, "url": "https://www.semanticscholar.org/paper/f8865cd38bb8a35f9c9cd965753518f609b2a8ff",
      "title": "Machine Learning in Airline Crew Pairing to Construct Initial Clusters
      for Dynamic Constraint Aggregation", "abstract": null, "venue": "EURO Journal
      on Transportation and Logistics", "year": 2020, "referenceCount": 57, "citationCount":
      17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-02-01", "journal": {"volume": "abs/2010.00134",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Soumis2020MachineLI,\n
      author = {F. Soumis and Yassine Yaakoubi and Simon Lacoste-Julien},\n booktitle
      = {EURO Journal on Transportation and Logistics},\n journal = {ArXiv},\n title
      = {Machine Learning in Airline Crew Pairing to Construct Initial Clusters for
      Dynamic Constraint Aggregation},\n volume = {abs/2010.00134},\n year = {2020}\n}\n"},
      "authors": [{"authorId": "1781104", "name": "F. Soumis"}, {"authorId": null,
      "name": "Yassine Yaakoubi"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "1d80e1f63b9dd30cb59b44acfdc13ab368dcc53e", "externalIds":
      {"MAG": "2997244791", "ArXiv": "2001.00602", "DBLP": "journals/corr/abs-2001-00602",
      "CorpusId": 209832324}, "corpusId": 209832324, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/1d80e1f63b9dd30cb59b44acfdc13ab368dcc53e",
      "title": "Accelerating Smooth Games by Manipulating Spectral Shapes", "abstract":
      "We use matrix iteration theory to characterize acceleration in smooth games.
      We define the spectral shape of a family of games as the set containing all
      eigenvalues of the Jacobians of standard gradient dynamics in the family. Shapes
      restricted to the real line represent well-understood classes of problems, like
      minimization. Shapes spanning the complex plane capture the added numerical
      challenges in solving smooth games. In this framework, we describe gradient-based
      methods, such as extragradient, as transformations on the spectral shape. Using
      this perspective, we propose an optimal algorithm for bilinear games. For smooth
      and strongly monotone operators, we identify a continuum between convex minimization,
      where acceleration is possible using Polyak''s momentum, and the worst case
      where gradient descent is optimal. Finally, going beyond first-order methods,
      we propose an accelerated version of consensus optimization.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2020, "referenceCount":
      61, "citationCount": 44, "influentialCitationCount": 11, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2020-01-02", "journal":
      {"pages": "1705-1715"}, "citationStyles": {"bibtex": "@Article{Azizian2020AcceleratingSG,\n
      author = {Wa\u00efss Azizian and Damien Scieur and Ioannis Mitliagkas and Simon
      Lacoste-Julien and Gauthier Gidel},\n booktitle = {International Conference
      on Artificial Intelligence and Statistics},\n pages = {1705-1715},\n title =
      {Accelerating Smooth Games by Manipulating Spectral Shapes},\n year = {2020}\n}\n"},
      "authors": [{"authorId": "1469119618", "name": "Wa\u00efss Azizian"}, {"authorId":
      "7665349", "name": "Damien Scieur"}, {"authorId": "3168518", "name": "Ioannis
      Mitliagkas"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "8150760", "name": "Gauthier Gidel"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "11396aae4adff67fcb073627f60d0f6049471d7c",
      "externalIds": {"MAG": "2980003999", "DBLP": "conf/aistats/MengVL0L20", "ArXiv":
      "1910.04920", "CorpusId": 204402754}, "corpusId": 204402754, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/11396aae4adff67fcb073627f60d0f6049471d7c",
      "title": "Fast and Furious Convergence: Stochastic Second Order Methods under
      Interpolation", "abstract": "We consider stochastic second-order methods for
      minimizing smooth and strongly-convex functions under an interpolation condition
      satisfied by over-parameterized models. Under this condition, we show that the
      regularized subsampled Newton method (R-SSN) achieves global linear convergence
      with an adaptive step-size and a constant batch-size. By growing the batch size
      for both the subsampled gradient and Hessian, we show that R-SSN can converge
      at a quadratic rate in a local neighbourhood of the solution. We also show that
      R-SSN attains local linear convergence for the family of self-concordant functions.
      Furthermore, we analyze stochastic BFGS algorithms in the interpolation setting
      and prove their global linear convergence. We empirically evaluate stochastic
      L-BFGS and a \"Hessian-free\" implementation of R-SSN for binary classification
      on synthetic, linearly-separable datasets and real datasets under a kernel mapping.
      Our experimental results demonstrate the fast convergence of these methods,
      both in terms of the number of iterations and wall-clock time.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2019, "referenceCount":
      83, "citationCount": 28, "influentialCitationCount": 1, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-11", "journal":
      {"volume": "abs/1910.04920", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Meng2019FastAF,\n author = {S. Meng and Sharan Vaswani and I. Laradji
      and Mark W. Schmidt and Simon Lacoste-Julien},\n booktitle = {International
      Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n
      title = {Fast and Furious Convergence: Stochastic Second Order Methods under
      Interpolation},\n volume = {abs/1910.04920},\n year = {2019}\n}\n"}, "authors":
      [{"authorId": "118126099", "name": "S. Meng"}, {"authorId": "1711940", "name":
      "Sharan Vaswani"}, {"authorId": "3266173", "name": "I. Laradji"}, {"authorId":
      "145610994", "name": "Mark W. Schmidt"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "56364a9dc5d51a619ee589172b19dfddaa77ad68",
      "externalIds": {"MAG": "3037501197", "DBLP": "conf/aistats/Gallego-PosadaV20",
      "ArXiv": "1906.08325", "CorpusId": 208138262}, "corpusId": 208138262, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/56364a9dc5d51a619ee589172b19dfddaa77ad68",
      "title": "GAIT: A Geometric Approach to Information Theory", "abstract": "We
      advocate the use of a notion of entropy that reflects the relative abundances
      of the symbols in an alphabet, as well as the similarities between them. This
      concept was originally introduced in theoretical ecology to study the diversity
      of ecosystems. Based on this notion of entropy, we introduce geometry-aware
      counterparts for several concepts and theorems in information theory. Notably,
      our proposed divergence exhibits performance on par with state-of-the-art methods
      based on the Wasserstein distance, but enjoys a closed-form expression that
      can be computed efficiently. We demonstrate the versatility of our method via
      experiments on a broad range of domains: training generative models, computing
      image barycenters, approximating empirical measures and counting modes.", "venue":
      "International Conference on Artificial Intelligence and Statistics", "year":
      2019, "referenceCount": 44, "citationCount": 8, "influentialCitationCount":
      1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Mathematics", "source": "s2-fos-model"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-06-19", "journal": {"pages": "2601-2611"}, "citationStyles": {"bibtex":
      "@Article{Gallego-Posada2019GAITAG,\n author = {Jose Gallego-Posada and A. Vani
      and Max Schwarzer and Simon Lacoste-Julien},\n booktitle = {International Conference
      on Artificial Intelligence and Statistics},\n pages = {2601-2611},\n title =
      {GAIT: A Geometric Approach to Information Theory},\n year = {2019}\n}\n"},
      "authors": [{"authorId": "1410596066", "name": "Jose Gallego-Posada"}, {"authorId":
      "34360821", "name": "A. Vani"}, {"authorId": "51881243", "name": "Max Schwarzer"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "512ca25646d03f58ddce0231c5e36babdc0816f5", "externalIds": {"MAG":
      "2950041755", "DBLP": "journals/corr/abs-1906-05945", "ArXiv": "1906.05945",
      "CorpusId": 189897674}, "corpusId": 189897674, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/512ca25646d03f58ddce0231c5e36babdc0816f5",
      "title": "A Tight and Unified Analysis of Extragradient for a Whole Spectrum
      of Differentiable Games", "abstract": "We consider differentiable games: multi-objective
      minimization problems, where the goal is to find a Nash equilibrium. The machine
      learning community has recently started using extrapolation-based variants of
      the gradient method. A prime example is the extragradient, which yields linear
      convergence in cases like bilinear games, where the standard gradient method
      fails. The full benefits of extrapolation-based methods are not known: i) there
      is no unified analysis for a large class of games that includes both strongly
      monotone and bilinear games; ii) it is not known whether the rate achieved by
      extragradient can be improved, e.g. by considering multiple extrapolation steps.
      We answer these questions through new analysis of the extragradient''s local
      and global convergence properties. Our analysis covers the whole range of settings
      between purely bilinear and strongly monotone games. It reveals that extragradient
      converges via different mechanisms at these extremes; in between, it exploits
      the most favorable mechanism for the given problem. We then present lower bounds
      on the rate of convergence for a wide class of algorithms with any number of
      extrapolations. Our bounds prove that the extragradient achieves the optimal
      rate in this class, and that our upper bounds are tight. Our precise characterization
      of the extragradient''s convergence behavior in games shows that, unlike in
      convex optimization, the extragradient method may be much faster than the gradient
      method.", "venue": "arXiv.org", "year": 2019, "referenceCount": 49, "citationCount":
      24, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2019-06-13", "journal": {"volume": "abs/1906.05945", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Azizian2019ATA,\n author =
      {Wa\u00efss Azizian and Ioannis Mitliagkas and Simon Lacoste-Julien and Gauthier
      Gidel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Tight and
      Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games},\n
      volume = {abs/1906.05945},\n year = {2019}\n}\n"}, "authors": [{"authorId":
      "1469119618", "name": "Wa\u00efss Azizian"}, {"authorId": "3168518", "name":
      "Ioannis Mitliagkas"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "8150760", "name": "Gauthier Gidel"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "ec09db517aa4399ebbb33fb3acc342178f2aca90", "externalIds": {"DBLP": "journals/corr/abs-1906-04848",
      "MAG": "2995128076", "ArXiv": "1906.04848", "CorpusId": 186206649}, "corpusId":
      186206649, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/ec09db517aa4399ebbb33fb3acc342178f2aca90",
      "title": "A Closer Look at the Optimization Landscapes of Generative Adversarial
      Networks", "abstract": "Generative adversarial networks have been very successful
      in generative modeling, however they remain relatively challenging to train
      compared to standard deep neural networks. In this paper, we propose new visualization
      techniques for the optimization landscapes of GANs that enable us to study the
      game vector field resulting from the concatenation of the gradient of both players.
      Using these visualization techniques we try to bridge the gap between theory
      and practice by showing empirically that the training of GANs exhibits significant
      rotations around Local Stable Stationary Points (LSSP), similar to the one predicted
      by theory on toy examples. Moreover, we provide empirical evidence that GAN
      training converge to a stable stationary point which is a saddle point for the
      generator loss, not a minimum, while still achieving excellent performance.",
      "venue": "International Conference on Learning Representations", "year": 2019,
      "referenceCount": 42, "citationCount": 57, "influentialCitationCount": 5, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-06-11", "journal":
      {"volume": "abs/1906.04848", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Berard2019ACL,\n author = {Hugo Berard and Gauthier Gidel and Amjad
      Almahairi and Pascal Vincent and Simon Lacoste-Julien},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Closer
      Look at the Optimization Landscapes of Generative Adversarial Networks},\n volume
      = {abs/1906.04848},\n year = {2019}\n}\n"}, "authors": [{"authorId": "40201329",
      "name": "Hugo Berard"}, {"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId":
      "2634674", "name": "Amjad Almahairi"}, {"authorId": "145467703", "name": "Pascal
      Vincent"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "ae39e54c451897999032cde7d3e1c33139d7fdc3", "externalIds": {"MAG":
      "2948607799", "DBLP": "conf/iclr/LachapelleBDL20", "ArXiv": "1906.02226", "CorpusId":
      174802415}, "corpusId": 174802415, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/ae39e54c451897999032cde7d3e1c33139d7fdc3",
      "title": "Gradient-Based Neural DAG Learning", "abstract": "We propose a novel
      score-based approach to learning a directed acyclic graph (DAG) from observational
      data. We adapt a recently proposed continuous constrained optimization formulation
      to allow for nonlinear relationships between variables using neural networks.
      This extension allows to model complex interactions while avoiding the combinatorial
      nature of the problem. In addition to comparing our method to existing continuous
      optimization methods, we provide missing empirical comparisons to nonlinear
      greedy search methods. On both synthetic and real-world data sets, this new
      method outperforms current continuous methods on most tasks, while being competitive
      with existing greedy search methods on important metrics for causal inference.",
      "venue": "International Conference on Learning Representations", "year": 2019,
      "referenceCount": 47, "citationCount": 194, "influentialCitationCount": 42,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-06-05", "journal": {"volume": "abs/1906.02226", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Lachapelle2019GradientBasedND,\n author = {S\u00e9bastien
      Lachapelle and P. Brouillard and T. Deleu and Simon Lacoste-Julien},\n booktitle
      = {International Conference on Learning Representations},\n journal = {ArXiv},\n
      title = {Gradient-Based Neural DAG Learning},\n volume = {abs/1906.02226},\n
      year = {2019}\n}\n"}, "authors": [{"authorId": "134730235", "name": "S\u00e9bastien
      Lachapelle"}, {"authorId": "23138044", "name": "P. Brouillard"}, {"authorId":
      "7636193", "name": "T. Deleu"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "d32aa34a807560c94d5018e875a021813890447d", "externalIds":
      {"ArXiv": "1905.09997", "MAG": "2970028551", "DBLP": "conf/nips/VaswaniMLSGL19",
      "CorpusId": 165163603}, "corpusId": 165163603, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/d32aa34a807560c94d5018e875a021813890447d",
      "title": "Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence
      Rates", "abstract": "Recent works have shown that stochastic gradient descent
      (SGD) achieves the fast convergence rates of full-batch gradient descent for
      over-parameterized models satisfying certain interpolation conditions. However,
      the step-size used in these works depends on unknown quantities and SGD''s practical
      performance heavily relies on the choice of this step-size. We propose to use
      line-search techniques to automatically set the step-size when training models
      that can interpolate the data. In the interpolation setting, we prove that SGD
      with a stochastic variant of the classic Armijo line-search attains the deterministic
      convergence rates for both convex and strongly-convex functions. Under additional
      assumptions, SGD with Armijo line-search is shown to achieve fast convergence
      for non-convex functions. Furthermore, we show that stochastic extra-gradient
      with a Lipschitz line-search attains linear convergence for an important class
      of non-convex functions and saddle-point problems satisfying interpolation.
      To improve the proposed methods'' practical performance, we give heuristics
      to use larger step-sizes and acceleration. We compare the proposed algorithms
      against numerous optimization methods on standard classification tasks using
      both kernel methods and deep networks. The proposed methods result in competitive
      performance across all models and datasets, while being robust to the precise
      choices of hyper-parameters. For multi-class classification using deep networks,
      SGD with Armijo line-search results in both faster convergence and better generalization.",
      "venue": "Neural Information Processing Systems", "year": 2019, "referenceCount":
      97, "citationCount": 161, "influentialCitationCount": 24, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-24", "journal":
      {"pages": "3727-3740"}, "citationStyles": {"bibtex": "@Article{Vaswani2019PainlessSG,\n
      author = {Sharan Vaswani and Aaron Mishkin and I. Laradji and Mark W. Schmidt
      and Gauthier Gidel and Simon Lacoste-Julien},\n booktitle = {Neural Information
      Processing Systems},\n pages = {3727-3740},\n title = {Painless Stochastic Gradient:
      Interpolation, Line-Search, and Convergence Rates},\n year = {2019}\n}\n"},
      "authors": [{"authorId": "1711940", "name": "Sharan Vaswani"}, {"authorId":
      "21529944", "name": "Aaron Mishkin"}, {"authorId": "3266173", "name": "I. Laradji"},
      {"authorId": "145610994", "name": "Mark W. Schmidt"}, {"authorId": "8150760",
      "name": "Gauthier Gidel"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "c232afff29037ca4058f2b45e987cd12081bae0a", "externalIds":
      {"DBLP": "journals/corr/abs-1904-13262", "ArXiv": "1904.13262", "MAG": "2970166047",
      "CorpusId": 140225885}, "corpusId": 140225885, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/c232afff29037ca4058f2b45e987cd12081bae0a",
      "title": "Implicit Regularization of Discrete Gradient Dynamics in Deep Linear
      Neural Networks", "abstract": "When optimizing over-parameterized models, such
      as deep neural networks, a large set of parameters can achieve zero training
      error. In such cases, the choice of the optimization algorithm and its respective
      hyper-parameters introduces biases that will lead to convergence to specific
      minimizers of the objective. Consequently, this choice can be considered as
      an implicit regularization for the training of over-parametrized models. In
      this work, we push this idea further by studying the discrete gradient dynamics
      of the training of a two-layer linear network with the least-squares loss. Using
      a time rescaling, we show that, with a vanishing initialization and a small
      enough step size, this dynamics sequentially learns the solutions of a reduced-rank
      regression with a gradually increasing rank.", "venue": "Neural Information
      Processing Systems", "year": 2019, "referenceCount": 29, "citationCount": 120,
      "influentialCitationCount": 16, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2019-04-30", "journal": {"pages": "3196-3206"}, "citationStyles":
      {"bibtex": "@Article{Gidel2019ImplicitRO,\n author = {Gauthier Gidel and F.
      Bach and Simon Lacoste-Julien},\n booktitle = {Neural Information Processing
      Systems},\n pages = {3196-3206},\n title = {Implicit Regularization of Discrete
      Gradient Dynamics in Deep Linear Neural Networks},\n year = {2019}\n}\n"}, "authors":
      [{"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId": "144570279",
      "name": "F. Bach"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "db56581b2f21b78bf063d47a0f708aef915200e6", "externalIds":
      {"ArXiv": "1904.08598", "DBLP": "conf/nips/ChavdarovaGFL19", "MAG": "2939856750",
      "CorpusId": 120411570}, "corpusId": 120411570, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/db56581b2f21b78bf063d47a0f708aef915200e6",
      "title": "Reducing Noise in GAN Training with Variance Reduced Extragradient",
      "abstract": "We study the effect of the stochastic gradient noise on the training
      of generative adversarial networks (GANs) and show that it can prevent the convergence
      of standard game optimization methods, while the batch version converges. We
      address this issue with a novel stochastic variance-reduced extragradient (SVRE)
      optimization algorithm, which for a large class of games improves upon the previous
      convergence rates proposed in the literature. We observe empirically that SVRE
      performs similarly to a batch method on MNIST while being computationally cheaper,
      and that SVRE yields more stable GAN training on standard datasets.", "venue":
      "Neural Information Processing Systems", "year": 2019, "referenceCount": 51,
      "citationCount": 117, "influentialCitationCount": 16, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-04-18", "journal":
      {"volume": "abs/1904.08598", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Chavdarova2019ReducingNI,\n author = {Tatjana Chavdarova and Gauthier
      Gidel and F. Fleuret and Simon Lacoste-Julien},\n booktitle = {Neural Information
      Processing Systems},\n journal = {ArXiv},\n title = {Reducing Noise in GAN Training
      with Variance Reduced Extragradient},\n volume = {abs/1904.08598},\n year =
      {2019}\n}\n"}, "authors": [{"authorId": "2458542", "name": "Tatjana Chavdarova"},
      {"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId": "2721983", "name":
      "F. Fleuret"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "externalIds":
      {"DBLP": "journals/corr/abs-1812-04948", "MAG": "2904367110", "ArXiv": "1812.04948",
      "DOI": "10.1109/CVPR.2019.00453", "CorpusId": 54482423}, "corpusId": 54482423,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "We propose an alternative generator architecture for generative
      adversarial networks, borrowing from style transfer literature. The new architecture
      leads to an automatically learned, unsupervised separation of high-level attributes
      (e.g., pose and identity when trained on human faces) and stochastic variation
      in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific
      control of the synthesis. The new generator improves the state-of-the-art in
      terms of traditional distribution quality metrics, leads to demonstrably better
      interpolation properties, and also better disentangles the latent factors of
      variation. To quantify interpolation quality and disentanglement, we propose
      two new, automated methods that are applicable to any generator architecture.
      Finally, we introduce a new, highly varied and high-quality dataset of human
      faces.", "venue": "Computer Vision and Pattern Recognition", "year": 2018, "referenceCount":
      66, "citationCount": 7272, "influentialCitationCount": 1687, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1812.04948", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-12-12",
      "journal": {"pages": "4396-4405", "name": "2019 IEEE/CVF Conference on Computer
      Vision and Pattern Recognition (CVPR)"}, "citationStyles": {"bibtex": "@Article{Karras2018ASG,\n
      author = {Tero Karras and S. Laine and Timo Aila},\n booktitle = {Computer Vision
      and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer
      Vision and Pattern Recognition (CVPR)},\n pages = {4396-4405},\n title = {A
      Style-Based Generator Architecture for Generative Adversarial Networks},\n year
      = {2018}\n}\n"}, "authors": [{"authorId": "2976930", "name": "Tero Karras"},
      {"authorId": "36436218", "name": "S. Laine"}, {"authorId": "1761103", "name":
      "Timo Aila"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "52f982d781a5bd5675bd9d31d17ce64c5caba9d3",
      "externalIds": {"ArXiv": "1810.11544", "DBLP": "conf/nips/StruminskyLO18", "MAG":
      "2892001400", "CorpusId": 53105897}, "corpusId": 53105897, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/52f982d781a5bd5675bd9d31d17ce64c5caba9d3",
      "title": "Quantifying Learning Guarantees for Convex but Inconsistent Surrogates",
      "abstract": "We study consistency properties of machine learning methods based
      on minimizing convex surrogates. We extend the recent framework of Osokin et
      al. (2017) for the quantitative analysis of consistency properties to the case
      of inconsistent surrogates. Our key technical contribution consists in a new
      lower bound on the calibration function for the quadratic surrogate, which is
      non-trivial (not always zero) for inconsistent cases. The new bound allows to
      quantify the level of inconsistency of the setting and shows how learning with
      inconsistent surrogates can have guarantees on sample complexity and optimization
      difficulty. We apply our theory to two concrete cases: multi-class classification
      with the tree-structured loss and ranking with the mean average precision loss.
      The results show the approximation-computation trade-offs caused by inconsistent
      surrogates and their potential benefits.", "venue": "Neural Information Processing
      Systems", "year": 2018, "referenceCount": 25, "citationCount": 5, "influentialCitationCount":
      0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-10-01", "journal": {"pages": "667-675"}, "citationStyles": {"bibtex":
      "@Article{Struminsky2018QuantifyingLG,\n author = {Kirill Struminsky and Simon
      Lacoste-Julien and A. Osokin},\n booktitle = {Neural Information Processing
      Systems},\n pages = {667-675},\n title = {Quantifying Learning Guarantees for
      Convex but Inconsistent Surrogates},\n year = {2018}\n}\n"}, "authors": [{"authorId":
      "8546356", "name": "Kirill Struminsky"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "145319877", "name": "A. Osokin"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613", "externalIds":
      {"ArXiv": "1809.11096", "DBLP": "journals/corr/abs-1809-11096", "MAG": "2893749619",
      "CorpusId": 52889459}, "corpusId": 52889459, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
      "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
      "abstract": "Despite recent progress in generative image modeling, successfully
      generating high-resolution, diverse samples from complex datasets such as ImageNet
      remains an elusive goal. To this end, we train Generative Adversarial Networks
      at the largest scale yet attempted, and study the instabilities specific to
      such scale. We find that applying orthogonal regularization to the generator
      renders it amenable to a simple \"truncation trick,\" allowing fine control
      over the trade-off between sample fidelity and variety by reducing the variance
      of the Generator''s input. Our modifications lead to models which set the new
      state of the art in class-conditional image synthesis. When trained on ImageNet
      at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS)
      of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous
      best IS of 52.52 and FID of 18.6.", "venue": "International Conference on Learning
      Representations", "year": 2018, "referenceCount": 59, "citationCount": 4234,
      "influentialCitationCount": 625, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-09-27", "journal": {"volume": "abs/1809.11096",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Brock2018LargeSG,\n
      author = {Andrew Brock and Jeff Donahue and K. Simonyan},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {Large
      Scale GAN Training for High Fidelity Natural Image Synthesis},\n volume = {abs/1809.11096},\n
      year = {2018}\n}\n"}, "authors": [{"authorId": "2065040247", "name": "Andrew
      Brock"}, {"authorId": "7408951", "name": "Jeff Donahue"}, {"authorId": "34838386",
      "name": "K. Simonyan"}]}}, {"contexts": ["Teaching 2017-2023 (6) University
      of Montreal Advanced Structured Prediction and Optimization (IFT 6132) - grad
      class 2016-2022 (6) University of Montreal Probabilistic Graphical Model (IFT
      6269) - grad class 2014, 2015 \u00c9cole Normale Sup\u00e9rieure Statistical
      Machine Learning - 1st year master 2014, 2015 \u00c9cole Normale Sup\u00e9rieure
      de Cachan Programming Projects for Machine Learning - 2nd year master 2015 \u00c9cole
      Normale Sup\u00e9rieure de Cachan Probabilistic Graphical Models - 2nd year
      master 2006, 2008 University of California, Berkeley Practical Machine Learning
      - grad class"], "intents": ["background"], "contextsWithIntent": [{"context":
      "Teaching 2017-2023 (6) University of Montreal Advanced Structured Prediction
      and Optimization (IFT 6132) - grad class 2016-2022 (6) University of Montreal
      Probabilistic Graphical Model (IFT 6269) - grad class 2014, 2015 \u00c9cole
      Normale Sup\u00e9rieure Statistical Machine Learning - 1st year master 2014,
      2015 \u00c9cole Normale Sup\u00e9rieure de Cachan Programming Projects for Machine
      Learning - 2nd year master 2015 \u00c9cole Normale Sup\u00e9rieure de Cachan
      Probabilistic Graphical Models - 2nd year master 2006, 2008 University of California,
      Berkeley Practical Machine Learning - grad class", "intents": ["background"]}],
      "isInfluential": false, "citedPaper": {"paperId": "099c704bdbd6344bba4c1aefa4dea17f1d00c61c",
      "externalIds": {"DBLP": "journals/corr/abs-1809-06367", "MAG": "2884013187",
      "ArXiv": "1809.06367", "DOI": "10.1109/TPAMI.2018.2855738", "CorpusId": 206769416,
      "PubMed": "30028690"}, "corpusId": 206769416, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type":
      "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"], "issn":
      "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls": ["http://www.computer.org/portal/web/tpami",
      "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]}, "url": "https://www.semanticscholar.org/paper/099c704bdbd6344bba4c1aefa4dea17f1d00c61c",
      "title": "Scattering Networks for Hybrid Representation Learning", "abstract":
      "Scattering networks are a class of designed Convolutional Neural Networks (CNNs)
      with fixed weights. We argue they can serve as generic representations for modelling
      images. In particular, by working in scattering space, we achieve competitive
      results both for supervised and unsupervised learning tasks, while making progress
      towards constructing more interpretable CNNs. For supervised learning, we demonstrate
      that the early layers of CNNs do not necessarily need to be learned, and can
      be replaced with a scattering network instead. Indeed, using hybrid architectures,
      we achieve the best results with predefined representations to-date, while being
      competitive with end-to-end learned CNNs. Specifically, even applying a shallow
      cascade of small-windowed scattering coefficients followed by $1\\times 1$1\u00d71-convolutions
      results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover,
      by combining scattering networks with deep residual networks, we achieve a single-crop
      top-5 error of 11.4 percent on ILSVRC2012. Also, we show they can yield excellent
      performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding
      their end-to-end counterparts, through their ability to incorporate geometrical
      priors. For unsupervised learning, scattering coefficients can be a competitive
      representation that permits image recovery. We use this fact to train hybrid
      GANs to generate images. Finally, we empirically analyze several properties
      related to stability and reconstruction of images from scattering coefficients.",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year":
      2018, "referenceCount": 62, "citationCount": 67, "influentialCitationCount":
      8, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1809.06367",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Medicine", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-09-17", "journal":
      {"volume": "41", "pages": "2208-2221", "name": "IEEE Transactions on Pattern
      Analysis and Machine Intelligence"}, "citationStyles": {"bibtex": "@Article{Oyallon2018ScatteringNF,\n
      author = {Edouard Oyallon and Sergey Zagoruyko and Gabriel Huang and N. Komodakis
      and Simon Lacoste-Julien and Matthew B. Blaschko and Eugene Belilovsky},\n booktitle
      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal
      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages
      = {2208-2221},\n title = {Scattering Networks for Hybrid Representation Learning},\n
      volume = {41},\n year = {2018}\n}\n"}, "authors": [{"authorId": "3306593", "name":
      "Edouard Oyallon"}, {"authorId": "2134433", "name": "Sergey Zagoruyko"}, {"authorId":
      "24040986", "name": "Gabriel Huang"}, {"authorId": "2505902", "name": "N. Komodakis"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1758219",
      "name": "Matthew B. Blaschko"}, {"authorId": "1829344", "name": "Eugene Belilovsky"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "461afff92a4b51c6b64a05dde407270029fd19ec", "externalIds":
      {"ArXiv": "1807.11876", "DBLP": "journals/informs/LarsenLBFLL22", "MAG": "2914375891",
      "DOI": "10.1287/ijoc.2021.1091", "CorpusId": 59158767}, "corpusId": 59158767,
      "publicationVenue": {"id": "b76a26bb-18f2-4155-bb35-7e7de85d01bb", "name": "INFORMS
      journal on computing", "type": "journal", "alternate_names": ["INFORMS j comput",
      "Informs J Comput", "Informs Journal on Computing"], "issn": "1091-9856", "url":
      "https://www.informs.org/", "alternate_urls": ["http://joc.pubs.informs.org/BackIssues.html"]},
      "url": "https://www.semanticscholar.org/paper/461afff92a4b51c6b64a05dde407270029fd19ec",
      "title": "Predicting Tactical Solutions to Operational Planning Problems under
      Imperfect Information", "abstract": "This paper offers a methodological contribution
      at the intersection of machine learning and operations research. Namely, we
      propose a methodology to quickly predict expected tactical descriptions of operational
      solutions (TDOSs). The problem we address occurs in the context of two-stage
      stochastic programming, where the second stage is demanding computationally.
      We aim to predict at a high speed the expected TDOS associated with the second-stage
      problem, conditionally on the first-stage variables. This may be used in support
      of the solution to the overall two-stage problem by avoiding the online generation
      of multiple second-stage scenarios and solutions. We formulate the tactical
      prediction problem as a stochastic optimal prediction program, whose solution
      we approximate with supervised machine learning. The training data set consists
      of a large number of deterministic operational problems generated by controlled
      probabilistic sampling. The labels are computed based on solutions to these
      problems (solved independently and offline), employing appropriate aggregation
      and subselection methods to address uncertainty. Results on our motivating application
      on load planning for rail transportation show that deep learning models produce
      accurate predictions in very short computing time (milliseconds or less). The
      predictive accuracy is close to the lower bounds calculated based on sample
      average approximation of the stochastic prediction programs.", "venue": "INFORMS
      journal on computing", "year": 2018, "referenceCount": 48, "citationCount":
      34, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1807.11876", "status": null}, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-07-31", "journal": {"volume": "34", "pages": "227-242",
      "name": "INFORMS J. Comput."}, "citationStyles": {"bibtex": "@Article{Larsen2018PredictingTS,\n
      author = {Eric Larsen and S\u00e9bastien Lachapelle and Yoshua Bengio and Emma
      Frejinger and Simon Lacoste-Julien and Andrea Lodi},\n booktitle = {INFORMS
      journal on computing},\n journal = {INFORMS J. Comput.},\n pages = {227-242},\n
      title = {Predicting Tactical Solutions to Operational Planning Problems under
      Imperfect Information},\n volume = {34},\n year = {2018}\n}\n"}, "authors":
      [{"authorId": "2065712703", "name": "Eric Larsen"}, {"authorId": "134730235",
      "name": "S\u00e9bastien Lachapelle"}, {"authorId": "1751762", "name": "Yoshua
      Bengio"}, {"authorId": "35394395", "name": "Emma Frejinger"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "144390922", "name": "Andrea Lodi"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "3f7c5bec7c86712602866d0e07241fe8cd4a64f1", "externalIds":
      {"DBLP": "journals/corr/abs-1807-04740", "MAG": "2962981216", "ArXiv": "1807.04740",
      "CorpusId": 49671556}, "corpusId": 49671556, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/3f7c5bec7c86712602866d0e07241fe8cd4a64f1",
      "title": "Negative Momentum for Improved Game Dynamics", "abstract": "Games
      generalize the single-objective optimization paradigm by introducing different
      objective functions for different players. Differentiable games often proceed
      by simultaneous or alternating gradient updates. In machine learning, games
      are gaining new importance through formulations like generative adversarial
      networks (GANs) and actor-critic systems. However, compared to single-objective
      optimization, game dynamics are more complex and less understood. In this paper,
      we analyze gradient-based methods with momentum on simple games. We prove that
      alternating updates are more stable than simultaneous updates. Next, we show
      both theoretically and empirically that alternating gradient updates with a
      negative momentum term achieves convergence in a difficult toy adversarial problem,
      but also on the notoriously difficult to train saturating GANs.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2018, "referenceCount":
      41, "citationCount": 168, "influentialCitationCount": 29, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-07-12", "journal": {"volume": "abs/1807.04740", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Gidel2018NegativeMF,\n author = {Gauthier Gidel and Reyhane
      Askari Hemmat and M. Pezeshki and Gabriel Huang and R\u00e9mi Le Priol and Simon
      Lacoste-Julien and Ioannis Mitliagkas},\n booktitle = {International Conference
      on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {Negative
      Momentum for Improved Game Dynamics},\n volume = {abs/1807.04740},\n year =
      {2018}\n}\n"}, "authors": [{"authorId": "8150760", "name": "Gauthier Gidel"},
      {"authorId": "7872299", "name": "Reyhane Askari Hemmat"}, {"authorId": "145507036",
      "name": "M. Pezeshki"}, {"authorId": "24040986", "name": "Gabriel Huang"}, {"authorId":
      "10712297", "name": "R\u00e9mi Le Priol"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "88a4b0ec1392f2a7d974ecce41b04aa46d47b4a9", "externalIds":
      {"ACL": "W18-5041", "ArXiv": "1805.11762", "DBLP": "conf/sigdial/LiuL18", "MAG":
      "2951222010", "DOI": "10.18653/v1/W18-5041", "CorpusId": 44117930}, "corpusId":
      44117930, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/88a4b0ec1392f2a7d974ecce41b04aa46d47b4a9",
      "title": "Adversarial Learning of Task-Oriented Neural Dialog Models", "abstract":
      "In this work, we propose an adversarial learning method for reward estimation
      in reinforcement learning (RL) based task-oriented dialog models. Most of the
      current RL based task-oriented dialog systems require the access to a reward
      signal from either user feedback or user ratings. Such user ratings, however,
      may not always be consistent or available in practice. Furthermore, online dialog
      policy learning with RL typically requires a large number of queries to users,
      suffering from sample efficiency problem. To address these challenges, we propose
      an adversarial learning method to learn dialog rewards directly from dialog
      samples. Such rewards are further used to optimize the dialog policy with policy
      gradient based RL. In the evaluation in a restaurant search domain, we show
      that the proposed adversarial dialog learning method achieves advanced dialog
      success rate comparing to strong baseline methods. We further discuss the covariate
      shift problem in online adversarial dialog learning and show how we can address
      that with partial access to user feedback.", "venue": "SIGDIAL Conference",
      "year": 2018, "referenceCount": 37, "citationCount": 35, "influentialCitationCount":
      4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W18-5041.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-05-30", "journal": {"pages": "350-359"}, "citationStyles":
      {"bibtex": "@Article{Liu2018AdversarialLO,\n author = {Bing Liu and Ian Lane},\n
      booktitle = {SIGDIAL Conference},\n pages = {350-359},\n title = {Adversarial
      Learning of Task-Oriented Neural Dialog Models},\n year = {2018}\n}\n"}, "authors":
      [{"authorId": "143830417", "name": "Bing Liu"}, {"authorId": "1765892", "name":
      "Ian Lane"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "46f9a738f1d238dc37813ae02ed3cbfadae30b2e",
      "externalIds": {"MAG": "2797899988", "DBLP": "conf/aistats/GidelPL18", "ArXiv":
      "1804.03176", "CorpusId": 4739494}, "corpusId": 4739494, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/46f9a738f1d238dc37813ae02ed3cbfadae30b2e",
      "title": "Frank-Wolfe Splitting via Augmented Lagrangian Method", "abstract":
      "Minimizing a function over an intersection of convex sets is an important task
      in optimization that is often much more challenging than minimizing it over
      each individual constraint set. While traditional methods such as Frank-Wolfe
      (FW) or proximal gradient descent assume access to a linear or quadratic oracle
      on the intersection, splitting techniques take advantage of the structure of
      each sets, and only require access to the oracle on the individual constraints.
      In this work, we develop and analyze the Frank-Wolfe Augmented Lagrangian (FW-AL)
      algorithm, a method for minimizing a smooth function over convex compact sets
      related by a \"linear consistency\" constraint that only requires access to
      a linear minimization oracle over the individual constraints. It is based on
      the Augmented Lagrangian Method (ALM), also known as Method of Multipliers,
      but unlike most existing splitting methods, it only requires access to linear
      (instead of quadratic) minimization oracles. We use recent advances in the analysis
      of Frank-Wolfe and the alternating direction method of multipliers algorithms
      to prove a sublinear convergence rate for FW-AL over general convex compact
      sets and a linear convergence rate for polytopes.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2018, "referenceCount":
      49, "citationCount": 24, "influentialCitationCount": 7, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-03-31", "journal":
      {"pages": "1456-1465"}, "citationStyles": {"bibtex": "@Article{Gidel2018FrankWolfeSV,\n
      author = {Gauthier Gidel and Fabian Pedregosa and Simon Lacoste-Julien},\n booktitle
      = {International Conference on Artificial Intelligence and Statistics},\n pages
      = {1456-1465},\n title = {Frank-Wolfe Splitting via Augmented Lagrangian Method},\n
      year = {2018}\n}\n"}, "authors": [{"authorId": "8150760", "name": "Gauthier
      Gidel"}, {"authorId": "2570016", "name": "Fabian Pedregosa"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "a4d513cfc9d4902ef1a80198582f29b8ba46ac28",
      "externalIds": {"ArXiv": "1802.07228", "DBLP": "journals/corr/abs-1802-07228",
      "MAG": "2787887017", "DOI": "10.17863/CAM.22520", "CorpusId": 3385567}, "corpusId":
      3385567, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/a4d513cfc9d4902ef1a80198582f29b8ba46ac28",
      "title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention,
      and Mitigation", "abstract": "The following organisations are named on the report:
      Future of Humanity Institute, University of Oxford, Centre for the Study of
      Existential Risk, University of Cambridge, Center for a New American Security,
      Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged
      as a funder.", "venue": "arXiv.org", "year": 2018, "referenceCount": 202, "citationCount":
      525, "influentialCitationCount": 24, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Political Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-02-20", "journal":
      {"volume": "abs/1802.07228", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Brundage2018TheMU,\n author = {Miles Brundage and S. Avin and Jack
      Clark and H. Toner and P. Eckersley and Ben Garfinkel and A. Dafoe and P. Scharre
      and Thomas Zeitzoff and Bobby Filar and H. Anderson and H. Roff and Gregory
      C. Allen and J. Steinhardt and Carrick Flynn and Se\u00e1n \u00d3 h\u00c9igeartaigh
      and S. Beard and Haydn Belfield and Sebastian Farquhar and Clare Lyle and Rebecca
      Crootof and Owain Evans and Michael Page and Joanna J. Bryson and Roman V. Yampolskiy
      and Dario Amodei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title =
      {The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and
      Mitigation},\n volume = {abs/1802.07228},\n year = {2018}\n}\n"}, "authors":
      [{"authorId": "35167962", "name": "Miles Brundage"}, {"authorId": "49344883",
      "name": "S. Avin"}, {"authorId": "2115193883", "name": "Jack Clark"}, {"authorId":
      "48625835", "name": "H. Toner"}, {"authorId": "2654106", "name": "P. Eckersley"},
      {"authorId": "39928654", "name": "Ben Garfinkel"}, {"authorId": "3198576", "name":
      "A. Dafoe"}, {"authorId": "35681920", "name": "P. Scharre"}, {"authorId": "46225273",
      "name": "Thomas Zeitzoff"}, {"authorId": "7888676", "name": "Bobby Filar"},
      {"authorId": "2639880", "name": "H. Anderson"}, {"authorId": "47075894", "name":
      "H. Roff"}, {"authorId": "2059529715", "name": "Gregory C. Allen"}, {"authorId":
      "5164568", "name": "J. Steinhardt"}, {"authorId": "152629250", "name": "Carrick
      Flynn"}, {"authorId": "35793299", "name": "Se\u00e1n \u00d3 h\u00c9igeartaigh"},
      {"authorId": "38992229", "name": "S. Beard"}, {"authorId": "36729401", "name":
      "Haydn Belfield"}, {"authorId": "33859827", "name": "Sebastian Farquhar"}, {"authorId":
      "39439114", "name": "Clare Lyle"}, {"authorId": "35431817", "name": "Rebecca
      Crootof"}, {"authorId": "47107786", "name": "Owain Evans"}, {"authorId": "2054564415",
      "name": "Michael Page"}, {"authorId": "2055073817", "name": "Joanna J. Bryson"},
      {"authorId": "26336155", "name": "Roman V. Yampolskiy"}, {"authorId": "2698777",
      "name": "Dario Amodei"}]}}, {"contexts": ["GANs struggle to generate discrete
      data because the back-propagation algorithm needs to propagate gradients from
      the discriminator through the output of the generator, but this problem is being
      gradually resolved.(9) Like most generative models, GANs can be used to fill
      in gaps in missing data."], "intents": [], "contextsWithIntent": [{"context":
      "GANs struggle to generate discrete data because the back-propagation algorithm
      needs to propagate gradients from the discriminator through the output of the
      generator, but this problem is being gradually resolved.(9) Like most generative
      models, GANs can be used to fill in gaps in missing data.", "intents": []}],
      "isInfluential": false, "citedPaper": {"paperId": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
      "externalIds": {"ArXiv": "1801.07736", "DBLP": "journals/corr/abs-1801-07736",
      "MAG": "2951433039", "CorpusId": 3655946}, "corpusId": 3655946, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
      "title": "MaskGAN: Better Text Generation via Filling in the ______", "abstract":
      "Neural text generation models are often autoregressive language models or seq2seq
      models. These models generate text by sampling words sequentially, with each
      word conditioned on the previous word, and are state-of-the-art for several
      machine translation and summarization benchmarks. These benchmarks are often
      defined by validation perplexity even though this is not a direct measure of
      the quality of the generated text. Additionally, these models are typically
      trained via maxi- mum likelihood and teacher forcing. These methods are well-suited
      to optimizing perplexity but can result in poor sample quality since generating
      text requires conditioning on sequences of words that may have never been observed
      at training time. We propose to improve sample quality using Generative Adversarial
      Networks (GANs), which explicitly train the generator to produce high quality
      samples and have shown a lot of success in image generation. GANs were originally
      designed to output differentiable values, so discrete language generation is
      challenging for them. We claim that validation perplexity alone is not indicative
      of the quality of text generated by a model. We introduce an actor-critic conditional
      GAN that fills in missing text conditioned on the surrounding context. We show
      qualitatively and quantitatively, evidence that this produces more realistic
      conditional and unconditional text samples compared to a maximum likelihood
      trained model.", "venue": "International Conference on Learning Representations",
      "year": 2018, "referenceCount": 38, "citationCount": 436, "influentialCitationCount":
      33, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-01-23", "journal": {"volume": "abs/1801.07736", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Fedus2018MaskGANBT,\n author
      = {W. Fedus and I. Goodfellow and Andrew M. Dai},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {MaskGAN:
      Better Text Generation via Filling in the ______},\n volume = {abs/1801.07736},\n
      year = {2018}\n}\n"}, "authors": [{"authorId": "26958176", "name": "W. Fedus"},
      {"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId": "2555924",
      "name": "Andrew M. Dai"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "45cd88aa4f6c34bf5c8fba16863ffa35fcf53ba2",
      "externalIds": {"DBLP": "journals/corr/abs-1801-03749", "MAG": "2783257164",
      "ArXiv": "1801.03749", "CorpusId": 22126900}, "corpusId": 22126900, "publicationVenue":
      {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine learning
      research", "type": "journal", "alternate_names": ["Journal of Machine Learning
      Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435", "alternate_issns":
      ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/", "alternate_urls":
      ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/45cd88aa4f6c34bf5c8fba16863ffa35fcf53ba2",
      "title": "Improved asynchronous parallel optimization analysis for stochastic
      incremental methods", "abstract": "As datasets continue to increase in size
      and multi-core computer architectures are developed, asynchronous parallel optimization
      algorithms become more and more essential to the field of Machine Learning.
      Unfortunately, conducting the theoretical analysis asynchronous methods is difficult,
      notably due to the introduction of delay and inconsistency in inherently sequential
      algorithms. Handling these issues often requires resorting to simplifying but
      unrealistic assumptions. Through a novel perspective, we revisit and clarify
      a subtle but important technical issue present in a large fraction of the recent
      convergence rate proofs for asynchronous parallel optimization algorithms, and
      propose a simplification of the recently introduced \"perturbed iterate\" framework
      that resolves it. We demonstrate the usefulness of our new framework by analyzing
      three distinct asynchronous parallel incremental optimization algorithms: Hogwild
      (asynchronous SGD), KROMAGNON (asynchronous SVRG) and ASAGA, a novel asynchronous
      parallel version of the incremental gradient algorithm SAGA that enjoys fast
      linear convergence rates. We are able to both remove problematic assumptions
      and obtain better theoretical results. Notably, we prove that ASAGA and KROMAGNON
      can obtain a theoretical linear speedup on multi-core systems even without sparsity
      assumptions. We present results of an implementation on a 40-core architecture
      illustrating the practical speedups as well as the hardware overhead. Finally,
      we investigate the overlap constant, an ill-understood but central quantity
      for the theoretical analysis of asynchronous parallel algorithms. We find that
      it encompasses much more complexity than suggested in previous work, and often
      is order-of-magnitude bigger than traditionally thought.", "venue": "Journal
      of machine learning research", "year": 2018, "referenceCount": 32, "citationCount":
      64, "influentialCitationCount": 14, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-01-11", "journal": {"volume": "19",
      "pages": "81:1-81:68", "name": "J. Mach. Learn. Res."}, "citationStyles": {"bibtex":
      "@Article{Leblond2018ImprovedAP,\n author = {R\u00e9mi Leblond and Fabian Pedregosa
      and Simon Lacoste-Julien},\n booktitle = {Journal of machine learning research},\n
      journal = {J. Mach. Learn. Res.},\n pages = {81:1-81:68},\n title = {Improved
      asynchronous parallel optimization analysis for stochastic incremental methods},\n
      volume = {19},\n year = {2018}\n}\n"}, "authors": [{"authorId": "37212795",
      "name": "R\u00e9mi Leblond"}, {"authorId": "2570016", "name": "Fabian Pedregosa"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "a62b6352a820418fce3284eef0ad94fe5817b52a", "externalIds": {"MAG":
      "2963438599", "DBLP": "journals/corr/abs-1712-08577", "ArXiv": "1712.08577",
      "CorpusId": 31093580}, "corpusId": 31093580, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
      "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf Uncertain
      Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"}, "url":
      "https://www.semanticscholar.org/paper/a62b6352a820418fce3284eef0ad94fe5817b52a",
      "title": "Adaptive Stochastic Dual Coordinate Ascent for Conditional Random
      Fields", "abstract": "This work investigates training Conditional Random Fields
      (CRF) by Stochastic Dual Coordinate Ascent (SDCA). SDCA enjoys a linear convergence
      rate and a strong empirical performance for independent classification problems.
      However, it has never been used to train CRF. Yet it benefits from an exact
      line search with a single marginalization oracle call, unlike previous approaches.
      In this paper, we adapt SDCA to train CRF and we enhance it with an adaptive
      non-uniform sampling strategy. Our preliminary experiments suggest that this
      method matches state-of-the-art CRF optimization techniques.", "venue": "Conference
      on Uncertainty in Artificial Intelligence", "year": 2017, "referenceCount":
      26, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-12-01",
      "journal": {"pages": "815-824"}, "citationStyles": {"bibtex": "@Article{Priol2017AdaptiveSD,\n
      author = {R\u00e9mi Le Priol and Alexandre Pich\u00e9 and Simon Lacoste-Julien},\n
      booktitle = {Conference on Uncertainty in Artificial Intelligence},\n pages
      = {815-824},\n title = {Adaptive Stochastic Dual Coordinate Ascent for Conditional
      Random Fields},\n year = {2017}\n}\n"}, "authors": [{"authorId": "10712297",
      "name": "R\u00e9mi Le Priol"}, {"authorId": "49504044", "name": "Alexandre Pich\u00e9"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851", "externalIds": {"MAG":
      "2963800363", "DBLP": "conf/cvpr/Wang0ZTKC18", "ArXiv": "1711.11585", "DOI":
      "10.1109/CVPR.2018.00917", "CorpusId": 41805341}, "corpusId": 41805341, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851",
      "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional
      GANs", "abstract": "We present a new method for synthesizing high-resolution
      photo-realistic images from semantic label maps using conditional generative
      adversarial networks (conditional GANs). Conditional GANs have enabled a variety
      of applications, but the results are often limited to low-resolution and still
      far from realistic. In this work, we generate 2048 \u00c3\u2014 1024 visually
      appealing results with a novel adversarial loss, as well as new multi-scale
      generator and discriminator architectures. Furthermore, we extend our framework
      to interactive visual manipulation with two additional features. First, we incorporate
      object instance segmentation information, which enables object manipulations
      such as removing/adding objects and changing the object category. Second, we
      propose a method to generate diverse results given the same input, allowing
      users to edit the object appearance interactively. Human opinion studies demonstrate
      that our method significantly outperforms existing methods, advancing both the
      quality and the resolution of deep image synthesis and editing.", "venue": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2017,
      "referenceCount": 68, "citationCount": 3282, "influentialCitationCount": 503,
      "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1711.11585",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-11-30", "journal": {"pages": "8798-8807", "name": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition"}, "citationStyles":
      {"bibtex": "@Article{Wang2017HighResolutionIS,\n author = {Ting-Chun Wang and
      Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and J. Kautz and Bryan Catanzaro},\n
      booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n
      journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n
      pages = {8798-8807},\n title = {High-Resolution Image Synthesis and Semantic
      Manipulation with Conditional GANs},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "2195314", "name": "Ting-Chun Wang"}, {"authorId": "39793900", "name": "Ming-Yu
      Liu"}, {"authorId": "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "29955511",
      "name": "Andrew Tao"}, {"authorId": "1690538", "name": "J. Kautz"}, {"authorId":
      "2301680", "name": "Bryan Catanzaro"}]}}, {"contexts": ["Very many different
      specific formulations of these costs are possible and so far most popular formulations
      seem to perform roughly the same.(18) In the original version of GANs, J(D)
      was defined to be the negative log-likelihood that the discriminator assigns
      to the real-vs-fake labels given the input to the discriminator."], "intents":
      [], "contextsWithIntent": [{"context": "Very many different specific formulations
      of these costs are possible and so far most popular formulations seem to perform
      roughly the same.(18) In the original version of GANs, J(D) was defined to be
      the negative log-likelihood that the discriminator assigns to the real-vs-fake
      labels given the input to the discriminator.", "intents": []}], "isInfluential":
      false, "citedPaper": {"paperId": "c88e8d85fd5160b0793598bda037f977366acf7a",
      "externalIds": {"DBLP": "conf/nips/LucicKMGB18", "MAG": "2768599997", "ArXiv":
      "1711.10337", "CorpusId": 4053393}, "corpusId": 4053393, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/c88e8d85fd5160b0793598bda037f977366acf7a",
      "title": "Are GANs Created Equal? A Large-Scale Study", "abstract": "Generative
      adversarial networks (GAN) are a powerful subclass of generative models. Despite
      a very rich research activity leading to numerous interesting GAN algorithms,
      it is still very hard to assess which algorithm(s) perform better than others.
      We conduct a neutral, multi-faceted large-scale empirical study on state-of-the
      art models and evaluation measures. We find that most models can reach similar
      scores with enough hyperparameter optimization and random restarts. This suggests
      that improvements can arise from a higher computational budget and tuning more
      than fundamental algorithmic changes. To overcome some limitations of the current
      metrics, we also propose several data sets on which precision and recall can
      be computed. Our experimental results suggest that future GAN research should
      be based on more systematic and objective evaluation procedures. Finally, we
      did not find evidence that any of the tested algorithms consistently outperforms
      the non-saturating GAN introduced in \\cite{goodfellow2014generative}.", "venue":
      "Neural Information Processing Systems", "year": 2017, "referenceCount": 28,
      "citationCount": 900, "influentialCitationCount": 82, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-11-28", "journal": {"pages": "698-707"}, "citationStyles": {"bibtex":
      "@Article{Lucic2017AreGC,\n author = {Mario Lucic and Karol Kurach and Marcin
      Michalski and S. Gelly and O. Bousquet},\n booktitle = {Neural Information Processing
      Systems},\n pages = {698-707},\n title = {Are GANs Created Equal? A Large-Scale
      Study},\n year = {2017}\n}\n"}, "authors": [{"authorId": "34302129", "name":
      "Mario Lucic"}, {"authorId": "2006889", "name": "Karol Kurach"}, {"authorId":
      "145605490", "name": "Marcin Michalski"}, {"authorId": "1802148", "name": "S.
      Gelly"}, {"authorId": "1698617", "name": "O. Bousquet"}]}}, {"contexts": [],
      "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e", "externalIds": {"DBLP":
      "conf/icml/OordLBSVKDLCSCG18", "MAG": "2950362437", "ArXiv": "1711.10433", "CorpusId":
      27706557}, "corpusId": 27706557, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e",
      "title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis", "abstract":
      "The recently-developed WaveNet architecture is the current state of the art
      in realistic speech synthesis, consistently rated as more natural sounding for
      many different languages than any previous system. However, because WaveNet
      relies on sequential generation of one audio sample at a time, it is poorly
      suited to today''s massively parallel computers, and therefore hard to deploy
      in a real-time production setting. This paper introduces Probability Density
      Distillation, a new method for training a parallel feed-forward network from
      a trained WaveNet with no significant difference in quality. The resulting system
      is capable of generating high-fidelity speech samples at more than 20 times
      faster than real-time, and is deployed online by Google Assistant, including
      serving multiple English and Japanese voices.", "venue": "International Conference
      on Machine Learning", "year": 2017, "referenceCount": 31, "citationCount": 768,
      "influentialCitationCount": 93, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-11-28", "journal":
      {"pages": "3915-3923"}, "citationStyles": {"bibtex": "@Article{Oord2017ParallelWF,\n
      author = {A\u00e4ron van den Oord and Yazhe Li and Igor Babuschkin and K. Simonyan
      and O. Vinyals and K. Kavukcuoglu and George van den Driessche and Edward Lockhart
      and Luis C. Cobo and Florian Stimberg and Norman Casagrande and Dominik Grewe
      and Seb Noury and S. Dieleman and Erich Elsen and Nal Kalchbrenner and H. Zen
      and Alex Graves and Helen King and T. Walters and Dan Belov and D. Hassabis},\n
      booktitle = {International Conference on Machine Learning},\n pages = {3915-3923},\n
      title = {Parallel WaveNet: Fast High-Fidelity Speech Synthesis},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "3422336", "name": "A\u00e4ron van den Oord"}, {"authorId":
      "2144417088", "name": "Yazhe Li"}, {"authorId": "2256699276", "name": "Igor
      Babuschkin"}, {"authorId": "34838386", "name": "K. Simonyan"}, {"authorId":
      "1689108", "name": "O. Vinyals"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"},
      {"authorId": "47568983", "name": "George van den Driessche"}, {"authorId": "49860549",
      "name": "Edward Lockhart"}, {"authorId": "38712164", "name": "Luis C. Cobo"},
      {"authorId": "3205302", "name": "Florian Stimberg"}, {"authorId": "2670752",
      "name": "Norman Casagrande"}, {"authorId": "2401609", "name": "Dominik Grewe"},
      {"authorId": "30155667", "name": "Seb Noury"}, {"authorId": "48373216", "name":
      "S. Dieleman"}, {"authorId": "152585800", "name": "Erich Elsen"}, {"authorId":
      "2583391", "name": "Nal Kalchbrenner"}, {"authorId": "1691713", "name": "H.
      Zen"}, {"authorId": "1753223", "name": "Alex Graves"}, {"authorId": "143776287",
      "name": "Helen King"}, {"authorId": "2066221078", "name": "T. Walters"}, {"authorId":
      "143813532", "name": "Dan Belov"}, {"authorId": "48987704", "name": "D. Hassabis"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64", "externalIds":
      {"MAG": "2952170190", "DBLP": "conf/cvpr/ChoiCKH0C18", "ArXiv": "1711.09020",
      "DOI": "10.1109/CVPR.2018.00916", "CorpusId": 9417016}, "corpusId": 9417016,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
      "title": "StarGAN: Unified Generative Adversarial Networks for Multi-domain
      Image-to-Image Translation", "abstract": "Recent studies have shown remarkable
      success in image-to-image translation for two domains. However, existing approaches
      have limited scalability and robustness in handling more than two domains, since
      different models should be built independently for every pair of image domains.
      To address this limitation, we propose StarGAN, a novel and scalable approach
      that can perform image-to-image translations for multiple domains using only
      a single model. Such a unified model architecture of StarGAN allows simultaneous
      training of multiple datasets with different domains within a single network.
      This leads to StarGAN''s superior quality of translated images compared to existing
      models as well as the novel capability of flexibly translating an input image
      to any desired target domain. We empirically demonstrate the effectiveness of
      our approach on a facial attribute transfer and a facial expression synthesis
      tasks.", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "year": 2017, "referenceCount": 33, "citationCount": 3041, "influentialCitationCount":
      566, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1711.09020",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-11-24", "journal": {"pages": "8789-8797", "name": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition"}, "citationStyles":
      {"bibtex": "@Article{Choi2017StarGANUG,\n author = {Yunjey Choi and Min-Je Choi
      and M. Kim and Jung-Woo Ha and Sunghun Kim and J. Choo},\n booktitle = {2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal =
      {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages
      = {8789-8797},\n title = {StarGAN: Unified Generative Adversarial Networks for
      Multi-domain Image-to-Image Translation},\n year = {2017}\n}\n"}, "authors":
      [{"authorId": "30187096", "name": "Yunjey Choi"}, {"authorId": "8486223", "name":
      "Min-Je Choi"}, {"authorId": "2110103236", "name": "M. Kim"}, {"authorId": "2577039",
      "name": "Jung-Woo Ha"}, {"authorId": "1787729", "name": "Sunghun Kim"}, {"authorId":
      "1795455", "name": "J. Choo"}]}}, {"contexts": ["Goodfellow et al. (2014) GANs
      are a new way to build generative models P(X )."], "intents": [], "contextsWithIntent":
      [{"context": "Goodfellow et al. (2014) GANs are a new way to build generative
      models P(X ).", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "e871d4ef27e4341e93a6d2d3785bd8a6de775266", "externalIds": {"MAG": "2772481884",
      "CorpusId": 64691374}, "corpusId": 64691374, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/e871d4ef27e4341e93a6d2d3785bd8a6de775266",
      "title": "Generative Adversarial Networks\u3092\u7528\u3044\u305f\u78ba\u7387\u7684\u8b58\u5225\u30e2\u30c7\u30eb\u304b\u3089\u8a13\u7df4\u30c7\u30fc\u30bf\u751f\u6210\u5206\u5e03\u306e\u63a8\u5b9a",
      "abstract": null, "venue": "", "year": 2017, "referenceCount": 0, "citationCount":
      1, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-11-02",
      "journal": {"volume": "117", "pages": "301-308", "name": ""}, "citationStyles":
      {"bibtex": "@Inproceedings{\u5149\u4eae2017GenerativeAN,\n author = {\u8349\u91ce
      \u5149\u4eae and \u6df3\u5b50 \u4f50\u4e45\u9593},\n pages = {301-308},\n title
      = {Generative Adversarial Networks\u3092\u7528\u3044\u305f\u78ba\u7387\u7684\u8b58\u5225\u30e2\u30c7\u30eb\u304b\u3089\u8a13\u7df4\u30c7\u30fc\u30bf\u751f\u6210\u5206\u5e03\u306e\u63a8\u5b9a},\n
      volume = {117},\n year = {2017}\n}\n"}, "authors": [{"authorId": "71773697",
      "name": "\u8349\u91ce \u5149\u4eae"}, {"authorId": "1520866776", "name": "\u6df3\u5b50
      \u4f50\u4e45\u9593"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
      "externalIds": {"MAG": "2766527293", "DBLP": "conf/iclr/KarrasALL18", "ArXiv":
      "1710.10196", "CorpusId": 3568073}, "corpusId": 3568073, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "We describe a new training methodology for generative adversarial
      networks. The key idea is to grow both the generator and discriminator progressively:
      starting from a low resolution, we add new layers that model increasingly fine
      details as training progresses. This both speeds the training up and greatly
      stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
      CelebA images at 1024^2. We also propose a simple way to increase the variation
      in generated images, and achieve a record inception score of 8.80 in unsupervised
      CIFAR10. Additionally, we describe several implementation details that are important
      for discouraging unhealthy competition between the generator and discriminator.
      Finally, we suggest a new metric for evaluating GAN results, both in terms of
      image quality and variation. As an additional contribution, we construct a higher-quality
      version of the CelebA dataset.", "venue": "International Conference on Learning
      Representations", "year": 2017, "referenceCount": 65, "citationCount": 5891,
      "influentialCitationCount": 1101, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2017-10-27", "journal": {"volume": "abs/1710.10196",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Karras2017ProgressiveGO,\n
      author = {Tero Karras and Timo Aila and S. Laine and J. Lehtinen},\n booktitle
      = {International Conference on Learning Representations},\n journal = {ArXiv},\n
      title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},\n
      volume = {abs/1710.10196},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "2976930", "name": "Tero Karras"}, {"authorId": "1761103", "name": "Timo Aila"},
      {"authorId": "36436218", "name": "S. Laine"}, {"authorId": "49244945", "name":
      "J. Lehtinen"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "471908e99d6965f0f6d249c9cd013485dc2b21df",
      "externalIds": {"MAG": "2766290211", "ArXiv": "1710.08446", "DBLP": "conf/iclr/FedusRLDMG18",
      "CorpusId": 3398677}, "corpusId": 3398677, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/471908e99d6965f0f6d249c9cd013485dc2b21df",
      "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence
      At Every Step", "abstract": "Generative adversarial networks (GANs) are a family
      of generative models that do not minimize a single training criterion. Unlike
      other generative models, the data distribution is learned via a game between
      a generator (the generative model) and a discriminator (a teacher providing
      training signal) that each minimize their own cost. GANs are designed to reach
      a Nash equilibrium at which each player cannot reduce their cost without changing
      the other players'' parameters. One useful approach for the theory of GANs is
      to show that a divergence between the training distribution and the model distribution
      obtains its minimum value at equilibrium. Several recent research directions
      have been motivated by the idea that this divergence is the primary guide for
      the learning process and that every step of learning should decrease the divergence.
      We show that this view is overly restrictive. During GAN training, the discriminator
      provides learning signal in situations where the gradients of the divergences
      between distributions would not be useful. We provide empirical counterexamples
      to the view of GAN training as divergence minimization. Specifically, we demonstrate
      that GANs are able to learn distributions in situations where the divergence
      minimization point of view predicts they would fail. We also show that gradient
      penalties motivated from the divergence minimization perspective are equally
      helpful when applied in other contexts in which the divergence minimization
      perspective does not predict they would be helpful. This contributes to a growing
      body of evidence that GAN training may be more usefully viewed as approaching
      Nash equilibria via trajectories that do not necessarily minimize a specific
      divergence at each step.", "venue": "International Conference on Learning Representations",
      "year": 2017, "referenceCount": 25, "citationCount": 199, "influentialCitationCount":
      24, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2017-10-23", "journal": {"volume": "abs/1710.08446", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Fedus2017ManyPT,\n author =
      {W. Fedus and Mihaela Rosca and Balaji Lakshminarayanan and Andrew M. Dai and
      S. Mohamed and I. Goodfellow},\n booktitle = {International Conference on Learning
      Representations},\n journal = {ArXiv},\n title = {Many Paths to Equilibrium:
      GANs Do Not Need to Decrease a Divergence At Every Step},\n volume = {abs/1710.08446},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "26958176", "name": "W. Fedus"},
      {"authorId": "35578586", "name": "Mihaela Rosca"}, {"authorId": "40627523",
      "name": "Balaji Lakshminarayanan"}, {"authorId": "2555924", "name": "Andrew
      M. Dai"}, {"authorId": "14594344", "name": "S. Mohamed"}, {"authorId": "153440022",
      "name": "I. Goodfellow"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "9f605f8d5d6b143b8c34ccd498986a460c32e641",
      "externalIds": {"MAG": "2751565964", "DBLP": "journals/corr/abs-1708-08819",
      "ArXiv": "1708.08819", "CorpusId": 7559933}, "corpusId": 7559933, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/9f605f8d5d6b143b8c34ccd498986a460c32e641",
      "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields",
      "abstract": "Generative adversarial networks (GANs) evolved into one of the
      most successful unsupervised techniques for generating realistic images. Even
      though it has recently been shown that GAN training converges, GAN models often
      end up in local Nash equilibria that are associated with mode collapse or otherwise
      fail to model the target distribution. We introduce Coulomb GANs, which pose
      the GAN learning problem as a potential field of charged particles, where generated
      samples are attracted to training set samples but repel each other. The discriminator
      learns a potential field while the generator decreases the energy by moving
      its samples along the vector (force) field determined by the gradient of the
      potential field. Through decreasing the energy, the GAN model learns to generate
      samples according to the whole target distribution and does not only cover some
      of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which
      is optimal in the sense that the model distribution equals the target distribution.
      We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN
      and celebA, Coulomb GANs set a new state of the art and produce a previously
      unseen variety of different samples.", "venue": "International Conference on
      Learning Representations", "year": 2017, "referenceCount": 43, "citationCount":
      69, "influentialCitationCount": 10, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2017-08-29", "journal": {"volume": "abs/1708.08819", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Unterthiner2017CoulombGP,\n
      author = {Thomas Unterthiner and Bernhard Nessler and G. Klambauer and M. Heusel
      and Hubert Ramsauer and S. Hochreiter},\n booktitle = {International Conference
      on Learning Representations},\n journal = {ArXiv},\n title = {Coulomb GANs:
      Provably Optimal Nash Equilibria via Potential Fields},\n volume = {abs/1708.08819},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "2465270", "name": "Thomas Unterthiner"},
      {"authorId": "37082831", "name": "Bernhard Nessler"}, {"authorId": "1994964",
      "name": "G. Klambauer"}, {"authorId": "2445103", "name": "M. Heusel"}, {"authorId":
      "19219270", "name": "Hubert Ramsauer"}, {"authorId": "3308557", "name": "S.
      Hochreiter"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "ea35ade941d16c0a62cbda3046fb1b08389ad497",
      "externalIds": {"DBLP": "conf/nips/PedregosaLL17", "ArXiv": "1707.06468", "MAG":
      "2952031106", "CorpusId": 780327}, "corpusId": 780327, "publicationVenue": {"id":
      "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/ea35ade941d16c0a62cbda3046fb1b08389ad497",
      "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite
      Optimization", "abstract": "Due to their simplicity and excellent performance,
      parallel asynchronous variants of stochastic gradient descent have become popular
      methods to solve a wide range of large-scale optimization problems on multi-core
      architectures. Yet, despite their practical success, support for nonsmooth objectives
      is still lacking, making them unsuitable for many problems of interest in machine
      learning, such as the Lasso, group Lasso or empirical risk minimization with
      convex constraints. \nIn this work, we propose and analyze ProxASAGA, a fully
      asynchronous sparse method inspired by SAGA, a variance reduced incremental
      gradient algorithm. The proposed method is easy to implement and significantly
      outperforms the state of the art on several nonsmooth, large-scale problems.
      We prove that our method achieves a theoretical linear speedup with respect
      to the sequential version under assumptions on the sparsity of gradients and
      block-separability of the proximal term. Empirical benchmarks on a multi-core
      architecture illustrate practical speedups of up to 12x on a 20-core machine.",
      "venue": "Neural Information Processing Systems", "year": 2017, "referenceCount":
      42, "citationCount": 36, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-07-20",
      "journal": {"pages": "56-65"}, "citationStyles": {"bibtex": "@Article{Pedregosa2017BreakingTN,\n
      author = {Fabian Pedregosa and R\u00e9mi Leblond and Simon Lacoste-Julien},\n
      booktitle = {Neural Information Processing Systems},\n pages = {56-65},\n title
      = {Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite
      Optimization},\n year = {2017}\n}\n"}, "authors": [{"authorId": "2570016", "name":
      "Fabian Pedregosa"}, {"authorId": "37212795", "name": "R\u00e9mi Leblond"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "041ed3b277e5852a28acd23740b0772a7ce3c6ef", "externalIds": {"MAG":
      "2961396908", "PubMedCentral": "7041894", "DOI": "10.1161/CIRCOUTCOMES.118.005122",
      "CorpusId": 23808696, "PubMed": "31284738"}, "corpusId": 23808696, "publicationVenue":
      {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0", "name": "bioRxiv", "type": "journal",
      "url": "http://biorxiv.org/"}, "url": "https://www.semanticscholar.org/paper/041ed3b277e5852a28acd23740b0772a7ce3c6ef",
      "title": "Privacy-Preserving Generative Deep Neural Networks Support Clinical
      Data Sharing", "abstract": "Background Data sharing accelerates scientific progress
      but sharing individual level data while preserving patient privacy presents
      a barrier. Methods and Results Using pairs of deep neural networks, we generated
      simulated, synthetic \u201cparticipants\u201d that closely resemble participants
      of the SPRINT trial. We showed that such paired networks can be trained with
      differential privacy, a formal privacy framework that limits the likelihood
      that queries of the synthetic participants\u2019 data could identify a real
      a participant in the trial. Machine-learning predictors built on the synthetic
      population generalize to the original dataset. This finding suggests that the
      synthetic data can be shared with others, enabling them to perform hypothesis-generating
      analyses as though they had the original trial data. Conclusions Deep neural
      networks that generate synthetic participants facilitate secondary analyses
      and reproducible investigation of clinical datasets by enhancing data sharing
      while preserving participant privacy.", "venue": "bioRxiv", "year": 2017, "referenceCount":
      48, "citationCount": 338, "influentialCitationCount": 19, "isOpenAccess": true,
      "openAccessPdf": null, "fieldsOfStudy": ["Medicine", "Computer Science", "Biology"],
      "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Biology", "source":
      "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2017-07-05", "journal": {"volume": "12", "pages": "e005122
      - e005122", "name": "Circulation. Cardiovascular Quality and Outcomes"}, "citationStyles":
      {"bibtex": "@Article{Beaulieu-Jones2017PrivacyPreservingGD,\n author = {B. Beaulieu-Jones
      and Zhiwei Steven Wu and Christopher J. Williams and Ran Lee and S. Bhavnani
      and James Brian Byrd and C. Greene},\n booktitle = {bioRxiv},\n journal = {Circulation.
      Cardiovascular Quality and Outcomes},\n pages = {e005122 - e005122},\n title
      = {Privacy-Preserving Generative Deep Neural Networks Support Clinical Data
      Sharing},\n volume = {12},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "1402651503", "name": "B. Beaulieu-Jones"}, {"authorId": "1768074", "name":
      "Zhiwei Steven Wu"}, {"authorId": "2110521722", "name": "Christopher J. Williams"},
      {"authorId": "2110703328", "name": "Ran Lee"}, {"authorId": "10301085", "name":
      "S. Bhavnani"}, {"authorId": "6031233", "name": "James Brian Byrd"}, {"authorId":
      "2104940", "name": "C. Greene"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
      "externalIds": {"MAG": "2682189153", "DBLP": "journals/corr/ArpitJBKBKMFCBL17",
      "ArXiv": "1706.05394", "CorpusId": 11455421}, "corpusId": 11455421, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
      "title": "A Closer Look at Memorization in Deep Networks", "abstract": "We examine
      the role of memorization in deep learning, drawing connections to capacity,
      generalization, and adversarial robustness. While deep networks are capable
      of memorizing noise data, our results suggest that they tend to prioritize learning
      simple patterns first. In our experiments, we expose qualitative differences
      in gradient-based optimization of deep neural networks (DNNs) on noise vs. real
      data. We also demonstrate that for appropriately tuned explicit regularization
      (e.g., dropout) we can degrade DNN training performance on noise datasets without
      compromising generalization on real data. Our analysis suggests that the notions
      of effective capacity which are dataset independent are unlikely to explain
      the generalization performance of deep networks when trained with gradient based
      methods because training data itself plays an important role in determining
      the degree of memorization.", "venue": "International Conference on Machine
      Learning", "year": 2017, "referenceCount": 42, "citationCount": 1397, "influentialCitationCount":
      137, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2017-06-16", "journal": {"pages": "233-242"},
      "citationStyles": {"bibtex": "@Article{Arpit2017ACL,\n author = {Devansh Arpit
      and Stanislaw Jastrzebski and Nicolas Ballas and David Krueger and Emmanuel
      Bengio and Maxinder S. Kanwal and Tegan Maharaj and Asja Fischer and Aaron C.
      Courville and Yoshua Bengio and Simon Lacoste-Julien},\n booktitle = {International
      Conference on Machine Learning},\n pages = {233-242},\n title = {A Closer Look
      at Memorization in Deep Networks},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "2309967", "name": "Devansh Arpit"}, {"authorId": "40569328", "name": "Stanislaw
      Jastrzebski"}, {"authorId": "2482072", "name": "Nicolas Ballas"}, {"authorId":
      "145055042", "name": "David Krueger"}, {"authorId": "2416433", "name": "Emmanuel
      Bengio"}, {"authorId": "19308176", "name": "Maxinder S. Kanwal"}, {"authorId":
      "3422058", "name": "Tegan Maharaj"}, {"authorId": "35988982", "name": "Asja
      Fischer"}, {"authorId": "1760871", "name": "Aaron C. Courville"}, {"authorId":
      "1751762", "name": "Yoshua Bengio"}, {"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "345bbd344177815dfb9214c61403cb7eac6de450",
      "externalIds": {"DBLP": "conf/iclr/LeblondAOL18", "ArXiv": "1706.04499", "MAG":
      "2950486958", "CorpusId": 34984289}, "corpusId": 34984289, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/345bbd344177815dfb9214c61403cb7eac6de450",
      "title": "SEARNN: Training RNNs with Global-Local Losses", "abstract": "We propose
      SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired
      by the \"learning to search\" (L2S) approach to structured prediction. RNNs
      have been widely successful in structured prediction applications such as machine
      translation or parsing, and are commonly trained using maximum likelihood estimation
      (MLE). Unfortunately, this training loss is not always an appropriate surrogate
      for the test error: by only maximizing the ground truth probability, it fails
      to exploit the wealth of information offered by structured losses. Further,
      it introduces discrepancies between training and predicting (such as exposure
      bias) that may hurt test performance. Instead, SEARNN leverages test-alike search
      space exploration to introduce global-local losses that are closer to the test
      error. We first demonstrate improved performance over MLE on two different tasks:
      OCR and spelling correction. Then, we propose a subsampling strategy to enable
      SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits
      of our approach on a machine translation task.", "venue": "International Conference
      on Learning Representations", "year": 2017, "referenceCount": 39, "citationCount":
      49, "influentialCitationCount": 10, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-06-14", "journal":
      {"volume": "abs/1706.04499", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Leblond2017SEARNNTR,\n author = {R\u00e9mi Leblond and Jean-Baptiste
      Alayrac and A. Osokin and Simon Lacoste-Julien},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {SEARNN:
      Training RNNs with Global-Local Losses},\n volume = {abs/1706.04499},\n year
      = {2017}\n}\n"}, "authors": [{"authorId": "37212795", "name": "R\u00e9mi Leblond"},
      {"authorId": "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId": "145319877",
      "name": "A. Osokin"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "21b25b025898bd1cabe60234434b49cf14016981", "externalIds":
      {"DBLP": "journals/corr/NagarajanK17", "MAG": "2951897565", "ArXiv": "1706.04156",
      "CorpusId": 8424807}, "corpusId": 8424807, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/21b25b025898bd1cabe60234434b49cf14016981",
      "title": "Gradient descent GAN optimization is locally stable", "abstract":
      "Despite the growing prominence of generative adversarial networks (GANs), optimization
      in GANs is still a poorly understood topic. In this paper, we analyze the \"gradient
      descent\" form of GAN optimization i.e., the natural setting where we simultaneously
      take small gradient steps in both generator and discriminator parameters. We
      show that even though GAN optimization does not correspond to a convex-concave
      game (even for simple parameterizations), under proper conditions, equilibrium
      points of this optimization procedure are still \\emph{locally asymptotically
      stable} for the traditional GAN formulation. On the other hand, we show that
      the recently proposed Wasserstein GAN can have non-convergent limit cycles near
      equilibrium. Motivated by this stability analysis, we propose an additional
      regularization term for gradient descent GAN updates, which \\emph{is} able
      to guarantee local stability for both the WGAN and the traditional GAN, and
      also shows practical promise in speeding up convergence and addressing mode
      collapse.", "venue": "Neural Information Processing Systems", "year": 2017,
      "referenceCount": 25, "citationCount": 319, "influentialCitationCount": 49,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2017-06-13", "journal": {"pages": "5585-5595"}, "citationStyles": {"bibtex":
      "@Article{Nagarajan2017GradientDG,\n author = {Vaishnavh Nagarajan and J. Z.
      Kolter},\n booktitle = {Neural Information Processing Systems},\n pages = {5585-5595},\n
      title = {Gradient descent GAN optimization is locally stable},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "34602162", "name": "Vaishnavh Nagarajan"}, {"authorId":
      "145116464", "name": "J. Z. Kolter"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "459fbc416eb9a55920645c741b1e4cce95f39786",
      "externalIds": {"ArXiv": "1705.10461", "DBLP": "conf/nips/MeschederNG17", "MAG":
      "2951567597", "CorpusId": 2349418}, "corpusId": 2349418, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/459fbc416eb9a55920645c741b1e4cce95f39786",
      "title": "The Numerics of GANs", "abstract": "In this paper, we analyze the
      numerics of common algorithms for training Generative Adversarial Networks (GANs).
      Using the formalism of smooth two-player games we analyze the associated gradient
      vector field of GAN training objectives. Our findings suggest that the convergence
      of current algorithms suffers due to two factors: i) presence of eigenvalues
      of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues
      with big imaginary part. Using these findings, we design a new algorithm that
      overcomes some of these limitations and has better convergence properties. Experimentally,
      we demonstrate its superiority on training common GAN architectures and show
      convergence on GAN architectures that are known to be notoriously hard to train.",
      "venue": "Neural Information Processing Systems", "year": 2017, "referenceCount":
      30, "citationCount": 417, "influentialCitationCount": 77, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-05-30",
      "journal": {"volume": "abs/1705.10461", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Mescheder2017TheNO,\n author = {L. Mescheder and Sebastian
      Nowozin and Andreas Geiger},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {The Numerics of GANs},\n volume = {abs/1705.10461},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "8226549", "name": "L. Mescheder"},
      {"authorId": "2388416", "name": "Sebastian Nowozin"}, {"authorId": "47237027",
      "name": "Andreas Geiger"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "bc995457cf5f4b2b5ef62106856571588d7d70f2",
      "externalIds": {"DBLP": "journals/corr/DanihelkaLUWD17", "MAG": "2615429765",
      "ArXiv": "1705.05263", "CorpusId": 1564507}, "corpusId": 1564507, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/bc995457cf5f4b2b5ef62106856571588d7d70f2",
      "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs",
      "abstract": "We train a generator by maximum likelihood and we also train the
      same generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein distances.
      We show that an independent critic trained to approximate Wasserstein distance
      between the validation set and the generator distribution helps detect overfitting.
      Finally, we use ideas from the one-shot learning literature to develop a novel
      fast learning critic.", "venue": "arXiv.org", "year": 2017, "referenceCount":
      19, "citationCount": 52, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-05-15", "journal":
      {"volume": "abs/1705.05263", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Danihelka2017ComparisonOM,\n author = {Ivo Danihelka and Balaji Lakshminarayanan
      and Benigno Uria and Daan Wierstra and P. Dayan},\n booktitle = {arXiv.org},\n
      journal = {ArXiv},\n title = {Comparison of Maximum Likelihood and GAN-based
      training of Real NVPs},\n volume = {abs/1705.05263},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "1841008", "name": "Ivo Danihelka"}, {"authorId": "40627523",
      "name": "Balaji Lakshminarayanan"}, {"authorId": "2825051", "name": "Benigno
      Uria"}, {"authorId": "1688276", "name": "Daan Wierstra"}, {"authorId": "1790646",
      "name": "P. Dayan"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
      "externalIds": {"ArXiv": "1704.00028", "MAG": "2605135824", "DBLP": "conf/nips/GulrajaniAADC17",
      "CorpusId": 10894094}, "corpusId": 10894094, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/edf73ab12595c6709f646f542a0d2b33eb20a3f4",
      "title": "Improved Training of Wasserstein GANs", "abstract": "Generative Adversarial
      Networks (GANs) are powerful generative models, but suffer from training instability.
      The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training
      of GANs, but sometimes can still generate only low-quality samples or fail to
      converge. We find that these problems are often due to the use of weight clipping
      in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired
      behavior. We propose an alternative to clipping weights: penalize the norm of
      gradient of the critic with respect to its input. Our proposed method performs
      better than standard WGAN and enables stable training of a wide variety of GAN
      architectures with almost no hyperparameter tuning, including 101-layer ResNets
      and language models over discrete data. We also achieve high quality generations
      on CIFAR-10 and LSUN bedrooms.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 37, "citationCount": 7887, "influentialCitationCount":
      1505, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2017-03-31", "journal": {"pages": "5767-5777"},
      "citationStyles": {"bibtex": "@Article{Gulrajani2017ImprovedTO,\n author = {Ishaan
      Gulrajani and Faruk Ahmed and Mart\u00edn Arjovsky and Vincent Dumoulin and
      Aaron C. Courville},\n booktitle = {Neural Information Processing Systems},\n
      pages = {5767-5777},\n title = {Improved Training of Wasserstein GANs},\n year
      = {2017}\n}\n"}, "authors": [{"authorId": "2708454", "name": "Ishaan Gulrajani"},
      {"authorId": "2054472270", "name": "Faruk Ahmed"}, {"authorId": "2877311", "name":
      "Mart\u00edn Arjovsky"}, {"authorId": "3074927", "name": "Vincent Dumoulin"},
      {"authorId": "1760871", "name": "Aaron C. Courville"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "c43d954cf8133e6254499f3d68e45218067e4941", "externalIds": {"MAG": "2951488027",
      "ArXiv": "1703.10593", "DBLP": "conf/iccv/ZhuPIE17", "DOI": "10.1109/ICCV.2017.244",
      "CorpusId": 206770979}, "corpusId": 206770979, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941",
      "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
      Networks", "abstract": "Image-to-image translation is a class of vision and
      graphics problems where the goal is to learn the mapping between an input image
      and an output image using a training set of aligned image pairs. However, for
      many tasks, paired training data will not be available. We present an approach
      for learning to translate an image from a source domain X to a target domain
      Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192
      Y such that the distribution of images from G(X) is indistinguishable from the
      distribution Y using an adversarial loss. Because this mapping is highly under-constrained,
      we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency
      loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented
      on several tasks where paired training data does not exist, including collection
      style transfer, object transfiguration, season transfer, photo enhancement,
      etc. Quantitative comparisons against several prior methods demonstrate the
      superiority of our approach.", "venue": "IEEE International Conference on Computer
      Vision", "year": 2017, "referenceCount": 68, "citationCount": 4596, "influentialCitationCount":
      730, "isOpenAccess": true, "openAccessPdf": {"url": "https://repositorio.unal.edu.co/bitstream/unal/82529/2/98562187.2022.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-03-30", "journal": {"pages": "2242-2251", "name": "2017
      IEEE International Conference on Computer Vision (ICCV)"}, "citationStyles":
      {"bibtex": "@Article{Zhu2017UnpairedIT,\n author = {Jun-Yan Zhu and Taesung
      Park and Phillip Isola and Alexei A. Efros},\n booktitle = {IEEE International
      Conference on Computer Vision},\n journal = {2017 IEEE International Conference
      on Computer Vision (ICCV)},\n pages = {2242-2251},\n title = {Unpaired Image-to-Image
      Translation Using Cycle-Consistent Adversarial Networks},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "2071929129",
      "name": "Taesung Park"}, {"authorId": "2094770", "name": "Phillip Isola"}, {"authorId":
      "1763086", "name": "Alexei A. Efros"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "4ebb5535b762c649ddc9b6619e937d245e5e249b",
      "externalIds": {"MAG": "2950672178", "DBLP": "journals/corr/OsokinBL17", "ArXiv":
      "1703.02403", "CorpusId": 14780228}, "corpusId": 14780228, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/4ebb5535b762c649ddc9b6619e937d245e5e249b",
      "title": "On Structured Prediction Theory with Calibrated Convex Surrogate Losses",
      "abstract": "We provide novel theoretical insights on structured prediction
      in the context of efficient convex surrogate loss minimization with consistency
      guarantees. For any task loss, we construct a convex surrogate that can be optimized
      via stochastic gradient descent and we prove tight bounds on the so-called \"calibration
      function\" relating the excess surrogate risk to the actual risk. In contrast
      to prior related work, we carefully monitor the effect of the exponential number
      of classes in the learning guarantees as well as on the optimization complexity.
      As an interesting consequence, we formalize the intuition that some task losses
      make learning harder than others, and that the classical 0-1 loss is ill-suited
      for structured prediction.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 54, "citationCount": 50, "influentialCitationCount":
      2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2017-03-07", "journal": {"pages": "302-313"}, "citationStyles": {"bibtex":
      "@Article{Osokin2017OnSP,\n author = {A. Osokin and F. Bach and Simon Lacoste-Julien},\n
      booktitle = {Neural Information Processing Systems},\n pages = {302-313},\n
      title = {On Structured Prediction Theory with Calibrated Convex Surrogate Losses},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "145319877", "name": "A. Osokin"},
      {"authorId": "144570279", "name": "F. Bach"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "641165c959554d8f03314778bd6dfb581d9a469e",
      "externalIds": {"DBLP": "conf/icml/Arora0LMZ17", "ArXiv": "1703.00573", "MAG":
      "2952745707", "CorpusId": 13141061}, "corpusId": 13141061, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/641165c959554d8f03314778bd6dfb581d9a469e",
      "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)",
      "abstract": "We show that training of generative adversarial network (GAN) may
      not have good generalization properties; e.g., training may appear successful
      but the trained distribution may be far from target distribution in standard
      metrics. However, generalization does occur for a weaker metric called neural
      net distance. It is also shown that an approximate pure equilibrium exists in
      the discriminator/generator game for a special class of generators with natural
      training objectives when generator capacity and training set sizes are moderate.
      \nThis existence of equilibrium inspires MIX+GAN protocol, which can be combined
      with any existing GAN training, and empirically shown to improve some of them.",
      "venue": "International Conference on Machine Learning", "year": 2017, "referenceCount":
      27, "citationCount": 623, "influentialCitationCount": 90, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-03-02",
      "journal": {"volume": "abs/1703.00573", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Arora2017GeneralizationAE,\n author = {Sanjeev Arora and
      Rong Ge and Yingyu Liang and Tengyu Ma and Yi Zhang},\n booktitle = {International
      Conference on Machine Learning},\n journal = {ArXiv},\n title = {Generalization
      and Equilibrium in Generative Adversarial Nets (GANs)},\n volume = {abs/1703.00573},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "145563459", "name": "Sanjeev
      Arora"}, {"authorId": "144804200", "name": "Rong Ge"}, {"authorId": "40609253",
      "name": "Yingyu Liang"}, {"authorId": "1901958", "name": "Tengyu Ma"}, {"authorId":
      "2153912718", "name": "Yi Zhang"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "b69badabc3fddc9710faa44c530473397303b0b9",
      "externalIds": {"MAG": "2962947361", "ArXiv": "1703.00848", "DBLP": "journals/corr/LiuBK17",
      "CorpusId": 3783306}, "corpusId": 3783306, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/b69badabc3fddc9710faa44c530473397303b0b9",
      "title": "Unsupervised Image-to-Image Translation Networks", "abstract": "Unsupervised
      image-to-image translation aims at learning a joint distribution of images in
      different domains by using images from the marginal distributions in individual
      domains. Since there exists an infinite set of joint distributions that can
      arrive the given marginal distributions, one could infer nothing about the joint
      distribution from the marginal distributions without additional assumptions.
      To address the problem, we make a shared-latent space assumption and propose
      an unsupervised image-to-image translation framework based on Coupled GANs.
      We compare the proposed framework with competing approaches and present high
      quality image translation results on various challenging unsupervised image
      translation tasks, including street scene image translation, animal image translation,
      and face image translation. We also apply the proposed framework to domain adaptation
      and achieve state-of-the-art performance on benchmark datasets. Code and additional
      results are available in this https URL .", "venue": "Neural Information Processing
      Systems", "year": 2017, "referenceCount": 38, "citationCount": 2365, "influentialCitationCount":
      335, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-03-02", "journal":
      {"pages": "700-708"}, "citationStyles": {"bibtex": "@Article{Liu2017UnsupervisedIT,\n
      author = {Ming-Yu Liu and T. Breuel and J. Kautz},\n booktitle = {Neural Information
      Processing Systems},\n pages = {700-708},\n title = {Unsupervised Image-to-Image
      Translation Networks},\n year = {2017}\n}\n"}, "authors": [{"authorId": "39793900",
      "name": "Ming-Yu Liu"}, {"authorId": "1733858", "name": "T. Breuel"}, {"authorId":
      "1690538", "name": "J. Kautz"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "993f0793ca7217e03afbd29346d03c01109acc49",
      "externalIds": {"DBLP": "conf/iccv/AlayracSLL17", "ArXiv": "1702.02738", "MAG":
      "2594270457", "DOI": "10.1109/ICCV.2017.234", "CorpusId": 2292730}, "corpusId":
      2292730, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/993f0793ca7217e03afbd29346d03c01109acc49",
      "title": "Joint Discovery of Object States and Manipulation Actions", "abstract":
      "Many human activities involve object manipulations aiming to modify the object
      state. Examples of common state changes include full/empty bottle, open/closed
      door, and attached/detached car wheel. In this work, we seek to automatically
      discover the states of objects and the associated manipulation actions. Given
      a set of videos for a particular task, we propose a joint model that learns
      to identify object states and to localize state-modifying actions. Our model
      is formulated as a discriminative clustering cost with constraints. We assume
      a consistent temporal order for the changes in object states and manipulation
      actions, and introduce new optimization techniques to learn model parameters
      without additional supervision. We demonstrate successful discovery of seven
      manipulation actions and corresponding object states on a new dataset of videos
      depicting real-life object manipulations. We show that our joint formulation
      results in an improvement of object state discovery by action recognition and
      vice versa.", "venue": "IEEE International Conference on Computer Vision", "year":
      2017, "referenceCount": 47, "citationCount": 65, "influentialCitationCount":
      5, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1702.02738",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-02-09", "journal": {"pages": "2146-2155", "name": "2017
      IEEE International Conference on Computer Vision (ICCV)"}, "citationStyles":
      {"bibtex": "@Article{Alayrac2017JointDO,\n author = {Jean-Baptiste Alayrac and
      Josef Sivic and I. Laptev and Simon Lacoste-Julien},\n booktitle = {IEEE International
      Conference on Computer Vision},\n journal = {2017 IEEE International Conference
      on Computer Vision (ICCV)},\n pages = {2146-2155},\n title = {Joint Discovery
      of Object States and Manipulation Actions},\n year = {2017}\n}\n"}, "authors":
      [{"authorId": "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId": "1782755",
      "name": "Josef Sivic"}, {"authorId": "143991676", "name": "I. Laptev"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "633f478bf07ee5fbee9388df84b07c87cc190ae9", "externalIds": {"MAG": "2572438701",
      "ArXiv": "1702.00403", "DBLP": "journals/corr/SchawinskiZZFS17", "DOI": "10.1093/mnrasl/slx008",
      "CorpusId": 7213940}, "corpusId": 7213940, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/633f478bf07ee5fbee9388df84b07c87cc190ae9",
      "title": "Generative Adversarial Networks recover features in astrophysical
      images of galaxies beyond the deconvolution limit", "abstract": "Observations
      of astrophysical objects such as galaxies are limited by various sources of
      random and systematic noise from the sky background, the optical system of the
      telescope and the detector used to record the data. Conventional deconvolution
      techniques are limited in their ability to recover features in imaging data
      by the Shannon-Nyquist sampling theorem. Here we train a generative adversarial
      network (GAN) on a sample of $4,550$ images of nearby galaxies at $0.01<z<0.02$
      from the Sloan Digital Sky Survey and conduct $10\\times$ cross validation to
      evaluate the results. We present a method using a GAN trained on galaxy images
      that can recover features from artificially degraded images with worse seeing
      and higher noise than the original with a performance which far exceeds simple
      deconvolution. The ability to better recover detailed features such as galaxy
      morphology from low-signal-to-noise and low angular resolution imaging data
      significantly increases our ability to study existing data sets of astrophysical
      objects as well as future observations with observatories such as the Large
      Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.",
      "venue": "arXiv.org", "year": 2017, "referenceCount": 11, "citationCount": 162,
      "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://academic.oup.com/mnrasl/article-pdf/467/1/L110/10730451/slx008.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Physics", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Physics", "source": "s2-fos-model"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Review"], "publicationDate": "2017-02-01", "journal": {"volume": "abs/1702.00403",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Schawinski2017GenerativeAN,\n
      author = {K. Schawinski and Ce Zhang and Hantian Zhang and Lucas Fowler and
      G. Santhanam},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generative
      Adversarial Networks recover features in astrophysical images of galaxies beyond
      the deconvolution limit},\n volume = {abs/1702.00403},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "6552704", "name": "K. Schawinski"}, {"authorId": "1776014",
      "name": "Ce Zhang"}, {"authorId": "2016429687", "name": "Hantian Zhang"}, {"authorId":
      "2105910294", "name": "Lucas Fowler"}, {"authorId": "9540353", "name": "G. Santhanam"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "2f85b7376769473d2bed56f855f115e23d727094", "externalIds":
      {"ArXiv": "1701.07875", "DBLP": "journals/corr/ArjovskyCB17", "CorpusId": 13943041},
      "corpusId": 13943041, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/2f85b7376769473d2bed56f855f115e23d727094",
      "title": "Wasserstein GAN", "abstract": "We introduce a new algorithm named
      WGAN, an alternative to traditional GAN training. In this new model, we show
      that we can improve the stability of learning, get rid of problems like mode
      collapse, and provide meaningful learning curves useful for debugging and hyperparameter
      searches. Furthermore, we show that the corresponding optimization problem is
      sound, and provide extensive theoretical work highlighting the deep connections
      to other distances between distributions.", "venue": "arXiv.org", "year": 2017,
      "referenceCount": 26, "citationCount": 3929, "influentialCitationCount": 732,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-01-26", "journal": {"volume": "abs/1701.07875", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Arjovsky2017WassersteinG,\n author = {Mart\u00edn Arjovsky
      and Soumith Chintala and L\u00e9on Bottou},\n booktitle = {arXiv.org},\n journal
      = {ArXiv},\n title = {Wasserstein GAN},\n volume = {abs/1701.07875},\n year
      = {2017}\n}\n"}, "authors": [{"authorId": "2066311389", "name": "Mart\u00edn
      Arjovsky"}, {"authorId": "2066272844", "name": "Soumith Chintala"}, {"authorId":
      "2065210987", "name": "L\u00e9on Bottou"}]}}, {"contexts": [], "intents": [],
      "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "externalIds": {"ArXiv": "1701.06547",
      "ACL": "D17-1230", "DBLP": "journals/corr/LiMSRJ17", "MAG": "2951520714", "DOI":
      "10.18653/v1/D17-1230", "CorpusId": 98180}, "corpusId": 98180, "publicationVenue":
      {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical
      Methods in Natural Language Processing", "type": "conference", "alternate_names":
      ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
      "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
      "url": "https://www.semanticscholar.org/paper/176f1d608b918eec8dc4b75e7b6e0acaba84a447",
      "title": "Adversarial Learning for Neural Dialogue Generation", "abstract":
      "We apply adversarial training to open-domain dialogue generation, training
      a system to produce sequences that are indistinguishable from human-generated
      dialogue utterances. We cast the task as a reinforcement learning problem where
      we jointly train two systems: a generative model to produce response sequences,
      and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014
      to distinguish between the human-generated dialogues and the machine-generated
      ones. In this generative adversarial network approach, the outputs from the
      discriminator are used to encourage the system towards more human-like dialogue.
      Further, we investigate models for adversarial evaluation that uses success
      in fooling an adversary as a dialogue evaluation metric, while avoiding a number
      of potential pitfalls. Experimental results on several metrics, including adversarial
      evaluation, demonstrate that the adversarially-trained system generates higher-quality
      responses than previous baselines", "venue": "Conference on Empirical Methods
      in Natural Language Processing", "year": 2017, "referenceCount": 52, "citationCount":
      863, "influentialCitationCount": 116, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://www.aclweb.org/anthology/D17-1230.pdf", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-01-23", "journal":
      {"volume": "abs/1701.06547", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Li2017AdversarialLF,\n author = {Jiwei Li and Will Monroe and Tianlin
      Shi and S\u00e9bastien Jean and Alan Ritter and Dan Jurafsky},\n booktitle =
      {Conference on Empirical Methods in Natural Language Processing},\n journal
      = {ArXiv},\n title = {Adversarial Learning for Neural Dialogue Generation},\n
      volume = {abs/1701.06547},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "49298465", "name": "Jiwei Li"}, {"authorId": "145768639", "name": "Will Monroe"},
      {"authorId": "10238549", "name": "Tianlin Shi"}, {"authorId": "152857609", "name":
      "S\u00e9bastien Jean"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId":
      "1746807", "name": "Dan Jurafsky"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
      "externalIds": {"MAG": "2581875816", "ArXiv": "1701.05927", "DOI": "10.1007/s41781-017-0004-6",
      "CorpusId": 88514467}, "corpusId": 88514467, "publicationVenue": {"id": "3a6a506a-7577-4f58-be4a-b4f786c2b892",
      "name": "Computing and Software for Big Science", "alternate_names": ["Comput
      Softw Big Sci"], "issn": "2510-2044", "url": "https://link.springer.com/journal/41781"},
      "url": "https://www.semanticscholar.org/paper/7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
      "title": "Learning Particle Physics by Example: Location-Aware Generative Adversarial
      Networks for Physics Synthesis", "abstract": null, "venue": "Computing and Software
      for Big Science", "year": 2017, "referenceCount": 46, "citationCount": 261,
      "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1701.05927", "status": null}, "fieldsOfStudy": ["Physics",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Physics", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Physics", "source":
      "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": "2017-01-20", "journal": {"volume":
      "1", "name": "Computing and Software for Big Science"}, "citationStyles": {"bibtex":
      "@Article{Oliveira2017LearningPP,\n author = {Luke de Oliveira and Michela Paganini
      and B. Nachman},\n booktitle = {Computing and Software for Big Science},\n journal
      = {Computing and Software for Big Science},\n title = {Learning Particle Physics
      by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis},\n
      volume = {1},\n year = {2017}\n}\n"}, "authors": [{"authorId": "8972778", "name":
      "Luke de Oliveira"}, {"authorId": "35550664", "name": "Michela Paganini"}, {"authorId":
      "3085579", "name": "B. Nachman"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "29831b8830e278c8c28e45c8e9c41c619c89f86a",
      "externalIds": {"DBLP": "conf/icml/MeschederNG17", "MAG": "2579277680", "ArXiv":
      "1701.04722", "CorpusId": 605416}, "corpusId": 605416, "publicationVenue": {"id":
      "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference on
      Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int Conf
      Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/29831b8830e278c8c28e45c8e9c41c619c89f86a",
      "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and
      Generative Adversarial Networks", "abstract": "Variational Autoencoders (VAEs)
      are expressive latent variable models that can be used to learn complex probability
      distributions from training data. However, the quality of the resulting model
      crucially relies on the expressiveness of the inference model. We introduce
      Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders
      with arbitrarily expressive inference models. We achieve this by introducing
      an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem
      as a two-player game, hence establishing a principled connection between VAEs
      and Generative Adversarial Networks (GANs). We show that in the nonparametric
      limit our method yields an exact maximum-likelihood assignment for the parameters
      of the generative model, as well as the exact posterior distribution over the
      latent variables given an observation. Contrary to competing approaches which
      combine VAEs with GANs, our approach has a clear theoretical justification,
      retains most advantages of standard Variational Autoencoders and is easy to
      implement.", "venue": "International Conference on Machine Learning", "year":
      2017, "referenceCount": 46, "citationCount": 484, "influentialCitationCount":
      63, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2017-01-17", "journal": {"volume": "abs/1701.04722", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Mescheder2017AdversarialVB,\n author = {L. Mescheder and
      Sebastian Nowozin and Andreas Geiger},\n booktitle = {International Conference
      on Machine Learning},\n journal = {ArXiv},\n title = {Adversarial Variational
      Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},\n
      volume = {abs/1701.04722},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "8226549", "name": "L. Mescheder"}, {"authorId": "2388416", "name": "Sebastian
      Nowozin"}, {"authorId": "47237027", "name": "Andreas Geiger"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "68cb9fce1e6af2740377494350b650533c9a29e1", "externalIds": {"MAG":
      "2567101557", "DBLP": "conf/cvpr/ShrivastavaPTSW17", "ArXiv": "1612.07828",
      "DOI": "10.1109/CVPR.2017.241", "CorpusId": 8229065}, "corpusId": 8229065, "publicationVenue":
      {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
      Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
      Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/68cb9fce1e6af2740377494350b650533c9a29e1",
      "title": "Learning from Simulated and Unsupervised Images through Adversarial
      Training", "abstract": "With recent progress in graphics, it has become more
      tractable to train models on synthetic images, potentially avoiding the need
      for expensive annotations. However, learning from synthetic images may not achieve
      the desired performance due to a gap between synthetic and real image distributions.
      To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where
      the task is to learn a model to improve the realism of a simulators output using
      unlabeled real data, while preserving the annotation information from the simulator.
      We develop a method for S+U learning that uses an adversarial network similar
      to Generative Adversarial Networks (GANs), but with synthetic images as inputs
      instead of random vectors. We make several key modifications to the standard
      GAN algorithm to preserve annotations, avoid artifacts, and stabilize training:
      (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating
      the discriminator using a history of refined images. We show that this enables
      generation of highly realistic images, which we demonstrate both qualitatively
      and with a user study. We quantitatively evaluate the generated images by training
      models for gaze estimation and hand pose estimation. We show a significant improvement
      over using synthetic images, and achieve state-of-the-art results on the MPIIGaze
      dataset without any labeled real data.", "venue": "Computer Vision and Pattern
      Recognition", "year": 2016, "referenceCount": 52, "citationCount": 1685, "influentialCitationCount":
      128, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1612.07828",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-12-22", "journal": {"pages": "2242-2251", "name": "2017
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "citationStyles":
      {"bibtex": "@Article{Shrivastava2016LearningFS,\n author = {A. Shrivastava and
      Tomas Pfister and Oncel Tuzel and J. Susskind and Wenda Wang and Russ Webb},\n
      booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE
      Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2242-2251},\n
      title = {Learning from Simulated and Unsupervised Images through Adversarial
      Training},\n year = {2016}\n}\n"}, "authors": [{"authorId": "1490900960", "name":
      "A. Shrivastava"}, {"authorId": "1945962", "name": "Tomas Pfister"}, {"authorId":
      "2577513", "name": "Oncel Tuzel"}, {"authorId": "49158771", "name": "J. Susskind"},
      {"authorId": "2108465550", "name": "Wenda Wang"}, {"authorId": "51138986", "name":
      "Russ Webb"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "488bb25e0b1777847f04c943e6dbc4f84415b712",
      "externalIds": {"DBLP": "journals/corr/MetzPPS16", "ArXiv": "1611.02163", "MAG":
      "2554314924", "CorpusId": 6610705}, "corpusId": 6610705, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/488bb25e0b1777847f04c943e6dbc4f84415b712",
      "title": "Unrolled Generative Adversarial Networks", "abstract": "We introduce
      a method to stabilize Generative Adversarial Networks (GANs) by defining the
      generator objective with respect to an unrolled optimization of the discriminator.
      This allows training to be adjusted between using the optimal discriminator
      in the generator''s objective, which is ideal but infeasible in practice, and
      using the current value of the discriminator, which is often unstable and leads
      to poor solutions. We show how this technique solves the common problem of mode
      collapse, stabilizes training of GANs with complex recurrent generators, and
      increases diversity and coverage of the data distribution by the generator.",
      "venue": "International Conference on Learning Representations", "year": 2016,
      "referenceCount": 55, "citationCount": 908, "influentialCitationCount": 118,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2016-11-04", "journal": {"volume": "abs/1611.02163", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Metz2016UnrolledGA,\n author = {Luke Metz and Ben Poole
      and David Pfau and Jascha Narain Sohl-Dickstein},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unrolled
      Generative Adversarial Networks},\n volume = {abs/1611.02163},\n year = {2016}\n}\n"},
      "authors": [{"authorId": "2096458", "name": "Luke Metz"}, {"authorId": "16443937",
      "name": "Ben Poole"}, {"authorId": "144846367", "name": "David Pfau"}, {"authorId":
      "1407546424", "name": "Jascha Narain Sohl-Dickstein"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "858dc7408c27702ec42778599fc8d11f73ef3f76", "externalIds": {"ArXiv": "1611.04273",
      "MAG": "2953116488", "DBLP": "conf/iclr/WuBSG17", "CorpusId": 13583585}, "corpusId":
      13583585, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/858dc7408c27702ec42778599fc8d11f73ef3f76",
      "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
      "abstract": "The past several years have seen remarkable progress in generative
      models which produce convincing samples of images and other modalities. A shared
      component of many powerful generative models is a decoder network, a parametric
      deep neural net that defines a generative distribution. Examples include variational
      autoencoders, generative adversarial networks, and generative moment matching
      networks. Unfortunately, it can be difficult to quantify the performance of
      these models because of the intractability of log-likelihood estimation, and
      inspecting samples can be misleading. We propose to use Annealed Importance
      Sampling for evaluating log-likelihoods for decoder-based models and validate
      its accuracy using bidirectional Monte Carlo. The evaluation code is provided
      at this https URL. Using this technique, we analyze the performance of decoder-based
      models, the effectiveness of existing log-likelihood estimators, the degree
      of overfitting, and the degree to which these models miss important modes of
      the data distribution.", "venue": "International Conference on Learning Representations",
      "year": 2016, "referenceCount": 38, "citationCount": 216, "influentialCitationCount":
      28, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2016-11-04", "journal": {"volume": "abs/1611.04273",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Wu2016OnTQ,\n author
      = {Yuhuai Wu and Yuri Burda and R. Salakhutdinov and R. Grosse},\n booktitle
      = {International Conference on Learning Representations},\n journal = {ArXiv},\n
      title = {On the Quantitative Analysis of Decoder-Based Generative Models},\n
      volume = {abs/1611.04273},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "3374063", "name": "Yuhuai Wu"}, {"authorId": "3080409", "name": "Yuri Burda"},
      {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1785346",
      "name": "R. Grosse"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "externalIds": {"ArXiv": "1610.09585", "MAG": "2548275288", "DBLP": "conf/icml/OdenaOS17",
      "CorpusId": 1099052}, "corpusId": 1099052, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "title": "Conditional Image Synthesis with Auxiliary Classifier GANs", "abstract":
      "In this paper we introduce new methods for the improved training of generative
      adversarial networks (GANs) for image synthesis. We construct a variant of GANs
      employing label conditioning that results in 128 x 128 resolution image samples
      exhibiting global coherence. We expand on previous work for image quality assessment
      to provide two new analyses for assessing the discriminability and diversity
      of samples from class-conditional image synthesis models. These analyses demonstrate
      that high resolution samples provide class information not present in low resolution
      samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice
      as discriminable as artificially resized 32 x 32 samples. In addition, 84.7%
      of the classes have samples exhibiting diversity comparable to real ImageNet
      data.", "venue": "International Conference on Machine Learning", "year": 2016,
      "referenceCount": 44, "citationCount": 2804, "influentialCitationCount": 426,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-10-30", "journal": {"pages": "2642-2651"},
      "citationStyles": {"bibtex": "@Article{Odena2016ConditionalIS,\n author = {Augustus
      Odena and C. Olah and Jonathon Shlens},\n booktitle = {International Conference
      on Machine Learning},\n pages = {2642-2651},\n title = {Conditional Image Synthesis
      with Auxiliary Classifier GANs},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "2624088", "name": "Augustus Odena"}, {"authorId": "37232298", "name": "C. Olah"},
      {"authorId": "1789737", "name": "Jonathon Shlens"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "f10ac91e32f162ea2d73372ae1d8b5d61d0c3294", "externalIds": {"ArXiv": "1610.07797",
      "MAG": "2543937159", "DBLP": "conf/aistats/GidelJL17", "CorpusId": 3949784},
      "corpusId": 3949784, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/f10ac91e32f162ea2d73372ae1d8b5d61d0c3294",
      "title": "Frank-Wolfe Algorithms for Saddle Point Problems", "abstract": "We
      extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth
      convex-concave saddle point (SP) problems. Remarkably, the method only requires
      access to linear minimization oracles. Leveraging recent advances in FW optimization,
      we provide the first proof of convergence of a FW-type saddle point solver over
      polytopes, thereby partially answering a 30 year-old conjecture. We also survey
      other convergence results and highlight gaps in the theoretical underpinnings
      of FW-style algorithms. Motivating applications without known efficient alternatives
      are explored through structured prediction with combinatorial penalties as well
      as games over matching polytopes involving an exponential number of constraints.",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "year": 2016, "referenceCount": 53, "citationCount": 62, "influentialCitationCount":
      6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate":
      "2016-10-25", "journal": {"pages": "362-371"}, "citationStyles": {"bibtex":
      "@Article{Gidel2016FrankWolfeAF,\n author = {Gauthier Gidel and T. Jebara and
      Simon Lacoste-Julien},\n booktitle = {International Conference on Artificial
      Intelligence and Statistics},\n pages = {362-371},\n title = {Frank-Wolfe Algorithms
      for Saddle Point Problems},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "8150760", "name": "Gauthier Gidel"}, {"authorId": "1768120", "name": "T. Jebara"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd", "externalIds": {"ArXiv":
      "1609.05473", "DBLP": "conf/aaai/YuZWY17", "MAG": "2964268978", "DOI": "10.1609/aaai.v31i1.10804",
      "CorpusId": 3439214}, "corpusId": 3439214, "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence", "type": "conference",
      "alternate_names": ["National Conference on Artificial Intelligence", "National
      Conf Artif Intell", "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"},
      "url": "https://www.semanticscholar.org/paper/32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
      "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
      "abstract": "\n \n As a new way of training generative models, Generative Adversarial
      Net (GAN) that uses a discriminative model to guide the training of the generative
      model has enjoyed considerable success in generating real-valued data. However,
      it has limitations when the goal is for generating sequences of discrete tokens.
      A major reason lies in that the discrete outputs from the generative model make
      it difficult to pass the gradient update from the discriminative model to the
      generative model. Also, the discriminative model can only assess a complete
      sequence, while for a partially generated sequence, it is non-trivial to balance
      its current score and the future one once the entire sequence has been generated.
      In this paper, we propose a sequence generation framework, called SeqGAN, to
      solve the problems. Modeling the data generator as a stochastic policy in reinforcement
      learning (RL), SeqGAN bypasses the generator differentiation problem by directly
      performing gradient policy update. The RL reward signal comes from the GAN discriminator
      judged on a complete sequence, and is passed back to the intermediate state-action
      steps using Monte Carlo search. Extensive experiments on synthetic data and
      real-world tasks demonstrate significant improvements over strong baselines.\n
      \n", "venue": "AAAI Conference on Artificial Intelligence", "year": 2016, "referenceCount":
      41, "citationCount": 2075, "influentialCitationCount": 278, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/10804/10663",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-09-18",
      "journal": {"pages": "2852-2858"}, "citationStyles": {"bibtex": "@Article{Yu2016SeqGANSG,\n
      author = {Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},\n booktitle
      = {AAAI Conference on Artificial Intelligence},\n pages = {2852-2858},\n title
      = {SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient},\n year
      = {2016}\n}\n"}, "authors": [{"authorId": "3469209", "name": "Lantao Yu"}, {"authorId":
      "2108309275", "name": "Weinan Zhang"}, {"authorId": "39055225", "name": "Jun
      Wang"}, {"authorId": "1811427", "name": "Yong Yu"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3", "externalIds": {"DBLP": "conf/cvpr/LedigTHCCAATTWS17",
      "MAG": "2963470893", "ArXiv": "1609.04802", "DOI": "10.1109/CVPR.2017.19", "CorpusId":
      211227}, "corpusId": 211227, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
      "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
      Network", "abstract": "Despite the breakthroughs in accuracy and speed of single
      image super-resolution using faster and deeper convolutional neural networks,
      one central problem remains largely unsolved: how do we recover the finer texture
      details when we super-resolve at large upscaling factors? The behavior of optimization-based
      super-resolution methods is principally driven by the choice of the objective
      function. Recent work has largely focused on minimizing the mean squared reconstruction
      error. The resulting estimates have high peak signal-to-noise ratios, but they
      are often lacking high-frequency details and are perceptually unsatisfying in
      the sense that they fail to match the fidelity expected at the higher resolution.
      In this paper, we present SRGAN, a generative adversarial network (GAN) for
      image super-resolution (SR). To our knowledge, it is the first framework capable
      of inferring photo-realistic natural images for 4x upscaling factors. To achieve
      this, we propose a perceptual loss function which consists of an adversarial
      loss and a content loss. The adversarial loss pushes our solution to the natural
      image manifold using a discriminator network that is trained to differentiate
      between the super-resolved images and original photo-realistic images. In addition,
      we use a content loss motivated by perceptual similarity instead of similarity
      in pixel space. Our deep residual network is able to recover photo-realistic
      textures from heavily downsampled images on public benchmarks. An extensive
      mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality
      using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original
      high-resolution images than to those obtained with any state-of-the-art method.",
      "venue": "Computer Vision and Pattern Recognition", "year": 2016, "referenceCount":
      77, "citationCount": 8835, "influentialCitationCount": 1193, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1609.04802", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-09-15",
      "journal": {"pages": "105-114", "name": "2017 IEEE Conference on Computer Vision
      and Pattern Recognition (CVPR)"}, "citationStyles": {"bibtex": "@Article{Ledig2016PhotoRealisticSI,\n
      author = {C. Ledig and Lucas Theis and Ferenc Husz\u00e1r and Jose Caballero
      and Andrew P. Aitken and A. Tejani and J. Totz and Zehan Wang and Wenzhe Shi},\n
      booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE
      Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {105-114},\n
      title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
      Network},\n year = {2016}\n}\n"}, "authors": [{"authorId": "1779917", "name":
      "C. Ledig"}, {"authorId": "2073063", "name": "Lucas Theis"}, {"authorId": "3108066",
      "name": "Ferenc Husz\u00e1r"}, {"authorId": "145372820", "name": "Jose Caballero"},
      {"authorId": "49931957", "name": "Andrew P. Aitken"}, {"authorId": "41203992",
      "name": "A. Tejani"}, {"authorId": "1853456", "name": "J. Totz"}, {"authorId":
      "34627233", "name": "Zehan Wang"}, {"authorId": "46810836", "name": "Wenzhe
      Shi"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
      "externalIds": {"MAG": "2520707650", "ArXiv": "1609.02612", "DBLP": "conf/nips/VondrickPT16",
      "DOI": "10.13016/M26GIH-TNYZ", "CorpusId": 9933254}, "corpusId": 9933254, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
      "title": "Generating Videos with Scene Dynamics", "abstract": "We capitalize
      on large amounts of unlabeled video in order to learn a model of scene dynamics
      for both video recognition tasks (e.g. action classification) and video generation
      tasks (e.g. future prediction). We propose a generative adversarial network
      for video with a spatio-temporal convolutional architecture that untangles the
      scene''s foreground from the background. Experiments suggest this model can
      generate tiny videos up to a second at full frame rate better than simple baselines,
      and we show its utility at predicting plausible futures of static images. Moreover,
      experiments and visualizations show the model internally learns useful features
      for recognizing actions with minimal supervision, suggesting scene dynamics
      are a promising signal for representation learning. We believe generative video
      models can impact many applications in video understanding and simulation.",
      "venue": "Neural Information Processing Systems", "year": 2016, "referenceCount":
      60, "citationCount": 1328, "influentialCitationCount": 145, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-09-08", "journal": {"pages": "613-621"},
      "citationStyles": {"bibtex": "@Article{Vondrick2016GeneratingVW,\n author =
      {Carl Vondrick and H. Pirsiavash and A. Torralba},\n booktitle = {Neural Information
      Processing Systems},\n pages = {613-621},\n title = {Generating Videos with
      Scene Dynamics},\n year = {2016}\n}\n"}, "authors": [{"authorId": "1856025",
      "name": "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"},
      {"authorId": "143805211", "name": "A. Torralba"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "8a3bf4d403a39ed33f0fa8cf78dc906d6130595f", "externalIds": {"DBLP": "conf/cvpr/Yeh0LSHD17",
      "MAG": "2963917315", "ArXiv": "1607.07539", "DOI": "10.1109/CVPR.2017.728",
      "CorpusId": 24005817}, "corpusId": 24005817, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/8a3bf4d403a39ed33f0fa8cf78dc906d6130595f",
      "title": "Semantic Image Inpainting with Deep Generative Models", "abstract":
      "Semantic image inpainting is a challenging task where large missing regions
      have to be filled based on the available visual data. Existing methods which
      extract information from only a single image generally produce unsatisfactory
      results due to the lack of high level context. In this paper, we propose a novel
      method for semantic image inpainting, which generates the missing content by
      conditioning on the available data. Given a trained generative model, we search
      for the closest encoding of the corrupted image in the latent image manifold
      using our context and prior losses. This encoding is then passed through the
      generative model to infer the missing content. In our method, inference is possible
      irrespective of how the missing content is structured, while the state-of-the-art
      learning based method requires specific information about the holes in the training
      phase. Experiments on three datasets show that our method successfully predicts
      information in large missing regions and achieves pixel-level photorealism,
      significantly outperforming the state-of-the-art methods.", "venue": "Computer
      Vision and Pattern Recognition", "year": 2016, "referenceCount": 40, "citationCount":
      1096, "influentialCitationCount": 80, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/1607.07539", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2016-07-26", "journal":
      {"pages": "6882-6890", "name": "2017 IEEE Conference on Computer Vision and
      Pattern Recognition (CVPR)"}, "citationStyles": {"bibtex": "@Article{Yeh2016SemanticII,\n
      author = {Raymond A. Yeh and Chen Chen and Teck-Yian Lim and A. Schwing and
      M. Hasegawa-Johnson and M. Do},\n booktitle = {Computer Vision and Pattern Recognition},\n
      journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n
      pages = {6882-6890},\n title = {Semantic Image Inpainting with Deep Generative
      Models},\n year = {2016}\n}\n"}, "authors": [{"authorId": "28919105", "name":
      "Raymond A. Yeh"}, {"authorId": null, "name": "Chen Chen"}, {"authorId": "33494814",
      "name": "Teck-Yian Lim"}, {"authorId": "2068227", "name": "A. Schwing"}, {"authorId":
      "1399115926", "name": "M. Hasegawa-Johnson"}, {"authorId": "1834451", "name":
      "M. Do"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "a1b3d8a94323122d63a1ec31c1d722d30c509cb4",
      "externalIds": {"MAG": "2479644247", "DBLP": "journals/corr/YehCLHD16", "CorpusId":
      15140030}, "corpusId": 15140030, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/a1b3d8a94323122d63a1ec31c1d722d30c509cb4",
      "title": "Semantic Image Inpainting with Perceptual and Contextual Losses",
      "abstract": "In this paper, we propose a novel method for image inpainting based
      on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a
      loss function consisting of two parts: (1) a contextual loss that preserves
      similarity between the input corrupted image and the recovered image, and (2)
      a perceptual loss that ensures a perceptually realistic output image. Given
      a corrupted image with missing values, we use back-propagation on this loss
      to map the corrupted image to a smaller latent space. The mapped vector is then
      passed through the generative model to predict the missing content. The proposed
      framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting
      tasks with random 80% corruption and large blocky corruption. Experiments show
      that our method can successfully predict semantic information in the missing
      region and achieve pixel-level photorealism, which is impossible by almost all
      existing methods.", "venue": "arXiv.org", "year": 2016, "referenceCount": 26,
      "citationCount": 353, "influentialCitationCount": 34, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-07-26", "journal": {"volume": "abs/1607.07539", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Yeh2016SemanticII,\n author
      = {Raymond A. Yeh and Chen Chen and Teck-Yian Lim and M. Hasegawa-Johnson and
      M. Do},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Semantic
      Image Inpainting with Perceptual and Contextual Losses},\n volume = {abs/1607.07539},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "28919105", "name": "Raymond
      A. Yeh"}, {"authorId": null, "name": "Chen Chen"}, {"authorId": "33494814",
      "name": "Teck-Yian Lim"}, {"authorId": "1399115926", "name": "M. Hasegawa-Johnson"},
      {"authorId": "1834451", "name": "M. Do"}]}}, {"contexts": [], "intents": [],
      "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "372bc106c61e7eb004835e85bbfee997409f176a", "externalIds": {"MAG": "2950919267",
      "DBLP": "journals/corr/0001T16", "ArXiv": "1606.07536", "CorpusId": 10627900},
      "corpusId": 10627900, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/372bc106c61e7eb004835e85bbfee997409f176a",
      "title": "Coupled Generative Adversarial Networks", "abstract": "We propose
      coupled generative adversarial network (CoGAN) for learning a joint distribution
      of multi-domain images. In contrast to the existing approaches, which require
      tuples of corresponding images in different domains in the training set, CoGAN
      can learn a joint distribution without any tuple of corresponding images. It
      can learn a joint distribution with just samples drawn from the marginal distributions.
      This is achieved by enforcing a weight-sharing constraint that limits the network
      capacity and favors a joint distribution solution over a product of marginal
      distributions one. We apply CoGAN to several joint distribution learning tasks,
      including learning a joint distribution of color and depth images, and learning
      a joint distribution of face images with different attributes. For each task
      it successfully learns the joint distribution without any tuple of corresponding
      images. We also demonstrate its applications to domain adaptation and image
      transformation.", "venue": "Neural Information Processing Systems", "year":
      2016, "referenceCount": 33, "citationCount": 1490, "influentialCitationCount":
      146, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2016-06-24", "journal":
      {"pages": "469-477"}, "citationStyles": {"bibtex": "@Article{Liu2016CoupledGA,\n
      author = {Ming-Yu Liu and Oncel Tuzel},\n booktitle = {Neural Information Processing
      Systems},\n pages = {469-477},\n title = {Coupled Generative Adversarial Networks},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "39793900", "name": "Ming-Yu
      Liu"}, {"authorId": "2577513", "name": "Oncel Tuzel"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "eb7ee0bc355652654990bcf9f92f124688fde493", "externalIds": {"MAG": "2963226019",
      "ArXiv": "1606.03657", "DBLP": "journals/corr/ChenDHSSA16", "CorpusId": 5002792},
      "corpusId": 5002792, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/eb7ee0bc355652654990bcf9f92f124688fde493",
      "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing
      Generative Adversarial Nets", "abstract": "This paper describes InfoGAN, an
      information-theoretic extension to the Generative Adversarial Network that is
      able to learn disentangled representations in a completely unsupervised manner.
      InfoGAN is a generative adversarial network that also maximizes the mutual information
      between a small subset of the latent variables and the observation. We derive
      a lower bound to the mutual information objective that can be optimized efficiently,
      and show that our training procedure can be interpreted as a variation of the
      Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing
      styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered
      images, and background digits from the central digit on the SVHN dataset. It
      also discovers visual concepts that include hair styles, presence/absence of
      eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN
      learns interpretable representations that are competitive with representations
      learned by existing fully supervised methods.", "venue": "Neural Information
      Processing Systems", "year": 2016, "referenceCount": 42, "citationCount": 3809,
      "influentialCitationCount": 435, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2016-06-12", "journal":
      {"pages": "2172-2180"}, "citationStyles": {"bibtex": "@Article{Chen2016InfoGANIR,\n
      author = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and I. Sutskever
      and P. Abbeel},\n booktitle = {Neural Information Processing Systems},\n pages
      = {2172-2180},\n title = {InfoGAN: Interpretable Representation Learning by
      Information Maximizing Generative Adversarial Nets},\n year = {2016}\n}\n"},
      "authors": [{"authorId": "41192764", "name": "Xi Chen"}, {"authorId": "144581158",
      "name": "Yan Duan"}, {"authorId": "3127100", "name": "Rein Houthooft"}, {"authorId":
      "47971768", "name": "John Schulman"}, {"authorId": "1701686", "name": "I. Sutskever"},
      {"authorId": "1689992", "name": "P. Abbeel"}]}}, {"contexts": ["34 GANs have
      proven very effective for learning to classify data using very few labeled training
      examples.(29) Evaluating the performance of generative models including GANs
      is a difficult research area in its own right."], "intents": [], "contextsWithIntent":
      [{"context": "34 GANs have proven very effective for learning to classify data
      using very few labeled training examples.(29) Evaluating the performance of
      generative models including GANs is a difficult research area in its own right.",
      "intents": []}], "isInfluential": false, "citedPaper": {"paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
      "externalIds": {"DBLP": "conf/nips/SalimansGZCRCC16", "ArXiv": "1606.03498",
      "MAG": "2949938177", "CorpusId": 1687220}, "corpusId": 1687220, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea",
      "title": "Improved Techniques for Training GANs", "abstract": "We present a
      variety of new architectural features and training procedures that we apply
      to the generative adversarial networks (GANs) framework. We focus on two applications
      of GANs: semi-supervised learning, and the generation of images that humans
      find visually realistic. Unlike most work on generative models, our primary
      goal is not to train a model that assigns high likelihood to test data, nor
      do we require the model to be able to learn well without using any labels. Using
      our new techniques, we achieve state-of-the-art results in semi-supervised classification
      on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed
      by a visual Turing test: our model generates MNIST samples that humans cannot
      distinguish from real data, and CIFAR-10 samples that yield a human error rate
      of 21.3%. We also present ImageNet samples with unprecedented resolution and
      show that our methods enable the model to learn recognizable features of ImageNet
      classes.", "venue": "Neural Information Processing Systems", "year": 2016, "referenceCount":
      28, "citationCount": 7442, "influentialCitationCount": 884, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-06-10", "journal": {"volume": "abs/1606.03498", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Salimans2016ImprovedTF,\n author
      = {Tim Salimans and I. Goodfellow and Wojciech Zaremba and Vicki Cheung and
      Alec Radford and Xi Chen},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {Improved Techniques for Training GANs},\n volume
      = {abs/1606.03498},\n year = {2016}\n}\n"}, "authors": [{"authorId": "2887364",
      "name": "Tim Salimans"}, {"authorId": "153440022", "name": "I. Goodfellow"},
      {"authorId": "2563432", "name": "Wojciech Zaremba"}, {"authorId": "34415167",
      "name": "Vicki Cheung"}, {"authorId": "38909097", "name": "Alec Radford"}, {"authorId":
      "41192764", "name": "Xi Chen"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
      "externalIds": {"ArXiv": "1606.00709", "DBLP": "journals/corr/NowozinCT16",
      "MAG": "2963800509", "CorpusId": 107645}, "corpusId": 107645, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/ffdcad14d2f6a12f607b59f88da4a939f4821691",
      "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence
      Minimization", "abstract": "Generative neural samplers are probabilistic models
      that implement sampling using feedforward neural networks: they take a random
      input vector and produce a sample from a probability distribution defined by
      the network weights. These models are expressive and allow efficient computation
      of samples and derivatives, but cannot be used for computing likelihoods or
      for marginalization. The generative-adversarial training method allows to train
      such models through the use of an auxiliary discriminative neural network. We
      show that the generative-adversarial approach is a special case of an existing
      more general variational divergence estimation approach. We show that any f-divergence
      can be used for training generative neural samplers. We discuss the benefits
      of various choices of divergence functions on training complexity and the quality
      of the obtained generative models.", "venue": "Neural Information Processing
      Systems", "year": 2016, "referenceCount": 39, "citationCount": 1483, "influentialCitationCount":
      177, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-06-02", "journal": {"volume": "abs/1606.00709", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Nowozin2016fGANTG,\n author = {Sebastian Nowozin and Botond
      Cseke and Ryota Tomioka},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {f-GAN: Training Generative Neural Samplers using
      Variational Divergence Minimization},\n volume = {abs/1606.00709},\n year =
      {2016}\n}\n"}, "authors": [{"authorId": "2388416", "name": "Sebastian Nowozin"},
      {"authorId": "2084925", "name": "Botond Cseke"}, {"authorId": "2870603", "name":
      "Ryota Tomioka"}]}}, {"contexts": ["Donahue et al. (2016) Encoder models P(Z
      | X ) Decoder models P(X | Z ) Discriminator takes a pair of x and z ."], "intents":
      [], "contextsWithIntent": [{"context": "Donahue et al. (2016) Encoder models
      P(Z | X ) Decoder models P(X | Z ) Discriminator takes a pair of x and z .",
      "intents": []}], "isInfluential": false, "citedPaper": {"paperId": "1db6e3078597386ac4222ba6c3f4f61b61f53539",
      "externalIds": {"MAG": "2963265008", "ArXiv": "1605.09782", "DBLP": "journals/corr/DonahueKD16",
      "CorpusId": 84591}, "corpusId": 84591, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/1db6e3078597386ac4222ba6c3f4f61b61f53539",
      "title": "Adversarial Feature Learning", "abstract": "The ability of the Generative
      Adversarial Networks (GANs) framework to learn generative models mapping from
      simple latent distributions to arbitrarily complex data distributions has been
      demonstrated empirically, with compelling results showing that the latent space
      of such generators captures semantic variation in the data distribution. Intuitively,
      models trained to predict these semantic latent representations given data may
      serve as useful feature representations for auxiliary problems where semantics
      are relevant. However, in their existing form, GANs have no means of learning
      the inverse mapping -- projecting data back into the latent space. We propose
      Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning
      this inverse mapping, and demonstrate that the resulting learned feature representation
      is useful for auxiliary supervised discrimination tasks, competitive with contemporary
      approaches to unsupervised and self-supervised feature learning.", "venue":
      "International Conference on Learning Representations", "year": 2016, "referenceCount":
      36, "citationCount": 1671, "influentialCitationCount": 223, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2016-05-31", "journal": {"volume": "abs/1605.09782", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Donahue2016AdversarialFL,\n author = {Jeff Donahue and
      Philipp Kr\u00e4henb\u00fchl and Trevor Darrell},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial
      Feature Learning},\n volume = {abs/1605.09782},\n year = {2016}\n}\n"}, "authors":
      [{"authorId": "7408951", "name": "Jeff Donahue"}, {"authorId": "2562966", "name":
      "Philipp Kr\u00e4henb\u00fchl"}, {"authorId": "1753210", "name": "Trevor Darrell"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "b7624bfd4099b9e0d32875c7480ca04a3d8deb03", "externalIds":
      {"MAG": "2416555906", "DBLP": "journals/corr/OsokinALDL16", "ArXiv": "1605.09346",
      "CorpusId": 8085785}, "corpusId": 8085785, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/b7624bfd4099b9e0d32875c7480ca04a3d8deb03",
      "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured
      SVMs", "abstract": "In this paper, we propose several improvements on the block-coordinate
      Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used
      to optimize the structured support vector machine (SSVM) objective in the context
      of structured prediction, though it has wider applications. The key intuition
      behind our improvements is that the estimates of block gaps maintained by BCFW
      reveal the block suboptimality that can be used as an adaptive criterion. First,
      we sample objects at each iteration of BCFW in an adaptive non-uniform way via
      gapbased sampling. Second, we incorporate pairwise and away-step variants of
      Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls
      with a cache-hit criterion based on the block gaps. Fourth, we provide the first
      method to compute an approximate regularization path for SSVM. Finally, we provide
      an exhaustive empirical evaluation of all our methods on four structured prediction
      datasets.", "venue": "International Conference on Machine Learning", "year":
      2016, "referenceCount": 59, "citationCount": 62, "influentialCitationCount":
      6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-05-30", "journal": {"volume": "abs/1605.09346", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Osokin2016MindingTG,\n author = {A. Osokin and Jean-Baptiste
      Alayrac and Isabella Lukasewitz and P. Dokania and Simon Lacoste-Julien},\n
      booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n
      title = {Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs},\n
      volume = {abs/1605.09346},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "145319877", "name": "A. Osokin"}, {"authorId": "2285263", "name": "Jean-Baptiste
      Alayrac"}, {"authorId": "1419451349", "name": "Isabella Lukasewitz"}, {"authorId":
      "144679302", "name": "P. Dokania"}, {"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "af65eadf393d3470281a3838ec81f29a35777773",
      "externalIds": {"MAG": "2963663068", "ArXiv": "1605.08636", "DBLP": "journals/corr/GermainBLL16",
      "CorpusId": 930133}, "corpusId": 930133, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/af65eadf393d3470281a3838ec81f29a35777773",
      "title": "PAC-Bayesian Theory Meets Bayesian Inference", "abstract": "We exhibit
      a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian
      marginal likelihood. That is, for the negative log-likelihood loss function,
      we show that the minimization of PAC-Bayesian generalization risk bounds maximizes
      the Bayesian marginal likelihood. This provides an alternative explanation to
      the Bayesian Occam''s razor criteria, under the assumption that the data is
      generated by an i.i.d distribution. Moreover, as the negative log-likelihood
      is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem
      tailored for the sub-gamma loss family, and we show that our approach is sound
      on classical Bayesian linear regression tasks.", "venue": "Neural Information
      Processing Systems", "year": 2016, "referenceCount": 51, "citationCount": 166,
      "influentialCitationCount": 36, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-05-27", "journal": {"volume": "abs/1605.08636",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Germain2016PACBayesianTM,\n
      author = {Pascal Germain and F. Bach and Alexandre Lacoste and Simon Lacoste-Julien},\n
      booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n
      title = {PAC-Bayesian Theory Meets Bayesian Inference},\n volume = {abs/1605.08636},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "31580144", "name": "Pascal Germain"},
      {"authorId": "144570279", "name": "F. Bach"}, {"authorId": "8651990", "name":
      "Alexandre Lacoste"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "externalIds":
      {"DBLP": "journals/corr/ReedAYLSL16", "ArXiv": "1605.05396", "MAG": "2949999304",
      "CorpusId": 1563370}, "corpusId": 1563370, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/6c7f040a150abf21dbcefe1f22e0f98fa184f41a",
      "title": "Generative Adversarial Text to Image Synthesis", "abstract": "Automatic
      synthesis of realistic images from text would be interesting and useful, but
      current AI systems are still far from this goal. However, in recent years generic
      and powerful recurrent neural network architectures have been developed to learn
      discriminative text feature representations. Meanwhile, deep convolutional generative
      adversarial networks (GANs) have begun to generate highly compelling images
      of specific categories, such as faces, album covers, and room interiors. In
      this work, we develop a novel deep architecture and GAN formulation to effectively
      bridge these advances in text and image modeling, translating visual concepts
      from characters to pixels. We demonstrate the capability of our model to generate
      plausible images of birds and flowers from detailed text descriptions.", "venue":
      "International Conference on Machine Learning", "year": 2016, "referenceCount":
      42, "citationCount": 2753, "influentialCitationCount": 218, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-05-17", "journal": {"pages": "1060-1069"},
      "citationStyles": {"bibtex": "@Article{Reed2016GenerativeAT,\n author = {Scott
      E. Reed and Zeynep Akata and Xinchen Yan and Lajanugen Logeswaran and B. Schiele
      and Honglak Lee},\n booktitle = {International Conference on Machine Learning},\n
      pages = {1060-1069},\n title = {Generative Adversarial Text to Image Synthesis},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "144828948", "name": "Scott E.
      Reed"}, {"authorId": "2893664", "name": "Zeynep Akata"}, {"authorId": "3084614",
      "name": "Xinchen Yan"}, {"authorId": "2876316", "name": "Lajanugen Logeswaran"},
      {"authorId": "48920094", "name": "B. Schiele"}, {"authorId": "1697141", "name":
      "Honglak Lee"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "d175093098decfb26d30f3d74e6cfadff16a7b8b",
      "externalIds": {"MAG": "2950455576", "DBLP": "conf/icml/PodosinnikovaBL16",
      "ArXiv": "1602.09013", "CorpusId": 11886704}, "corpusId": 11886704, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/d175093098decfb26d30f3d74e6cfadff16a7b8b",
      "title": "Beyond CCA: Moment Matching for Multi-View Models", "abstract": "We
      introduce three novel semi-parametric extensions of probabilistic canonical
      correlation analysis with identifiability guarantees. We consider moment matching
      techniques for estimation in these models. For that, by drawing explicit links
      between the new models and a discrete version of independent component analysis
      (DICA), we first extend the DICA cumulant tensors to the new discrete version
      of CCA. By further using a close connection with independent component analysis,
      we introduce generalized covariance matrices, which can replace the cumulant
      tensors in the moment matching framework, and, therefore, improve sample complexity
      and simplify derivations and algorithms significantly. As the tensor power method
      or orthogonal joint diagonalization are not applicable in the new setting, we
      use non-orthogonal joint diagonalization techniques for matching the cumulants.
      We demonstrate performance of the proposed models and estimation techniques
      on experiments with both synthetic and real datasets.", "venue": "International
      Conference on Machine Learning", "year": 2016, "referenceCount": 65, "citationCount":
      16, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-02-29", "journal": {"volume": "abs/1602.09013",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Podosinnikova2016BeyondCM,\n
      author = {A. Podosinnikova and F. Bach and Simon Lacoste-Julien},\n booktitle
      = {International Conference on Machine Learning},\n journal = {ArXiv},\n title
      = {Beyond CCA: Moment Matching for Multi-View Models},\n volume = {abs/1602.09013},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "3343377", "name": "A. Podosinnikova"},
      {"authorId": "144570279", "name": "F. Bach"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "32c09d2933a4638b034343f9be20544dacf6031f",
      "externalIds": {"MAG": "2949933669", "DBLP": "journals/corr/ImKJM16", "ArXiv":
      "1602.05110", "CorpusId": 6281601}, "corpusId": 6281601, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/32c09d2933a4638b034343f9be20544dacf6031f",
      "title": "Generating images with recurrent adversarial networks", "abstract":
      "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional
      network with respect reference image features is a way to render images of high
      visual quality. We show that unrolling this gradient-based optimization yields
      a recurrent computation that creates images by incrementally adding onto a visual
      \"canvas\". We propose a recurrent generative model inspired by this view, and
      show that it can be trained using adversarial training to generate very good
      image samples. We also propose a way to quantitatively compare adversarial networks
      by having the generators and discriminators of these networks compete against
      each other.", "venue": "arXiv.org", "year": 2016, "referenceCount": 20, "citationCount":
      210, "influentialCitationCount": 14, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2016-02-01", "journal":
      {"volume": "abs/1602.05110", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Im2016GeneratingIW,\n author = {Daniel Jiwoong Im and C. Kim and Hui
      Jiang and R. Memisevic},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title
      = {Generating images with recurrent adversarial networks},\n volume = {abs/1602.05110},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "2903841", "name": "Daniel Jiwoong
      Im"}, {"authorId": "32821535", "name": "C. Kim"}, {"authorId": "36357862", "name":
      "Hui Jiang"}, {"authorId": "1710604", "name": "R. Memisevic"}]}}, {"contexts":
      ["DCGAN DCGAN made several improvements allowing GANs to be trained on larger/deeper
      CNNs Radford et al. (2015). Came after LapGAN and produced even cleaner results."],
      "intents": [], "contextsWithIntent": [{"context": "DCGAN DCGAN made several
      improvements allowing GANs to be trained on larger/deeper CNNs Radford et al.
      (2015). Came after LapGAN and produced even cleaner results.", "intents": []}],
      "isInfluential": false, "citedPaper": {"paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
      "externalIds": {"MAG": "2949811265", "ArXiv": "1511.06434", "DBLP": "journals/corr/RadfordMC15",
      "CorpusId": 11758569}, "corpusId": 11758569, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative
      Adversarial Networks", "abstract": "In recent years, supervised learning with
      convolutional networks (CNNs) has seen huge adoption in computer vision applications.
      Comparatively, unsupervised learning with CNNs has received less attention.
      In this work we hope to help bridge the gap between the success of CNNs for
      supervised learning and unsupervised learning. We introduce a class of CNNs
      called deep convolutional generative adversarial networks (DCGANs), that have
      certain architectural constraints, and demonstrate that they are a strong candidate
      for unsupervised learning. Training on various image datasets, we show convincing
      evidence that our deep convolutional adversarial pair learns a hierarchy of
      representations from object parts to scenes in both the generator and discriminator.
      Additionally, we use the learned features for novel tasks - demonstrating their
      applicability as general image representations.", "venue": "International Conference
      on Learning Representations", "year": 2015, "referenceCount": 46, "citationCount":
      12458, "influentialCitationCount": 1831, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-11-19", "journal":
      {"volume": "abs/1511.06434", "name": "CoRR"}, "citationStyles": {"bibtex": "@Article{Radford2015UnsupervisedRL,\n
      author = {Alec Radford and Luke Metz and Soumith Chintala},\n booktitle = {International
      Conference on Learning Representations},\n journal = {CoRR},\n title = {Unsupervised
      Representation Learning with Deep Convolutional Generative Adversarial Networks},\n
      volume = {abs/1511.06434},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "38909097", "name": "Alec Radford"}, {"authorId": "2096458", "name": "Luke Metz"},
      {"authorId": "2127604", "name": "Soumith Chintala"}]}}, {"contexts": ["Springenberg
      (2015) Generator tries to fool discriminator into assigning a label (low label
      entropy) Discriminator tries to leave generated samples as uncategorized (high
      entropy) while real samples are categorized (low entropy) Optional supervision
      on labels can be provided"], "intents": [], "contextsWithIntent": [{"context":
      "Springenberg (2015) Generator tries to fool discriminator into assigning a
      label (low label entropy) Discriminator tries to leave generated samples as
      uncategorized (high entropy) while real samples are categorized (low entropy)
      Optional supervision on labels can be provided", "intents": []}], "isInfluential":
      false, "citedPaper": {"paperId": "543f21d81bbea89f901dfcc01f4e332a9af6682d",
      "externalIds": {"MAG": "2963250052", "DBLP": "journals/corr/Springenberg15",
      "ArXiv": "1511.06390", "CorpusId": 6230637}, "corpusId": 6230637, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/543f21d81bbea89f901dfcc01f4e332a9af6682d",
      "title": "Unsupervised and Semi-supervised Learning with Categorical Generative
      Adversarial Networks", "abstract": "In this paper we present a method for learning
      a discriminative classifier from unlabeled or partially labeled data. Our approach
      is based on an objective function that trades-off mutual information between
      observed examples and their predicted categorical class distribution, against
      robustness of the classifier to an adversarial generative model. The resulting
      algorithm can either be interpreted as a natural generalization of the generative
      adversarial networks (GAN) framework or as an extension of the regularized information
      maximization (RIM) framework to robust classification against an optimal adversary.
      We empirically evaluate our method - which we dub categorical generative adversarial
      networks (or CatGAN) - on synthetic data as well as on challenging image classification
      tasks, demonstrating the robustness of the learned classifiers. We further qualitatively
      assess the fidelity of samples generated by the adversarial generator that is
      learned alongside the discriminative classifier, and identify links between
      the CatGAN objective and discriminative clustering algorithms (such as RIM).",
      "venue": "International Conference on Learning Representations", "year": 2015,
      "referenceCount": 45, "citationCount": 693, "influentialCitationCount": 60,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2015-11-19", "journal": {"volume": "abs/1511.06390", "name":
      "CoRR"}, "citationStyles": {"bibtex": "@Article{Springenberg2015UnsupervisedAS,\n
      author = {J. T. Springenberg},\n booktitle = {International Conference on Learning
      Representations},\n journal = {CoRR},\n title = {Unsupervised and Semi-supervised
      Learning with Categorical Generative Adversarial Networks},\n volume = {abs/1511.06390},\n
      year = {2015}\n}\n"}, "authors": [{"authorId": "2060551", "name": "J. T. Springenberg"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "8b56449fa9aeadcf86181697e9ec2269419d8817", "externalIds":
      {"DBLP": "conf/nips/Lacoste-JulienJ15", "MAG": "2949435166", "ArXiv": "1511.05932",
      "CorpusId": 14606229}, "corpusId": 14606229, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/8b56449fa9aeadcf86181697e9ec2269419d8817",
      "title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants",
      "abstract": "The Frank-Wolfe (FW) optimization algorithm has lately re-gained
      popularity thanks in particular to its ability to nicely handle the structured
      constraints appearing in machine learning applications. However, its convergence
      rate is known to be slow (sublinear) when the solution lies at the boundary.
      A simple less-known fix is to add the possibility to take ''away steps'' during
      optimization, an operation that importantly does not require a feasibility oracle.
      In this paper, we highlight and clarify several variants of the Frank-Wolfe
      optimization algorithm that have been successfully applied in practice: away-steps
      FW, pairwise FW, fully-corrective FW and Wolfe''s minimum norm point algorithm,
      and prove for the first time that they all enjoy global linear convergence,
      under a weaker condition than strong convexity of the objective. The constant
      in the convergence rate has an elegant interpretation as the product of the
      (classical) condition number of the function with a novel geometric quantity
      that plays the role of a ''condition number'' of the constraint set. We provide
      pointers to where these algorithms have made a difference in practice, in particular
      with the flow polytope, the marginal polytope and the base polytope for submodular
      optimization.", "venue": "Neural Information Processing Systems", "year": 2015,
      "referenceCount": 39, "citationCount": 382, "influentialCitationCount": 85,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2015-11-18", "journal": {"pages": "496-504"}, "citationStyles": {"bibtex":
      "@Article{Lacoste-Julien2015OnTG,\n author = {Simon Lacoste-Julien and Martin
      Jaggi},\n booktitle = {Neural Information Processing Systems},\n pages = {496-504},\n
      title = {On the Global Linear Convergence of Frank-Wolfe Optimization Variants},\n
      year = {2015}\n}\n"}, "authors": [{"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}, {"authorId": "2456863", "name": "Martin Jaggi"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "c8c04ed972d38e2326a53d322a6f2d7e0f8218c1", "externalIds": {"DBLP":
      "journals/corr/MakhzaniSJG15", "ArXiv": "1511.05644", "CorpusId": 5092785},
      "corpusId": 5092785, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
      "title": "Adversarial Autoencoders", "abstract": "In this paper, we propose
      the\"adversarial autoencoder\"(AAE), which is a probabilistic autoencoder that
      uses the recently proposed generative adversarial networks (GAN) to perform
      variational inference by matching the aggregated posterior of the hidden code
      vector of the autoencoder with an arbitrary prior distribution. Matching the
      aggregated posterior to the prior ensures that generating from any part of prior
      space results in meaningful samples. As a result, the decoder of the adversarial
      autoencoder learns a deep generative model that maps the imposed prior to the
      data distribution. We show how the adversarial autoencoder can be used in applications
      such as semi-supervised classification, disentangling style and content of images,
      unsupervised clustering, dimensionality reduction and data visualization. We
      performed experiments on MNIST, Street View House Numbers and Toronto Face datasets
      and show that adversarial autoencoders achieve competitive results in generative
      modeling and semi-supervised classification tasks.", "venue": "arXiv.org", "year":
      2015, "referenceCount": 26, "citationCount": 1910, "influentialCitationCount":
      270, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2015-11-18", "journal": {"volume": "abs/1511.05644",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Makhzani2015AdversarialA,\n
      author = {Alireza Makhzani and Jonathon Shlens and N. Jaitly and I. Goodfellow},\n
      booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Adversarial Autoencoders},\n
      volume = {abs/1511.05644},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "2730435", "name": "Alireza Makhzani"}, {"authorId": "1789737", "name": "Jonathon
      Shlens"}, {"authorId": "3111912", "name": "N. Jaitly"}, {"authorId": "153440022",
      "name": "I. Goodfellow"}]}}, {"contexts": ["GANs are thus great for learning
      in situations where there are many possible correct answers, such as predicting
      the many possible futures that can happen in video generation.(19) GANs and
      GAN-like models can be used to learn to transform data from one domain into
      data from another domain, even without any labeled pairs of examples from those
      domains (e."], "intents": [], "contextsWithIntent": [{"context": "GANs are thus
      great for learning in situations where there are many possible correct answers,
      such as predicting the many possible futures that can happen in video generation.(19)
      GANs and GAN-like models can be used to learn to transform data from one domain
      into data from another domain, even without any labeled pairs of examples from
      those domains (e.", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "17fa1c2a24ba8f731c8b21f1244463bc4b465681", "externalIds": {"MAG": "2949900324",
      "DBLP": "journals/corr/MathieuCL15", "ArXiv": "1511.05440", "CorpusId": 205514},
      "corpusId": 205514, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
      "title": "Deep multi-scale video prediction beyond mean square error", "abstract":
      "Learning to predict future images from a video sequence involves the construction
      of an internal representation that models the image evolution accurately, and
      therefore, to some degree, its content and dynamics. This is why pixel-space
      video prediction may be viewed as a promising avenue for unsupervised feature
      learning. In addition, while optical flow has been a very studied problem in
      computer vision for a long time, future frame prediction is rarely approached.
      Still, many vision applications could benefit from the knowledge of the next
      frames of videos, that does not require the complexity of tracking every pixel
      trajectories. In this work, we train a convolutional network to generate future
      frames given an input sequence. To deal with the inherently blurry predictions
      obtained from the standard Mean Squared Error (MSE) loss function, we propose
      three different and complementary feature learning strategies: a multi-scale
      architecture, an adversarial training method, and an image gradient difference
      loss function. We compare our predictions to different published results based
      on recurrent neural networks on the UCF101 dataset", "venue": "International
      Conference on Learning Representations", "year": 2015, "referenceCount": 35,
      "citationCount": 1751, "influentialCitationCount": 205, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-11-17", "journal":
      {"volume": "abs/1511.05440", "name": "CoRR"}, "citationStyles": {"bibtex": "@Article{Mathieu2015DeepMV,\n
      author = {Micha\u00ebl Mathieu and C. Couprie and Yann LeCun},\n booktitle =
      {International Conference on Learning Representations},\n journal = {CoRR},\n
      title = {Deep multi-scale video prediction beyond mean square error},\n volume
      = {abs/1511.05440},\n year = {2015}\n}\n"}, "authors": [{"authorId": "143949035",
      "name": "Micha\u00ebl Mathieu"}, {"authorId": "2341378", "name": "C. Couprie"},
      {"authorId": "1688882", "name": "Yann LeCun"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "ccf9b939330f73d7f87d3d606349c3352bf2713d", "externalIds": {"DBLP": "conf/nips/KrishnanLS15",
      "MAG": "2964168821", "ArXiv": "1511.02124", "CorpusId": 6403759}, "corpusId":
      6403759, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/ccf9b939330f73d7f87d3d606349c3352bf2713d",
      "title": "Barrier Frank-Wolfe for Marginal Inference", "abstract": "We introduce
      a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational
      objective over the marginal polytope. The algorithm is based on the conditional
      gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal
      polytope through repeated maximum a posteriori (MAP) calls. This modular structure
      enables us to leverage black-box MAP solvers (both exact and approximate) for
      variational inference, and obtains more accurate results than tree-reweighted
      algorithms that optimize over the local consistency relaxation. Theoretically,
      we bound the sub-optimality for the proposed algorithm despite the TRW objective
      having unbounded gradients at the boundary of the marginal polytope. Empirically,
      we demonstrate the increased quality of results found by tightening the relaxation
      over the marginal polytope as well as the spanning tree polytope on synthetic
      and real-world instances.", "venue": "Neural Information Processing Systems",
      "year": 2015, "referenceCount": 34, "citationCount": 35, "influentialCitationCount":
      3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2015-11-06", "journal": {"pages": "532-540"}, "citationStyles": {"bibtex":
      "@Article{Krishnan2015BarrierFF,\n author = {R. G. Krishnan and Simon Lacoste-Julien
      and D. Sontag},\n booktitle = {Neural Information Processing Systems},\n pages
      = {532-540},\n title = {Barrier Frank-Wolfe for Marginal Inference},\n year
      = {2015}\n}\n"}, "authors": [{"authorId": "145253891", "name": "R. G. Krishnan"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1746662",
      "name": "D. Sontag"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
      "externalIds": {"MAG": "2099057450", "DBLP": "journals/corr/TheisOB15", "ArXiv":
      "1511.01844", "CorpusId": 2187805}, "corpusId": 2187805, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/39e0c341351f8f4a39ac890b96217c7f4bde5369",
      "title": "A note on the evaluation of generative models", "abstract": "Probabilistic
      generative models can be used for compression, denoising, inpainting, texture
      synthesis, semi-supervised learning, unsupervised feature learning, and other
      tasks. Given this wide range of applications, it is not surprising that a lot
      of heterogeneity exists in the way these models are formulated, trained, and
      evaluated. As a consequence, direct comparison between models is often difficult.
      This article reviews mostly known but often underappreciated properties relating
      to the evaluation and interpretation of generative models with a focus on image
      models. In particular, we show that three of the currently most commonly used
      criteria---average log-likelihood, Parzen window estimates, and visual fidelity
      of samples---are largely independent of each other when the data is high-dimensional.
      Good performance with respect to one criterion therefore need not imply good
      performance with respect to the other criteria. Our results show that extrapolation
      from one criterion to another is not warranted and generative models need to
      be evaluated directly with respect to the application(s) they were intended
      for. In addition, we provide examples demonstrating that Parzen window estimates
      should generally be avoided.", "venue": "International Conference on Learning
      Representations", "year": 2015, "referenceCount": 39, "citationCount": 1000,
      "influentialCitationCount": 64, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Review"], "publicationDate": "2015-11-05", "journal": {"volume": "abs/1511.01844",
      "name": "CoRR"}, "citationStyles": {"bibtex": "@Article{Theis2015ANO,\n author
      = {Lucas Theis and A\u00e4ron van den Oord and M. Bethge},\n booktitle = {International
      Conference on Learning Representations},\n journal = {CoRR},\n title = {A note
      on the evaluation of generative models},\n volume = {abs/1511.01844},\n year
      = {2015}\n}\n"}, "authors": [{"authorId": "2073063", "name": "Lucas Theis"},
      {"authorId": "3422336", "name": "A\u00e4ron van den Oord"}, {"authorId": "1731199",
      "name": "M. Bethge"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "2d9f970000a4309f32d37afcf7833c213e530810",
      "externalIds": {"DBLP": "conf/nips/PodosinnikovaBL15", "MAG": "1783362711",
      "ArXiv": "1507.01784", "CorpusId": 10035651}, "corpusId": 10035651, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/2d9f970000a4309f32d37afcf7833c213e530810",
      "title": "Rethinking LDA: Moment Matching for Discrete ICA", "abstract": "We
      consider moment matching techniques for estimation in latent Dirichlet allocation
      (LDA). By drawing explicit links between LDA and discrete versions of independent
      component analysis (ICA), we first derive a new set of cumulant-based tensors,
      with an improved sample complexity. Moreover, we reuse standard ICA techniques
      such as joint diagonalization of tensors to improve over existing methods based
      on the tensor power method. In an extensive set of experiments on both synthetic
      and real datasets, we show that our new combination of tensors and orthogonal
      joint diagonalization techniques outperforms existing moment matching methods.",
      "venue": "Neural Information Processing Systems", "year": 2015, "referenceCount":
      36, "citationCount": 24, "influentialCitationCount": 5, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-07-07",
      "journal": {"volume": "abs/1507.01784", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Podosinnikova2015RethinkingLM,\n author = {A. Podosinnikova
      and F. Bach and Simon Lacoste-Julien},\n booktitle = {Neural Information Processing
      Systems},\n journal = {ArXiv},\n title = {Rethinking LDA: Moment Matching for
      Discrete ICA},\n volume = {abs/1507.01784},\n year = {2015}\n}\n"}, "authors":
      [{"authorId": "3343377", "name": "A. Podosinnikova"}, {"authorId": "144570279",
      "name": "F. Bach"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "35e3fa06068a39996a7185a4b74f332ac8196566", "externalIds":
      {"DBLP": "journals/corr/AlayracBASLL15", "MAG": "2967417124", "DOI": "10.1109/TPAMI.2017.2749223",
      "CorpusId": 215811147, "PubMed": "28885149"}, "corpusId": 215811147, "publicationVenue":
      {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b", "name": "IEEE Transactions on
      Pattern Analysis and Machine Intelligence", "type": "journal", "alternate_names":
      ["IEEE Trans Pattern Anal Mach Intell"], "issn": "0162-8828", "url": "http://www.computer.org/tpami/",
      "alternate_urls": ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
      "url": "https://www.semanticscholar.org/paper/35e3fa06068a39996a7185a4b74f332ac8196566",
      "title": "Learning from Narrated Instruction Videos", "abstract": "Automatic
      assistants could guide a person or a robot in performing new tasks, such as
      changing a car tire or repotting a plant. Creating such assistants, however,
      is non-trivial and requires understanding of visual and verbal content of a
      video. Towards this goal, we here address the problem of automatically learning
      the main steps of a task from a set of narrated instruction videos. We develop
      a new unsupervised learning approach that takes advantage of the complementary
      nature of the input video and the associated narration. The method sequentially
      clusters textual and visual representations of a task, where the two clustering
      problems are linked by joint constraints to obtain a single coherent sequence
      of steps in both modalities. To evaluate our method, we collect and annotate
      a new challenging dataset of real-world instruction videos from the Internet.
      The dataset contains videos for five different tasks with complex interactions
      between people and objects, captured in a variety of indoor and outdoor settings.
      We experimentally demonstrate that the proposed method can automatically discover,
      learn and localize the main steps of a task in input videos.", "venue": "IEEE
      Transactions on Pattern Analysis and Machine Intelligence", "year": 2015, "referenceCount":
      44, "citationCount": 21, "influentialCitationCount": 1, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-01580630/file/pami2016alayrac.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-06-30", "journal":
      {"volume": "40", "pages": "2194-2208", "name": "IEEE Transactions on Pattern
      Analysis and Machine Intelligence"}, "citationStyles": {"bibtex": "@Article{Alayrac2015LearningFN,\n
      author = {Jean-Baptiste Alayrac and Piotr Bojanowski and Nishant Agrawal and
      Josef Sivic and I. Laptev and Simon Lacoste-Julien},\n booktitle = {IEEE Transactions
      on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions
      on Pattern Analysis and Machine Intelligence},\n pages = {2194-2208},\n title
      = {Learning from Narrated Instruction Videos},\n volume = {40},\n year = {2015}\n}\n"},
      "authors": [{"authorId": "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId":
      "2329288", "name": "Piotr Bojanowski"}, {"authorId": "143688116", "name": "Nishant
      Agrawal"}, {"authorId": "1782755", "name": "Josef Sivic"}, {"authorId": "143991676",
      "name": "I. Laptev"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "e17ba2b5d0769e7f2602d859ea77a153846cf27d", "externalIds":
      {"MAG": "2185243164", "ArXiv": "1506.09215", "DBLP": "conf/cvpr/AlayracBASLL16",
      "DOI": "10.1109/CVPR.2016.495", "CorpusId": 2617244}, "corpusId": 2617244, "publicationVenue":
      {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
      Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
      Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/e17ba2b5d0769e7f2602d859ea77a153846cf27d",
      "title": "Unsupervised Learning from Narrated Instruction Videos", "abstract":
      "We address the problem of automatically learning the main steps to complete
      a certain task, such as changing a car tire, from a set of narrated instruction
      videos. The contributions of this paper are three-fold. First, we develop a
      new unsupervised learning approach that takes advantage of the complementary
      nature of the input video and the associated narration. The method solves two
      clustering problems, one in text and one in video, applied one after each other
      and linked by joint constraints to obtain a single coherent sequence of steps
      in both modalities. Second, we collect and annotate a new challenging dataset
      of real-world instruction videos from the Internet. The dataset contains about
      800,000 frames for five different tasks1 that include complex interactions between
      people and objects, and are captured in a variety of indoor and outdoor settings.
      Third, we experimentally demonstrate that the proposed method can automatically
      discover, in an unsupervised manner, the main steps to achieve the task and
      locate the steps in the input videos.", "venue": "Computer Vision and Pattern
      Recognition", "year": 2015, "referenceCount": 38, "citationCount": 257, "influentialCitationCount":
      35, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1506.09215",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2015-06-30", "journal": {"pages": "4575-4583", "name": "2016
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "citationStyles":
      {"bibtex": "@Article{Alayrac2015UnsupervisedLF,\n author = {Jean-Baptiste Alayrac
      and Piotr Bojanowski and Nishant Agrawal and Josef Sivic and I. Laptev and Simon
      Lacoste-Julien},\n booktitle = {Computer Vision and Pattern Recognition},\n
      journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n
      pages = {4575-4583},\n title = {Unsupervised Learning from Narrated Instruction
      Videos},\n year = {2015}\n}\n"}, "authors": [{"authorId": "2285263", "name":
      "Jean-Baptiste Alayrac"}, {"authorId": "2329288", "name": "Piotr Bojanowski"},
      {"authorId": "143688116", "name": "Nishant Agrawal"}, {"authorId": "1782755",
      "name": "Josef Sivic"}, {"authorId": "143991676", "name": "I. Laptev"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": ["Laplacian-pyramid
      GANs (LapGANs) LapGAN utilizes conditional GANs and a trick from image processing
      known as a \u201dpyramid\u201d Denton et al. (2015). Given a blurry image, a
      single CGAN can generate a slightly less blurry image A stack of CGANs can generate
      a very sharp image"], "intents": [], "contextsWithIntent": [{"context": "Laplacian-pyramid
      GANs (LapGANs) LapGAN utilizes conditional GANs and a trick from image processing
      known as a \u201dpyramid\u201d Denton et al. (2015). Given a blurry image, a
      single CGAN can generate a slightly less blurry image A stack of CGANs can generate
      a very sharp image", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "47900aca2f0b50da3010ad59b394c870f0e6c02e", "externalIds": {"MAG":
      "648143168", "DBLP": "conf/nips/DentonCSF15", "ArXiv": "1506.05751", "CorpusId":
      1282515}, "corpusId": 1282515, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e",
      "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial
      Networks", "abstract": "In this paper we introduce a generative parametric model
      capable of producing high quality samples of natural images. Our approach uses
      a cascade of convolutional networks within a Laplacian pyramid framework to
      generate images in a coarse-to-fine fashion. At each level of the pyramid, a
      separate generative convnet model is trained using the Generative Adversarial
      Nets (GAN) approach [11]. Samples drawn from our model are of significantly
      higher quality than alternate approaches. In a quantitative assessment by human
      evaluators, our CIFAR10 samples were mistaken for real images around 40% of
      the time, compared to 10% for samples drawn from a GAN baseline model. We also
      show samples from models trained on the higher resolution images of the LSUN
      scene dataset.", "venue": "Neural Information Processing Systems", "year": 2015,
      "referenceCount": 36, "citationCount": 2130, "influentialCitationCount": 109,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-06-18", "journal":
      {"volume": "abs/1506.05751", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Denton2015DeepGI,\n author = {Emily L. Denton and Soumith Chintala
      and Arthur Szlam and R. Fergus},\n booktitle = {Neural Information Processing
      Systems},\n journal = {ArXiv},\n title = {Deep Generative Image Models using
      a Laplacian Pyramid of Adversarial Networks},\n volume = {abs/1506.05751},\n
      year = {2015}\n}\n"}, "authors": [{"authorId": "40081727", "name": "Emily L.
      Denton"}, {"authorId": "2127604", "name": "Soumith Chintala"}, {"authorId":
      "3149531", "name": "Arthur Szlam"}, {"authorId": "2276554", "name": "R. Fergus"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "1291c0b301f77f6f24ac654689a45f2df34ddbfb", "externalIds":
      {"MAG": "2136667596", "DBLP": "conf/nips/HofmannLLM15", "ArXiv": "1506.03662",
      "CorpusId": 8779016}, "corpusId": 8779016, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/1291c0b301f77f6f24ac654689a45f2df34ddbfb",
      "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "abstract":
      "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its
      slow convergence can be a computational bottleneck. Variance reduction techniques
      such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving
      linear convergence. However, these methods are either based on computations
      of full gradients at pivot points, or on keeping per data point corrections
      in memory. Therefore speed-ups relative to SGD may need a minimal number of
      epochs in order to materialize. This paper investigates algorithms that can
      exploit neighborhood structure in the training data to share and re-use information
      about past stochastic gradients across data points, which offers advantages
      in the transient optimization phase. As a side-product we provide a unified
      convergence analysis for a family of variance reduction algorithms, which we
      call memorization algorithms. We provide experimental results supporting our
      theory.", "venue": "Neural Information Processing Systems", "year": 2015, "referenceCount":
      13, "citationCount": 141, "influentialCitationCount": 31, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-06-11",
      "journal": {"pages": "2305-2313"}, "citationStyles": {"bibtex": "@Article{Hofmann2015VarianceRS,\n
      author = {T. Hofmann and Aur\u00e9lien Lucchi and Simon Lacoste-Julien and B.
      McWilliams},\n booktitle = {Neural Information Processing Systems},\n pages
      = {2305-2313},\n title = {Variance Reduced Stochastic Gradient Descent with
      Neighbors},\n year = {2015}\n}\n"}, "authors": [{"authorId": "153379696", "name":
      "T. Hofmann"}, {"authorId": "40401747", "name": "Aur\u00e9lien Lucchi"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "2953855", "name":
      "B. McWilliams"}]}}, {"contexts": ["DRAW attempts to recurrently draw an image
      with attention, instead of emitting a single image all-at-once Gregor et al.
      (2015)."], "intents": [], "contextsWithIntent": [{"context": "DRAW attempts
      to recurrently draw an image with attention, instead of emitting a single image
      all-at-once Gregor et al. (2015).", "intents": []}], "isInfluential": false,
      "citedPaper": {"paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "externalIds":
      {"MAG": "1850742715", "DBLP": "journals/corr/GregorDGW15", "ArXiv": "1502.04623",
      "CorpusId": 1930231}, "corpusId": 1930231, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
      "title": "DRAW: A Recurrent Neural Network For Image Generation", "abstract":
      "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network
      architecture for image generation. DRAW networks combine a novel spatial attention
      mechanism that mimics the foveation of the human eye, with a sequential variational
      auto-encoding framework that allows for the iterative construction of complex
      images. The system substantially improves on the state of the art for generative
      models on MNIST, and, when trained on the Street View House Numbers dataset,
      it generates images that cannot be distinguished from real data with the naked
      eye.", "venue": "International Conference on Machine Learning", "year": 2015,
      "referenceCount": 36, "citationCount": 1850, "influentialCitationCount": 142,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-02-16", "journal":
      {"volume": "abs/1502.04623", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Gregor2015DRAWAR,\n author = {Karol Gregor and Ivo Danihelka and Alex
      Graves and Danilo Jimenez Rezende and Daan Wierstra},\n booktitle = {International
      Conference on Machine Learning},\n journal = {ArXiv},\n title = {DRAW: A Recurrent
      Neural Network For Image Generation},\n volume = {abs/1502.04623},\n year =
      {2015}\n}\n"}, "authors": [{"authorId": "144717963", "name": "Karol Gregor"},
      {"authorId": "1841008", "name": "Ivo Danihelka"}, {"authorId": "1753223", "name":
      "Alex Graves"}, {"authorId": "1748523", "name": "Danilo Jimenez Rezende"}, {"authorId":
      "1688276", "name": "Daan Wierstra"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "2904a9932f4cd0f0886121dc1f2d4aaac0455176",
      "externalIds": {"ArXiv": "1502.02761", "DBLP": "journals/corr/LiSZ15", "MAG":
      "1487641199", "CorpusId": 536962}, "corpusId": 536962, "publicationVenue": {"id":
      "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference on
      Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int Conf
      Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/2904a9932f4cd0f0886121dc1f2d4aaac0455176",
      "title": "Generative Moment Matching Networks", "abstract": "We consider the
      problem of learning deep generative models from data. We formulate a method
      that generates an independent sample via a single feedforward pass through a
      multilayer perceptron, as in the recently proposed generative adversarial networks
      (Goodfellow et al., 2014). Training a generative adversarial network, however,
      requires careful optimization of a difficult minimax program. Instead, we utilize
      a technique from statistical hypothesis testing known as maximum mean discrepancy
      (MMD), which leads to a simple objective that can be interpreted as matching
      all orders of statistics between a dataset and samples from the model, and can
      be trained by backpropagation. We further boost the performance of this approach
      by combining our generative network with an auto-encoder network, using MMD
      to learn to generate codes that can then be decoded to produce samples. We show
      that the combination of these techniques yields excellent generative models
      compared to baseline approaches as measured on MNIST and the Toronto Face Database.",
      "venue": "International Conference on Machine Learning", "year": 2015, "referenceCount":
      51, "citationCount": 767, "influentialCitationCount": 109, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-02-09",
      "journal": {"pages": "1718-1727"}, "citationStyles": {"bibtex": "@Article{Li2015GenerativeMM,\n
      author = {Yujia Li and Kevin Swersky and R. Zemel},\n booktitle = {International
      Conference on Machine Learning},\n pages = {1718-1727},\n title = {Generative
      Moment Matching Networks},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "47002813", "name": "Yujia Li"}, {"authorId": "1754860", "name": "Kevin Swersky"},
      {"authorId": "1804104", "name": "R. Zemel"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "3dba006990a306420dcde24bdd48420def1c7e17", "externalIds": {"MAG": "2962808554",
      "DBLP": "journals/corr/Lacoste-JulienLB15", "ArXiv": "1501.02056", "CorpusId":
      215825724}, "corpusId": 215825724, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/3dba006990a306420dcde24bdd48420def1c7e17",
      "title": "Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering",
      "abstract": "Recently, the Frank-Wolfe optimization algorithm was suggested
      as a procedure to obtain adaptive quadrature rules for integrals of functions
      in a reproducing kernel Hilbert space (RKHS) with a potentially faster rate
      of convergence than Monte Carlo integration (and \"kernel herding\" was shown
      to be a special case of this procedure). In this paper, we propose to replace
      the random sampling step in a particle filter by Frank-Wolfe optimization. By
      optimizing the position of the particles, we can obtain better accuracy than
      random or quasi-Monte Carlo sampling. In applications where the evaluation of
      the emission probabilities is expensive (such as in robot localization), the
      additional computational cost to generate the particles through optimization
      can be justified. Experiments on standard synthetic examples as well as on a
      robot localization task indicate indeed an improvement of accuracy over random
      and quasi-Monte Carlo sampling.", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2015, "referenceCount": 33, "citationCount":
      74, "influentialCitationCount": 8, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2015-01-08", "journal": {"volume": "abs/1501.02056", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Lacoste-Julien2015SequentialKH,\n
      author = {Simon Lacoste-Julien and F. Lindsten and F. Bach},\n booktitle = {International
      Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n
      title = {Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering},\n
      volume = {abs/1501.02056},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1759780", "name":
      "F. Lindsten"}, {"authorId": "144570279", "name": "F. Bach"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '300133'
      Content-Type:
      - application/json
      Date:
      - Wed, 27 Dec 2023 21:47:34 GMT
      Via:
      - 1.1 7d4cada96f0b733f4f539a3bab2e2d16.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - mmd1VIWjVaF1N9Sf3xHLVOGwqL43nRkRVhgKTRXkxMmQOaaykmJ56Q==
      X-Amz-Cf-Pop:
      - GRU3-P4
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - Qn06eHCIPHcESRQ=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '300133'
      x-amzn-Remapped-Date:
      - Wed, 27 Dec 2023 21:47:34 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 9475dcf0-fa0d-4256-8218-7ba934a3c981
    http_version: HTTP/1.1
    status_code: 200
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.25.2
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=100&limit=100
  response:
    content: '{"offset": 100, "data": [{"contexts": ["Conditional GAN: Mirza and Osindero
      (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al. (2015) CatGAN: Springenberg
      (2015) GRAN: Im et al. (2016) InfoGAN: Chen et al.", "Conditional GAN: Mirza
      and Osindero (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al. (2015)
      CatGAN: Springenberg (2015) GRAN: Im et al. (2016) InfoGAN: Chen et al. (2016)
      AAE: Makhzani et al.", "Fedus, W., Goodfellow, I., Dai, A.M. MaskGAN: Better
      text generation via filling in the _____.", "Conditional GAN: Mirza and Osindero
      (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al. (2015) CatGAN: Springenberg
      (2015) GRAN: Im et al. (2016) InfoGAN: Chen et al. (2016) AAE: Makhzani et al.
      (2015) BiGAN: Donahue et al. (2016) Benjamin Striner (CMU 11-785) Generative
      Adversarial Networks March 21, 2018 31 / 79", "Conditional GANs Conditional
      GANs include a label and learn P(X | Y ) Mirza and Osindero (2014). Generator
      learns P(X | Z ,Y ) Discriminator learns P(L | X ,Y )", "Conditional GAN: Mirza
      and Osindero (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al. (2015)
      CatGAN: Springenberg (2015) GRAN: Im et al.", "Conditional GAN: Mirza and Osindero
      (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al.", "Conditional GAN:
      Mirza and Osindero (2014) LapGAN: Denton et al.", "Conditional GAN: Mirza and
      Osindero (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al. (2015) CatGAN:
      Springenberg (2015) GRAN: Im et al. (2016) InfoGAN: Chen et al. (2016) AAE:
      Makhzani et al. (2015) BiGAN: Donahue et al."], "intents": [], "contextsWithIntent":
      [{"context": "Conditional GAN: Mirza and Osindero (2014) LapGAN: Denton et al.
      (2015) DCGAN: Radford et al. (2015) CatGAN: Springenberg (2015) GRAN: Im et
      al. (2016) InfoGAN: Chen et al.", "intents": []}, {"context": "Conditional GAN:
      Mirza and Osindero (2014) LapGAN: Denton et al. (2015) DCGAN: Radford et al.
      (2015) CatGAN: Springenberg (2015) GRAN: Im et al. (2016) InfoGAN: Chen et al.
      (2016) AAE: Makhzani et al.", "intents": []}, {"context": "Fedus, W., Goodfellow,
      I., Dai, A.M. MaskGAN: Better text generation via filling in the _____.", "intents":
      []}, {"context": "Conditional GAN: Mirza and Osindero (2014) LapGAN: Denton
      et al. (2015) DCGAN: Radford et al. (2015) CatGAN: Springenberg (2015) GRAN:
      Im et al. (2016) InfoGAN: Chen et al. (2016) AAE: Makhzani et al. (2015) BiGAN:
      Donahue et al. (2016) Benjamin Striner (CMU 11-785) Generative Adversarial Networks
      March 21, 2018 31 / 79", "intents": []}, {"context": "Conditional GANs Conditional
      GANs include a label and learn P(X | Y ) Mirza and Osindero (2014). Generator
      learns P(X | Z ,Y ) Discriminator learns P(L | X ,Y )", "intents": []}, {"context":
      "Conditional GAN: Mirza and Osindero (2014) LapGAN: Denton et al. (2015) DCGAN:
      Radford et al. (2015) CatGAN: Springenberg (2015) GRAN: Im et al.", "intents":
      []}, {"context": "Conditional GAN: Mirza and Osindero (2014) LapGAN: Denton
      et al. (2015) DCGAN: Radford et al.", "intents": []}, {"context": "Conditional
      GAN: Mirza and Osindero (2014) LapGAN: Denton et al.", "intents": []}, {"context":
      "Conditional GAN: Mirza and Osindero (2014) LapGAN: Denton et al. (2015) DCGAN:
      Radford et al. (2015) CatGAN: Springenberg (2015) GRAN: Im et al. (2016) InfoGAN:
      Chen et al. (2016) AAE: Makhzani et al. (2015) BiGAN: Donahue et al.", "intents":
      []}], "isInfluential": true, "citedPaper": {"paperId": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "externalIds": {"MAG": "2125389028", "ArXiv": "1411.1784", "DBLP": "journals/corr/MirzaO14",
      "CorpusId": 12803511}, "corpusId": 12803511, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "title": "Conditional Generative Adversarial Nets", "abstract": "Generative
      Adversarial Nets [8] were recently introduced as a novel way to train generative
      models. In this work we introduce the conditional version of generative adversarial
      nets, which can be constructed by simply feeding the data, y, we wish to condition
      on to both the generator and discriminator. We show that this model can generate
      MNIST digits conditioned on class labels. We also illustrate how this model
      could be used to learn a multi-modal model, and provide preliminary examples
      of an application to image tagging in which we demonstrate how this approach
      can generate descriptive tags which are not part of training labels.", "venue":
      "arXiv.org", "year": 2014, "referenceCount": 18, "citationCount": 8598, "influentialCitationCount":
      1270, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2014-11-06", "journal": {"volume": "abs/1411.1784", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Mirza2014ConditionalGA,\n author
      = {Mehdi Mirza and Simon Osindero},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n
      title = {Conditional Generative Adversarial Nets},\n volume = {abs/1411.1784},\n
      year = {2014}\n}\n"}, "authors": [{"authorId": "153583218", "name": "Mehdi Mirza"},
      {"authorId": "2217144", "name": "Simon Osindero"}]}}, {"contexts": ["3 GAN-like
      models called domain-adversarial networks can be used for domain adaptation.(12)
      GANs can be used for a variety of interactive digital media effects where the
      end goal is to produce compelling imagery."], "intents": [], "contextsWithIntent":
      [{"context": "3 GAN-like models called domain-adversarial networks can be used
      for domain adaptation.(12) GANs can be used for a variety of interactive digital
      media effects where the end goal is to produce compelling imagery.", "intents":
      []}], "isInfluential": false, "citedPaper": {"paperId": "2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
      "externalIds": {"ArXiv": "1409.7495", "MAG": "2951688345", "DBLP": "conf/icml/GaninL15",
      "CorpusId": 6755881}, "corpusId": 6755881, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
      "title": "Unsupervised Domain Adaptation by Backpropagation", "abstract": "Top-performing
      deep architectures are trained on massive amounts of labeled data. In the absence
      of labeled data for a certain task, domain adaptation often provides an attractive
      option given that labeled data of similar nature but from a different domain
      (e.g. synthetic images) are available. Here, we propose a new approach to domain
      adaptation in deep architectures that can be trained on large amount of labeled
      data from the source domain and large amount of unlabeled data from the target
      domain (no labeled target-domain data is necessary). \nAs the training progresses,
      the approach promotes the emergence of \"deep\" features that are (i) discriminative
      for the main learning task on the source domain and (ii) invariant with respect
      to the shift between the domains. We show that this adaptation behaviour can
      be achieved in almost any feed-forward model by augmenting it with few standard
      layers and a simple new gradient reversal layer. The resulting augmented architecture
      can be trained using standard backpropagation. \nOverall, the approach can be
      implemented with little effort using any of the deep-learning packages. The
      method performs very well in a series of image classification experiments, achieving
      adaptation effect in the presence of big domain shifts and outperforming previous
      state-of-the-art on Office datasets.", "venue": "International Conference on
      Machine Learning", "year": 2014, "referenceCount": 44, "citationCount": 4758,
      "influentialCitationCount": 1061, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2014-09-26", "journal":
      {"pages": "1180-1189"}, "citationStyles": {"bibtex": "@Article{Ganin2014UnsupervisedDA,\n
      author = {Yaroslav Ganin and V. Lempitsky},\n booktitle = {International Conference
      on Machine Learning},\n pages = {1180-1189},\n title = {Unsupervised Domain
      Adaptation by Backpropagation},\n year = {2014}\n}\n"}, "authors": [{"authorId":
      "2825246", "name": "Yaroslav Ganin"}, {"authorId": "1740145", "name": "V. Lempitsky"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "c291a92d89e88a8efb493fe39b6a4b033f1bbef7", "externalIds":
      {"DBLP": "conf/cvpr/ChariLLS15", "MAG": "2951115265", "DOI": "10.1109/CVPR.2015.7299193",
      "CorpusId": 8974920}, "corpusId": 8974920, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/c291a92d89e88a8efb493fe39b6a4b033f1bbef7",
      "title": "On pairwise costs for network flow multi-object tracking", "abstract":
      "Multi-object tracking has been recently approached with the min-cost network
      flow optimization techniques. Such methods simultaneously resolve multiple object
      tracks in a video and enable modeling of dependencies among tracks. Min-cost
      network flow methods also fit well within the \u201ctracking-by-detection\u201d
      paradigm where object trajectories are obtained by connecting per-frame outputs
      of an object detector. Object detectors, however, often fail due to occlusions
      and clutter in the video. To cope with such situations, we propose to add pairwise
      costs to the min-cost network flow framework. While integer solutions to such
      a problem become NP-hard, we design a convex relaxation solution with an efficient
      rounding heuristic which empirically gives certificates of small suboptimality.
      We evaluate two particular types of pairwise costs and demonstrate improvements
      over recent tracking methods in real-world video sequences.", "venue": "Computer
      Vision and Pattern Recognition", "year": 2014, "referenceCount": 29, "citationCount":
      136, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1408.3304", "status": null}, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2014-08-14", "journal": {"pages": "5537-5545",
      "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"},
      "citationStyles": {"bibtex": "@Article{Chari2014OnPC,\n author = {Visesh Chari
      and Simon Lacoste-Julien and I. Laptev and Josef Sivic},\n booktitle = {Computer
      Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer
      Vision and Pattern Recognition (CVPR)},\n pages = {5537-5545},\n title = {On
      pairwise costs for network flow multi-object tracking},\n year = {2014}\n}\n"},
      "authors": [{"authorId": "13614028", "name": "Visesh Chari"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "143991676", "name": "I. Laptev"},
      {"authorId": "1782755", "name": "Josef Sivic"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "4daec165c1f4aa1206b0d91c0b26f0287d1ef52d", "externalIds": {"DBLP": "journals/corr/DefazioBL14",
      "MAG": "2135482703", "ArXiv": "1407.0202", "CorpusId": 218654665}, "corpusId":
      218654665, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/4daec165c1f4aa1206b0d91c0b26f0287d1ef52d",
      "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly
      Convex Composite Objectives", "abstract": "In this work we introduce a new optimisation
      method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently
      proposed incremental gradient algorithms with fast linear convergence rates.
      SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence
      rates, and has support for composite objectives where a proximal operator is
      used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems
      directly, and is adaptive to any inherent strong convexity of the problem. We
      give experimental results showing the effectiveness of our method.", "venue":
      "Neural Information Processing Systems", "year": 2014, "referenceCount": 13,
      "citationCount": 1665, "influentialCitationCount": 319, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2014-07-01",
      "journal": {"pages": "1646-1654"}, "citationStyles": {"bibtex": "@Article{Defazio2014SAGAAF,\n
      author = {Aaron Defazio and F. Bach and Simon Lacoste-Julien},\n booktitle =
      {Neural Information Processing Systems},\n pages = {1646-1654},\n title = {SAGA:
      A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite
      Objectives},\n year = {2014}\n}\n"}, "authors": [{"authorId": "34597877", "name":
      "Aaron Defazio"}, {"authorId": "144570279", "name": "F. Bach"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"contexts": ["en used ina
      feedback loop. More recent examples of training a generative machine by back-propagating
      into it include recent work on auto-encoding variational Bayes [20] and stochastic
      backpropagation [24]. 3 Adversarial nets The adversarial modeling framework
      is most straightforward to apply when the models are both multilayer perceptrons.
      To learn the generator\u2019s distribution p g over data x, we de\ufb01n"],
      "intents": [], "contextsWithIntent": [{"context": "en used ina feedback loop.
      More recent examples of training a generative machine by back-propagating into
      it include recent work on auto-encoding variational Bayes [20] and stochastic
      backpropagation [24]. 3 Adversarial nets The adversarial modeling framework
      is most straightforward to apply when the models are both multilayer perceptrons.
      To learn the generator\u2019s distribution p g over data x, we de\ufb01n", "intents":
      []}], "isInfluential": false, "citedPaper": {"paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
      "externalIds": {"MAG": "1909320841", "DBLP": "conf/icml/RezendeMW14", "CorpusId":
      16895865}, "corpusId": 16895865, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/484ad17c926292fbe0d5211540832a8c8a8e958b",
      "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models", "abstract": "We marry ideas from deep neural networks and approximate
      Bayesian inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning. Our
      algorithm introduces a recognition model to represent approximate posterior
      distributions, and that acts as a stochastic encoder of the data. We develop
      stochastic back-propagation -- rules for back-propagation through stochastic
      variables -- and use this to develop an algorithm that allows for joint optimisation
      of the parameters of both the generative and recognition model. We demonstrate
      on several real-world data sets that the model generates realistic samples,
      provides accurate imputations of missing data and is a useful tool for high-dimensional
      data visualisation.", "venue": "International Conference on Machine Learning",
      "year": 2014, "referenceCount": 38, "citationCount": 4713, "influentialCitationCount":
      711, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2014-01-16", "journal": {"pages": "1278-1286"},
      "citationStyles": {"bibtex": "@Article{Rezende2014StochasticBA,\n author = {Danilo
      Jimenez Rezende and S. Mohamed and Daan Wierstra},\n booktitle = {International
      Conference on Machine Learning},\n pages = {1278-1286},\n title = {Stochastic
      Backpropagation and Approximate Inference in Deep Generative Models},\n year
      = {2014}\n}\n"}, "authors": [{"authorId": "1748523", "name": "Danilo Jimenez
      Rezende"}, {"authorId": "14594344", "name": "S. Mohamed"}, {"authorId": "1688276",
      "name": "Daan Wierstra"}]}}, {"contexts": ["roblems with unbounded activation
      when used ina feedback loop. More recent examples of training a generative machine
      by back-propagating into it include recent work on auto-encoding variational
      Bayes [20] and stochastic backpropagation [24]. 3 Adversarial nets The adversarial
      modeling framework is most straightforward to apply when the models are both
      multilayer perceptrons. To learn the generator\u2019s d"], "intents": [], "contextsWithIntent":
      [{"context": "roblems with unbounded activation when used ina feedback loop.
      More recent examples of training a generative machine by back-propagating into
      it include recent work on auto-encoding variational Bayes [20] and stochastic
      backpropagation [24]. 3 Adversarial nets The adversarial modeling framework
      is most straightforward to apply when the models are both multilayer perceptrons.
      To learn the generator\u2019s d", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "6bacf99c87991523a97165610b492f28c8cacbb2", "externalIds": {"DBLP":
      "journals/corr/KingmaW13", "ArXiv": "1312.6114", "MAG": "1959608418", "CorpusId":
      216078090}, "corpusId": 216078090, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/6bacf99c87991523a97165610b492f28c8cacbb2",
      "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we
      perform efficient inference and learning in directed probabilistic models, in
      the presence of continuous latent variables with intractable posterior distributions,
      and large datasets? We introduce a stochastic variational inference and learning
      algorithm that scales to large datasets and, under some mild differentiability
      conditions, even works in the intractable case. Our contributions is two-fold.
      First, we show that a reparameterization of the variational lower bound yields
      a lower bound estimator that can be straightforwardly optimized using standard
      stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
      latent variables per datapoint, posterior inference can be made especially efficient
      by fitting an approximate inference model (also called a recognition model)
      to the intractable posterior using the proposed lower bound estimator. Theoretical
      advantages are reflected in experimental results.", "venue": "International
      Conference on Learning Representations", "year": 2013, "referenceCount": 26,
      "citationCount": 20907, "influentialCitationCount": 4389, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2013-12-20", "journal":
      {"volume": "abs/1312.6114", "name": "CoRR"}, "citationStyles": {"bibtex": "@Article{Kingma2013AutoEncodingVB,\n
      author = {Diederik P. Kingma and M. Welling},\n booktitle = {International Conference
      on Learning Representations},\n journal = {CoRR},\n title = {Auto-Encoding Variational
      Bayes},\n volume = {abs/1312.6114},\n year = {2013}\n}\n"}, "authors": [{"authorId":
      "1726807", "name": "Diederik P. Kingma"}, {"authorId": "1678311", "name": "M.
      Welling"}]}}, {"contexts": [" subset of the indices of x by training a family
      of conditional models that share parameters. Essentially, one can use adversarial
      nets to implement a stochastic extension of the deterministic MP-DBM [11]. 4.
      Semi-supervised learning: features from the discriminator or inference net could
      improve performance of classi\ufb01ers when limited labeled data is available.
      5. Ef\ufb01ciency improvements: training coul"], "intents": [], "contextsWithIntent":
      [{"context": " subset of the indices of x by training a family of conditional
      models that share parameters. Essentially, one can use adversarial nets to implement
      a stochastic extension of the deterministic MP-DBM [11]. 4. Semi-supervised
      learning: features from the discriminator or inference net could improve performance
      of classi\ufb01ers when limited labeled data is available. 5. Ef\ufb01ciency
      improvements: training coul", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "5656fa5aa6e1beeb98703fc53ec112ad227c49ca", "externalIds": {"MAG":
      "2098617596", "DBLP": "conf/nips/GoodfellowMCB13", "CorpusId": 6442575}, "corpusId":
      6442575, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/5656fa5aa6e1beeb98703fc53ec112ad227c49ca",
      "title": "Multi-Prediction Deep Boltzmann Machines", "abstract": "We introduce
      the multi-prediction deep Boltzmann machine (MP-DBM). The MP-DBM can be seen
      as a single probabilistic model trained to maximize a variational approximation
      to the generalized pseudolikelihood, or as a family of recurrent nets that share
      parameters and approximately solve different inference problems. Prior methods
      of training DBMs either do not perform well on classification tasks or require
      an initial learning pass that trains the DBM greedily, one layer at a time.
      The MP-DBM does not require greedy layerwise pretraining, and outperforms the
      standard DBM at classification, classification with missing inputs, and mean
      field prediction tasks.1", "venue": "Neural Information Processing Systems",
      "year": 2013, "referenceCount": 25, "citationCount": 135, "influentialCitationCount":
      14, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2013-12-05", "journal":
      {"pages": "548-556"}, "citationStyles": {"bibtex": "@Article{Goodfellow2013MultiPredictionDB,\n
      author = {I. Goodfellow and Mehdi Mirza and Aaron C. Courville and Yoshua Bengio},\n
      booktitle = {Neural Information Processing Systems},\n pages = {548-556},\n
      title = {Multi-Prediction Deep Boltzmann Machines},\n year = {2013}\n}\n"},
      "authors": [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId":
      "153583218", "name": "Mehdi Mirza"}, {"authorId": "1760871", "name": "Aaron
      C. Courville"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "857036a25401c19e484cc32d974c90cd9a46cd66", "externalIds": {"MAG":
      "2085963752", "DBLP": "conf/allerton/RatliffBS13", "DOI": "10.1109/Allerton.2013.6736623",
      "CorpusId": 857142}, "corpusId": 857142, "publicationVenue": {"id": "e3e363b2-60f3-46d7-9067-5deaddc3f3f2",
      "name": "Allerton Conference on Communication, Control, and Computing", "type":
      "conference", "alternate_names": ["Allerton", "T Conf Commun Control Comput"]},
      "url": "https://www.semanticscholar.org/paper/857036a25401c19e484cc32d974c90cd9a46cd66",
      "title": "Characterization and computation of local Nash equilibria in continuous
      games", "abstract": "We present derivative-based necessary and sufficient conditions
      ensuring player strategies constitute local Nash equilibria in non-cooperative
      continuous games. Our results can be interpreted as generalizations of analogous
      second-order conditions for local optimality from nonlinear programming and
      optimal control theory. Drawing on this analogy, we propose an iterative steepest
      descent algorithm for numerical approximation of local Nash equilibria and provide
      a sufficient condition ensuring local convergence of the algorithm. We demonstrate
      our analytical and computational techniques by computing local Nash equilibria
      in games played on a finite-dimensional differentiable manifold or an infinite-dimensional
      Hilbert space.", "venue": "Allerton Conference on Communication, Control, and
      Computing", "year": 2013, "referenceCount": 25, "citationCount": 187, "influentialCitationCount":
      4, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Mathematics", "source": "s2-fos-model"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2013-10-01", "journal": {"pages": "917-924", "name": "2013 51st Annual Allerton
      Conference on Communication, Control, and Computing (Allerton)"}, "citationStyles":
      {"bibtex": "@Article{Ratliff2013CharacterizationAC,\n author = {L. Ratliff},\n
      booktitle = {Allerton Conference on Communication, Control, and Computing},\n
      journal = {2013 51st Annual Allerton Conference on Communication, Control, and
      Computing (Allerton)},\n pages = {917-924},\n title = {Characterization and
      computation of local Nash equilibria in continuous games},\n year = {2013}\n}\n"},
      "authors": [{"authorId": "1785356", "name": "L. Ratliff"}]}}, {"contexts": [],
      "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "2f7ad26514bce4df6c8ebc42c90383ef3a974df4", "externalIds": {"DBLP":
      "journals/corr/GoodfellowWLDMPBBB13", "ArXiv": "1308.4214", "MAG": "1872489089",
      "CorpusId": 2172854}, "corpusId": 2172854, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/2f7ad26514bce4df6c8ebc42c90383ef3a974df4",
      "title": "Pylearn2: a machine learning research library", "abstract": "Pylearn2
      is a machine learning research library. This does not just mean that it is a
      collection of machine learning algorithms that share a common API; it means
      that it has been designed for flexibility and extensibility in order to facilitate
      research projects that involve new or unusual use cases. In this paper we give
      a brief history of the library, an overview of its basic philosophy, a summary
      of the library''s architecture, and a description of how the Pylearn2 community
      functions socially.", "venue": "arXiv.org", "year": 2013, "referenceCount":
      55, "citationCount": 301, "influentialCitationCount": 15, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": "2013-08-19", "journal": {"volume": "abs/1308.4214", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Goodfellow2013Pylearn2AM,\n
      author = {I. Goodfellow and David Warde-Farley and Pascal Lamblin and Vincent
      Dumoulin and Mehdi Mirza and Razvan Pascanu and J. Bergstra and Fr\u00e9d\u00e9ric
      Bastien and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n
      title = {Pylearn2: a machine learning research library},\n volume = {abs/1308.4214},\n
      year = {2013}\n}\n"}, "authors": [{"authorId": "153440022", "name": "I. Goodfellow"},
      {"authorId": "1393680089", "name": "David Warde-Farley"}, {"authorId": "3087941",
      "name": "Pascal Lamblin"}, {"authorId": "3074927", "name": "Vincent Dumoulin"},
      {"authorId": "153583218", "name": "Mehdi Mirza"}, {"authorId": "1996134", "name":
      "Razvan Pascanu"}, {"authorId": "32837403", "name": "J. Bergstra"}, {"authorId":
      "3227028", "name": "Fr\u00e9d\u00e9ric Bastien"}, {"authorId": "1751762", "name":
      "Yoshua Bengio"}]}}, {"contexts": ["ractable for all but the most trivial instances,
      although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing
      poses a signi\ufb01cant problem for learning algorithms that rely on MCMC [3,
      5]. Deep belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the com", "on. This approach has the advantage
      that such machines can be designed to be trained by back-propagation. Prominent
      recent work in this area includes the generative stochastic network (GSN) framework
      [5], which extends generalized denoising auto-encoders [4]: both can be seen
      as de\ufb01ning a parameterized Markov chain, i.e., one learns the parameters
      of a machine that performs one step of a generative M", "ans was obtained by
      cross validation on the validation set. This procedure was introduced in Breuleux
      et al. [8] and used for various generative models for which the exact likelihood
      is not tractable [25, 3, 5]. Results are reported in Table 1. This method of
      estimating the likelihood has somewhat high variance and does not perform well
      in high dimensional spaces but it is the best method available to our k"], "intents":
      [], "contextsWithIntent": [{"context": "ractable for all but the most trivial
      instances, although they can be estimated by Markov chain Monte Carlo (MCMC)
      methods. Mixing poses a signi\ufb01cant problem for learning algorithms that
      rely on MCMC [3, 5]. Deep belief networks (DBNs) [16] are hybrid models containing
      a single undirected layer and several directed layers. While a fast approximate
      layer-wise training criterion exists, DBNs incur the com", "intents": []}, {"context":
      "on. This approach has the advantage that such machines can be designed to be
      trained by back-propagation. Prominent recent work in this area includes the
      generative stochastic network (GSN) framework [5], which extends generalized
      denoising auto-encoders [4]: both can be seen as de\ufb01ning a parameterized
      Markov chain, i.e., one learns the parameters of a machine that performs one
      step of a generative M", "intents": []}, {"context": "ans was obtained by cross
      validation on the validation set. This procedure was introduced in Breuleux
      et al. [8] and used for various generative models for which the exact likelihood
      is not tractable [25, 3, 5]. Results are reported in Table 1. This method of
      estimating the likelihood has somewhat high variance and does not perform well
      in high dimensional spaces but it is the best method available to our k", "intents":
      []}], "isInfluential": false, "citedPaper": {"paperId": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
      "externalIds": {"MAG": "2951446714", "ArXiv": "1306.1091", "DBLP": "conf/icml/BengioLAY14",
      "CorpusId": 9494295}, "corpusId": 9494295, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
      "title": "Deep Generative Stochastic Networks Trainable by Backprop", "abstract":
      "We introduce a novel training principle for probabilistic models that is an
      alternative to maximum likelihood. The proposed Generative Stochastic Networks
      (GSN) framework is based on learning the transition operator of a Markov chain
      whose stationary distribution estimates the data distribution. The transition
      distribution of the Markov chain is conditional on the previous state, generally
      involving a small move, so this conditional distribution has fewer dominant
      modes, being unimodal in the limit of small moves. Thus, it is easier to learn
      because it is easier to approximate its partition function, more like learning
      to perform supervised function approximation, with gradients that can be obtained
      by backprop. We provide theorems that generalize recent work on the probabilistic
      interpretation of denoising autoencoders and obtain along the way an interesting
      justification for dependency networks and generalized pseudolikelihood, along
      with a definition of an appropriate joint distribution and sampling mechanism
      even when the conditionals are not consistent. GSNs can be used with missing
      inputs and can be used to sample subsets of variables given the rest. We validate
      these theoretical results with experiments on two image datasets using an architecture
      that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to
      proceed with simple backprop, without the need for layerwise pretraining.",
      "venue": "International Conference on Machine Learning", "year": 2013, "referenceCount":
      41, "citationCount": 384, "influentialCitationCount": 27, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2013-06-05",
      "journal": {"pages": "226-234"}, "citationStyles": {"bibtex": "@Article{Bengio2013DeepGS,\n
      author = {Yoshua Bengio and Eric Thibodeau-Laufer and Guillaume Alain and J.
      Yosinski},\n booktitle = {International Conference on Machine Learning},\n pages
      = {226-234},\n title = {Deep Generative Stochastic Networks Trainable by Backprop},\n
      year = {2013}\n}\n"}, "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"},
      {"authorId": "1398746441", "name": "Eric Thibodeau-Laufer"}, {"authorId": "1815021",
      "name": "Guillaume Alain"}, {"authorId": "2965424", "name": "J. Yosinski"}]}},
      {"contexts": ["can be designed to be trained by back-propagation. Prominent
      recent work in this area includes the generative stochastic network (GSN) framework
      [5], which extends generalized denoising auto-encoders [4]: both can be seen
      as de\ufb01ning a parameterized Markov chain, i.e., one learns the parameters
      of a machine that performs one step of a generative Markov chain. Compared to
      GSNs, the adversarial nets fra"], "intents": [], "contextsWithIntent": [{"context":
      "can be designed to be trained by back-propagation. Prominent recent work in
      this area includes the generative stochastic network (GSN) framework [5], which
      extends generalized denoising auto-encoders [4]: both can be seen as de\ufb01ning
      a parameterized Markov chain, i.e., one learns the parameters of a machine that
      performs one step of a generative Markov chain. Compared to GSNs, the adversarial
      nets fra", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0", "externalIds": {"DBLP": "conf/nips/BengioYAV13",
      "ArXiv": "1305.6663", "MAG": "2134842679", "CorpusId": 5554756}, "corpusId":
      5554756, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
      "title": "Generalized Denoising Auto-Encoders as Generative Models", "abstract":
      "Recent work has shown how denoising and contractive autoencoders implicitly
      capture the structure of the data-generating density, in the case where the
      corruption noise is Gaussian, the reconstruction error is the squared error,
      and the data is continuous-valued. This has led to various proposals for sampling
      from this implicitly learned density function, using Langevin and Metropolis-Hastings
      MCMC. However, it remained unclear how to connect the training procedure of
      regularized auto-encoders to the implicit estimation of the underlying data-generating
      distribution when the data are discrete, or using other forms of corruption
      process and reconstruction errors. Another issue is the mathematical justification
      which is only valid in the limit of small corruption noise. We propose here
      a different attack on the problem, which deals with all these issues: arbitrary
      (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood),
      handling both discrete and continuous-valued variables, and removing the bias
      due to non-infinitesimal corruption noise (or non-infinitesimal contractive
      penalty).", "venue": "Neural Information Processing Systems", "year": 2013,
      "referenceCount": 19, "citationCount": 481, "influentialCitationCount": 37,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-05-28", "journal": {"volume": "abs/1305.6663",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Bengio2013GeneralizedDA,\n
      author = {Yoshua Bengio and L. Yao and Guillaume Alain and Pascal Vincent},\n
      booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n
      title = {Generalized Denoising Auto-Encoders as Generative Models},\n volume
      = {abs/1305.6663},\n year = {2013}\n}\n"}, "authors": [{"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "145095579", "name": "L. Yao"}, {"authorId":
      "1815021", "name": "Guillaume Alain"}, {"authorId": "145467703", "name": "Pascal
      Vincent"}]}}, {"contexts": ["at map a high-dimensional, rich sensory input to
      a class label [14, 22]. These striking successes have primarily been based on
      the backpropagation and dropout algorithms, using piecewise linear units [19,
      9, 10] which have a particularly well-behaved gradient . Deep generative models
      have had less of an impact, due to the dif\ufb01culty of approximating many
      intractable probabilistic computations that arise in ma", "23], the Toronto
      Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture
      of recti\ufb01er linear activations [19, 9] and sigmoid activations, while the
      discriminator net used maxout [10] activations. Dropout [17] was applied in
      training the discriminator net. While our theoretical framework permits the
      use of dropout and other noise at intermediate layers of the generator, we used
      no", "ersarial nets framework does not require a Markov chain for sampling.
      Because adversarial nets do not require feedback loops during generation, they
      are better able to leverage piecewise linear units [19, 9, 10], which improve
      the performance of backpropagation but have problems with unbounded activation
      when used ina feedback loop. More recent examples of training a generative machine
      by back-propagating in"], "intents": [], "contextsWithIntent": [{"context":
      "at map a high-dimensional, rich sensory input to a class label [14, 22]. These
      striking successes have primarily been based on the backpropagation and dropout
      algorithms, using piecewise linear units [19, 9, 10] which have a particularly
      well-behaved gradient . Deep generative models have had less of an impact, due
      to the dif\ufb01culty of approximating many intractable probabilistic computations
      that arise in ma", "intents": []}, {"context": "23], the Toronto Face Database
      (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10] activations. Dropout [17] was applied in training the discriminator
      net. While our theoretical framework permits the use of dropout and other noise
      at intermediate layers of the generator, we used no", "intents": []}, {"context":
      "ersarial nets framework does not require a Markov chain for sampling. Because
      adversarial nets do not require feedback loops during generation, they are better
      able to leverage piecewise linear units [19, 9, 10], which improve the performance
      of backpropagation but have problems with unbounded activation when used ina
      feedback loop. More recent examples of training a generative machine by back-propagating
      in", "intents": []}], "isInfluential": false, "citedPaper": {"paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
      "externalIds": {"MAG": "3037950864", "DBLP": "conf/icml/GoodfellowWMCB13", "ArXiv":
      "1302.4389", "CorpusId": 10600578}, "corpusId": 10600578, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29",
      "title": "Maxout Networks", "abstract": "We consider the problem of designing
      models to leverage a recently introduced approximate model averaging technique
      called dropout. We define a simple new model called maxout (so named because
      its output is the max of a set of inputs, and because it is a natural companion
      to dropout) designed to both facilitate optimization by dropout and improve
      the accuracy of dropout''s fast approximate model averaging technique. We empirically
      verify that the model successfully accomplishes both of these tasks. We use
      maxout and dropout to demonstrate state of the art classification performance
      on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "venue":
      "International Conference on Machine Learning", "year": 2013, "referenceCount":
      26, "citationCount": 2064, "influentialCitationCount": 212, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-02-18", "journal": {"pages": "1319-1327"}, "citationStyles":
      {"bibtex": "@Article{Goodfellow2013MaxoutN,\n author = {I. Goodfellow and David
      Warde-Farley and Mehdi Mirza and Aaron C. Courville and Yoshua Bengio},\n booktitle
      = {International Conference on Machine Learning},\n pages = {1319-1327},\n title
      = {Maxout Networks},\n year = {2013}\n}\n"}, "authors": [{"authorId": "153440022",
      "name": "I. Goodfellow"}, {"authorId": "1393680089", "name": "David Warde-Farley"},
      {"authorId": "153583218", "name": "Mehdi Mirza"}, {"authorId": "1760871", "name":
      "Aaron C. Courville"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}},
      {"contexts": ["s in natural language corpora. So far, the most striking successes
      in deep learning have involved discriminative models, usually those that map
      a high-dimensional, rich sensory input to a class label [14, 22]. These striking
      successes have primarily been based on the backpropagation and dropout algorithms,
      using piecewise linear units [19, 9, 10] which have a particularly well-behaved
      gradient . Deep gene"], "intents": [], "contextsWithIntent": [{"context": "s
      in natural language corpora. So far, the most striking successes in deep learning
      have involved discriminative models, usually those that map a high-dimensional,
      rich sensory input to a class label [14, 22]. These striking successes have
      primarily been based on the backpropagation and dropout algorithms, using piecewise
      linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep
      gene", "intents": []}], "isInfluential": false, "citedPaper": {"paperId": "e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "externalIds": {"MAG": "2184045248", "CorpusId": 7230302}, "corpusId": 7230302,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "abstract": "Most current speech recognition systems use hidden Markov models
      ( HMMs) to deal with the temporal variability of speech and Gaussian mixture
      models to determine how well each state of each HMM fits a frame or a short
      window of frames of coefficients that represents the acoustic input. An alternati
      ve way to evaluate the fit is to use a feedforward neural network that takes
      several frames of coefficients a s input and produces posterior probabilities
      over HMM states as output. Deep neural networks with many hidden layers, that
      are trained using new methods have been shown to outperform Gaussian mixture
      models on a variety of speech rec ognition benchmarks, sometimes by a large
      margin. This paper provides an overview of this progress and repres nts the
      shared views of four research groups who have had recent successes in using
      deep neural networks for a coustic modeling in speech recognition.", "venue":
      "", "year": 2012, "referenceCount": 79, "citationCount": 2391, "influentialCitationCount":
      161, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["Review"], "publicationDate": "2012-11-01", "journal": {"volume": "29", "pages":
      "82", "name": "IEEE Signal Processing Magazine"}, "citationStyles": {"bibtex":
      "@Article{Hinton2012DeepNN,\n author = {Geoffrey E. Hinton and L. Deng and Dong
      Yu and George E. Dahl and Abdel-rahman Mohamed and N. Jaitly and A. Senior and
      Vincent Vanhoucke and Patrick Nguyen and Tara N. Sainath and Brian Kingsbury},\n
      journal = {IEEE Signal Processing Magazine},\n pages = {82},\n title = {Deep
      Neural Networks for Acoustic Modeling in Speech Recognition},\n volume = {29},\n
      year = {2012}\n}\n"}, "authors": [{"authorId": "1695689", "name": "Geoffrey
      E. Hinton"}, {"authorId": "144718788", "name": "L. Deng"}, {"authorId": "144580027",
      "name": "Dong Yu"}, {"authorId": "35188630", "name": "George E. Dahl"}, {"authorId":
      "40360972", "name": "Abdel-rahman Mohamed"}, {"authorId": "3111912", "name":
      "N. Jaitly"}, {"authorId": "33666044", "name": "A. Senior"}, {"authorId": "2657155",
      "name": "Vincent Vanhoucke"}, {"authorId": "14902530", "name": "Patrick Nguyen"},
      {"authorId": "1784851", "name": "Tara N. Sainath"}, {"authorId": "144707379",
      "name": "Brian Kingsbury"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "b832e1ce81132533c01c2170a88009673d902fa9",
      "externalIds": {"DBLP": "journals/corr/abs-1207-4747", "MAG": "2950261534",
      "ArXiv": "1207.4747", "CorpusId": 16199335}, "corpusId": 16199335, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/b832e1ce81132533c01c2170a88009673d902fa9",
      "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "abstract":
      "Reference EPFL-ARTICLE-229252 URL: http://arxiv.org/abs/1207.4747 Record created
      on 2017-06-21, modified on 2017-06-21", "venue": "International Conference on
      Machine Learning", "year": 2012, "referenceCount": 43, "citationCount": 355,
      "influentialCitationCount": 53, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-07-19", "journal": {"pages": "53-61"},
      "citationStyles": {"bibtex": "@Article{Lacoste-Julien2012BlockCoordinateFO,\n
      author = {Simon Lacoste-Julien and Martin Jaggi and Mark W. Schmidt and Patrick
      A. Pletscher},\n booktitle = {International Conference on Machine Learning},\n
      pages = {53-61},\n title = {Block-Coordinate Frank-Wolfe Optimization for Structural
      SVMs},\n year = {2012}\n}\n"}, "authors": [{"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "2456863", "name": "Martin Jaggi"}, {"authorId":
      "145610994", "name": "Mark W. Schmidt"}, {"authorId": "7344844", "name": "Patrick
      A. Pletscher"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [],
      "isInfluential": false, "citedPaper": {"paperId": "6acc357015305f8299e228524ccc464bf77ea885",
      "externalIds": {"MAG": "2951938755", "DBLP": "conf/kdd/Lacoste-JulienPDKGG13",
      "ArXiv": "1207.4525", "DOI": "10.1145/2487575.2487592", "CorpusId": 6814936},
      "corpusId": 6814936, "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef",
      "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names":
      ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "url":
      "https://www.semanticscholar.org/paper/6acc357015305f8299e228524ccc464bf77ea885",
      "title": "SIGMa: simple greedy matching for aligning large knowledge bases",
      "abstract": "The Internet has enabled the creation of a growing number of large-scale
      knowledge bases in a variety of domains containing complementary information.
      Tools for automatically aligning these knowledge bases would make it possible
      to unify many sources of structured knowledge and answer complex queries. However,
      the efficient alignment of large-scale knowledge bases still poses a considerable
      challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm
      for aligning knowledge bases with millions of entities and facts. SiGMa is an
      iterative propagation algorithm that leverages both the structural information
      from the relationship graph and flexible similarity measures between entity
      properties in a greedy local search, which makes it scalable. Despite its greedy
      nature, our experiments indicate that SiGMa can efficiently match some of the
      world''s largest knowledge bases with high accuracy. We provide additional experiments
      on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art
      approaches both in accuracy and efficiency.", "venue": "Knowledge Discovery
      and Data Mining", "year": 2012, "referenceCount": 36, "citationCount": 179,
      "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://hal.inria.fr/hal-00918671/file/fp0172-Lacoste-Julien.pdf", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate":
      "2012-07-18", "journal": {"name": "Proceedings of the 19th ACM SIGKDD international
      conference on Knowledge discovery and data mining"}, "citationStyles": {"bibtex":
      "@Book{Lacoste-Julien2012SIGMaSG,\n author = {Simon Lacoste-Julien and Konstantina
      Palla and A. Davies and G. Kasneci and T. Graepel and Zoubin Ghahramani},\n
      booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings
      of the 19th ACM SIGKDD international conference on Knowledge discovery and data
      mining},\n title = {SIGMa: simple greedy matching for aligning large knowledge
      bases},\n year = {2012}\n}\n"}, "authors": [{"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "2524644", "name": "Konstantina Palla"},
      {"authorId": "144731918", "name": "A. Davies"}, {"authorId": "1686448", "name":
      "G. Kasneci"}, {"authorId": "1686971", "name": "T. Graepel"}, {"authorId": "1744700",
      "name": "Zoubin Ghahramani"}]}}, {"contexts": ["ractable for all but the most
      trivial instances, although they can be estimated by Markov chain Monte Carlo
      (MCMC) methods. Mixing poses a signi\ufb01cant problem for learning algorithms
      that rely on MCMC [3, 5]. Deep belief networks (DBNs) [16] are hybrid models
      containing a single undirected layer and several directed layers. While a fast
      approximate layer-wise training criterion exists, DBNs incur the com", "ability
      of the test set data under p g by \ufb01tting a Gaussian Parzen window to the
      samples generated with Gand reporting the log-likelihood under this distribution.
      The \u02d9parameter 5 Model MNIST TFD DBN [3] 138 2 1909 66 Stacked CAE [3]
      121 1:6 2110 50 Deep GSN [6] 214 1:1 1890 29 Adversarial nets 225 2 2057 26
      Table 1: Parzen window-based log-likelihood estimates. The reported numbers
      on MNIST are the ", "ans was obtained by cross validation on the validation
      set. This procedure was introduced in Breuleux et al. [8] and used for various
      generative models for which the exact likelihood is not tractable [25, 3, 5].
      Results are reported in Table 1. This method of estimating the likelihood has
      somewhat high variance and does not perform well in high dimensional spaces
      but it is the best method available to our k"], "intents": [], "contextsWithIntent":
      [{"context": "ractable for all but the most trivial instances, although they
      can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a
      signi\ufb01cant problem for learning algorithms that rely on MCMC [3, 5]. Deep
      belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the com", "intents": []}, {"context": "ability
      of the test set data under p g by \ufb01tting a Gaussian Parzen window to the
      samples generated with Gand reporting the log-likelihood under this distribution.
      The \u02d9parameter 5 Model MNIST TFD DBN [3] 138 2 1909 66 Stacked CAE [3]
      121 1:6 2110 50 Deep GSN [6] 214 1:1 1890 29 Adversarial nets 225 2 2057 26
      Table 1: Parzen window-based log-likelihood estimates. The reported numbers
      on MNIST are the ", "intents": []}, {"context": "ans was obtained by cross validation
      on the validation set. This procedure was introduced in Breuleux et al. [8]
      and used for various generative models for which the exact likelihood is not
      tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating
      the likelihood has somewhat high variance and does not perform well in high
      dimensional spaces but it is the best method available to our k", "intents":
      []}], "isInfluential": false, "citedPaper": {"paperId": "d0965d8f9842f2db960b36b528107ca362c00d1a",
      "externalIds": {"DBLP": "conf/icml/BengioMDR13", "MAG": "2952742172", "ArXiv":
      "1207.4404", "CorpusId": 1334653}, "corpusId": 1334653, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/d0965d8f9842f2db960b36b528107ca362c00d1a",
      "title": "Better Mixing via Deep Representations", "abstract": "It has been
      hypothesized, and supported with experimental evidence, that deeper representations,
      when well trained, tend to do a better job at disentangling the underlying factors
      of variation. We study the following related conjecture: better representations,
      in the sense of better disentangling, can be exploited to produce Markov chains
      that mix faster between modes. Consequently, mixing between modes would be more
      efficient at higher levels of representation. To better understand this, we
      propose a secondary conjecture: the higher-level samples fill more uniformly
      the space they occupy and the high-density manifolds tend to unfold when represented
      at higher levels. The paper discusses these hypotheses and tests them experimentally
      through visualization and measurements of mixing between modes and interpolating
      between samples.", "venue": "International Conference on Machine Learning",
      "year": 2012, "referenceCount": 37, "citationCount": 312, "influentialCitationCount":
      19, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2012-07-18", "journal": {"pages": "552-560"}, "citationStyles": {"bibtex":
      "@Article{Bengio2012BetterMV,\n author = {Yoshua Bengio and Gr\u00e9goire Mesnil
      and Yann Dauphin and Salah Rifai},\n booktitle = {International Conference on
      Machine Learning},\n pages = {552-560},\n title = {Better Mixing via Deep Representations},\n
      year = {2012}\n}\n"}, "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"},
      {"authorId": "1935910", "name": "Gr\u00e9goire Mesnil"}, {"authorId": "2921469",
      "name": "Yann Dauphin"}, {"authorId": "2425018", "name": "Salah Rifai"}]}},
      {"contexts": ["odel is also a multilayer perceptron. We refer to this special
      case as adversarial nets. In this case, we can train both models using only
      the highly successful backpropagation and dropout algorithms [17] and sample
      from the generative model using only forward propagation. No approximate inference
      or Markov chains are necessary. 2 Related work An alternative to directed graphical
      models with latent va", "base (TFD) [28], and CIFAR-10 [21]. The generator nets
      used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid activations,
      while the discriminator net used maxout [10] activations. Dropout [17] was applied
      in training the discriminator net. While our theoretical framework permits the
      use of dropout and other noise at intermediate layers of the generator, we used
      noise as the input to only t"], "intents": [], "contextsWithIntent": [{"context":
      "odel is also a multilayer perceptron. We refer to this special case as adversarial
      nets. In this case, we can train both models using only the highly successful
      backpropagation and dropout algorithms [17] and sample from the generative model
      using only forward propagation. No approximate inference or Markov chains are
      necessary. 2 Related work An alternative to directed graphical models with latent
      va", "intents": []}, {"context": "base (TFD) [28], and CIFAR-10 [21]. The generator
      nets used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid
      activations, while the discriminator net used maxout [10] activations. Dropout
      [17] was applied in training the discriminator net. While our theoretical framework
      permits the use of dropout and other noise at intermediate layers of the generator,
      we used noise as the input to only t", "intents": []}], "isInfluential": false,
      "citedPaper": {"paperId": "0060745e006c5f14ec326904119dca19c6545e51", "externalIds":
      {"ArXiv": "1207.0580", "MAG": "1904365287", "DBLP": "journals/corr/abs-1207-0580",
      "CorpusId": 14832074}, "corpusId": 14832074, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "abstract": "When a large feedforward neural network is trained on a small training
      set, it typically performs poorly on held-out test data. This \"overfitting\"
      is greatly reduced by randomly omitting half of the feature detectors on each
      training case. This prevents complex co-adaptations in which a feature detector
      is only helpful in the context of several other specific feature detectors.
      Instead, each neuron learns to detect a feature that is generally helpful for
      producing the correct answer given the combinatorially large variety of internal
      contexts in which it must operate. Random \"dropout\" gives big improvements
      on many benchmark tasks and sets new records for speech and object recognition.",
      "venue": "arXiv.org", "year": 2012, "referenceCount": 25, "citationCount": 7137,
      "influentialCitationCount": 560, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2012-07-02", "journal": {"volume": "abs/1207.0580", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Hinton2012ImprovingNN,\n author = {Geoffrey E. Hinton and
      Nitish Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},\n
      booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving neural networks
      by preventing co-adaptation of feature detectors},\n volume = {abs/1207.0580},\n
      year = {2012}\n}\n"}, "authors": [{"authorId": "1695689", "name": "Geoffrey
      E. Hinton"}, {"authorId": "2897313", "name": "Nitish Srivastava"}, {"authorId":
      "2064160", "name": "A. Krizhevsky"}, {"authorId": "1701686", "name": "I. Sutskever"},
      {"authorId": "145124475", "name": "R. Salakhutdinov"}]}}, {"contexts": ["ans
      was obtained by cross validation on the validation set. This procedure was introduced
      in Breuleux et al. [8] and used for various generative models for which the
      exact likelihood is not tractable [25, 3, 5]. Results are reported in Table
      1. This method of estimating the likelihood has somewhat high variance and does
      not perform well in high dimensional spaces but it is the best method available
      to our k"], "intents": [], "contextsWithIntent": [{"context": "ans was obtained
      by cross validation on the validation set. This procedure was introduced in
      Breuleux et al. [8] and used for various generative models for which the exact
      likelihood is not tractable [25, 3, 5]. Results are reported in Table 1. This
      method of estimating the likelihood has somewhat high variance and does not
      perform well in high dimensional spaces but it is the best method available
      to our k", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b", "externalIds": {"ArXiv": "1206.6434",
      "DBLP": "conf/icml/RifaiDVB12", "MAG": "2123284177", "CorpusId": 122643575},
      "corpusId": 122643575, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b",
      "title": "A Generative Process for Contractive Auto-Encoders", "abstract": "The
      contractive auto-encoder learns a representation of the input data that captures
      the local manifold structure around each data point, through the leading singular
      vectors of the Jacobian of the transformation from input to representation.
      The corresponding singular values specify how much local variation is plausible
      in directions associated with the corresponding singular vectors, while remaining
      in a high-density region of the input space. This paper proposes a procedure
      for generating samples that are consistent with the local structure captured
      by a contractive auto-encoder. The associated stochastic process defines a distribution
      from which one can sample, and which experimentally appears to converge quickly
      and mix well between modes, compared to Restricted Boltzmann Machines and Deep
      Belief Networks. The intuitions behind this procedure can also be used to train
      the second layer of contraction that pools lower-level features and learns to
      be invariant to the local directions of variation discovered in the first layer.
      We show that this can help learn and represent invariances present in the data
      and improve classification error.", "venue": "International Conference on Machine
      Learning", "year": 2012, "referenceCount": 19, "citationCount": 71, "influentialCitationCount":
      3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-06-26", "journal": {"volume": "", "pages":
      "1811-1818", "name": ""}, "citationStyles": {"bibtex": "@Article{Rifai2012AGP,\n
      author = {Salah Rifai and Yoshua Bengio and Yann Dauphin and Pascal Vincent},\n
      booktitle = {International Conference on Machine Learning},\n pages = {1811-1818},\n
      title = {A Generative Process for Contractive Auto-Encoders},\n year = {2012}\n}\n"},
      "authors": [{"authorId": "2425018", "name": "Salah Rifai"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "2921469", "name": "Yann Dauphin"}, {"authorId":
      "145467703", "name": "Pascal Vincent"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "6f0aef1646a83f81029204794c1950d61388ab9f",
      "externalIds": {"DBLP": "conf/icml/BachLO12", "MAG": "2963169589", "ArXiv":
      "1203.4523", "CorpusId": 3815860}, "corpusId": 3815860, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/6f0aef1646a83f81029204794c1950d61388ab9f",
      "title": "On the Equivalence between Herding and Conditional Gradient Algorithms",
      "abstract": "We show that the herding procedure of Welling (2009) takes exactly
      the form of a standard convex optimization algorithm--namely a conditional gradient
      algorithm minimizing a quadratic moment discrepancy. This link enables us to
      invoke convergence results from convex optimization and to consider faster alternatives
      for the task of approximating integrals in a reproducing kernel Hilbert space.
      We study the behavior of the different variants through numerical simulations.
      The experiments indicate that while we can improve over herding on the task
      of approximating integrals, the original herding algorithm tends to approach
      more often the maximum entropy distribution, shedding more light on the learning
      bias behind herding.", "venue": "International Conference on Machine Learning",
      "year": 2012, "referenceCount": 19, "citationCount": 161, "influentialCitationCount":
      38, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-03-20", "journal": {"volume": "abs/1203.4523",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Bach2012OnTE,\n author
      = {F. Bach and Simon Lacoste-Julien and G. Obozinski},\n booktitle = {International
      Conference on Machine Learning},\n journal = {ArXiv},\n title = {On the Equivalence
      between Herding and Conditional Gradient Algorithms},\n volume = {abs/1203.4523},\n
      year = {2012}\n}\n"}, "authors": [{"authorId": "144570279", "name": "F. Bach"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "2533906",
      "name": "G. Obozinski"}]}}, {"contexts": ["inst other models of the real-valued
      (rather than binary) version of dataset. of the Gaussians was obtained by cross
      validation on the validation set. This procedure was introduced in Breuleux
      et al. [8] and used for various generative models for which the exact likelihood
      is not tractable [25, 3, 5]. Results are reported in Table 1. This method of
      estimating the likelihood has somewhat high variance"], "intents": [], "contextsWithIntent":
      [{"context": "inst other models of the real-valued (rather than binary) version
      of dataset. of the Gaussians was obtained by cross validation on the validation
      set. This procedure was introduced in Breuleux et al. [8] and used for various
      generative models for which the exact likelihood is not tractable [25, 3, 5].
      Results are reported in Table 1. This method of estimating the likelihood has
      somewhat high variance", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c", "externalIds": {"MAG":
      "2106439909", "DBLP": "journals/neco/BreuleuxBV11", "DOI": "10.1162/NECO_a_00158",
      "CorpusId": 907908}, "corpusId": 907908, "publicationVenue": {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
      "name": "Neural Computation", "type": "journal", "alternate_names": ["Neural
      Comput"], "issn": "0899-7667", "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
      "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
      "http://www.mitpressjournals.org/loi/neco", "https://www.mitpressjournals.org/loi/neco"]},
      "url": "https://www.semanticscholar.org/paper/d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c",
      "title": "Quickly Generating Representative Samples from an RBM-Derived Process",
      "abstract": "Two recently proposed learning algorithms, herding and fast persistent
      contrastive divergence (FPCD), share the following interesting characteristic:
      they exploit changes in the model parameters while sampling in order to escape
      modes and mix better during the sampling process that is part of the learning
      algorithm. We justify such approaches as ways to escape modes while keeping
      approximately the same asymptotic distribution of the Markov chain. In that
      spirit, we extend FPCD using an idea borrowed from Herding in order to obtain
      a pure sampling algorithm, which we call the rates-FPCD sampler. Interestingly,
      this sampler can improve the model as we collect more samples, since it optimizes
      a lower bound on the log likelihood of the training data. We provide empirical
      evidence that this new algorithm displays substantially better and more robust
      mixing than Gibbs sampling.", "venue": "Neural Computation", "year": 2011, "referenceCount":
      13, "citationCount": 80, "influentialCitationCount": 5, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2011-08-01", "journal": {"volume": "23", "pages": "2058-2073", "name": "Neural
      Computation"}, "citationStyles": {"bibtex": "@Article{Breuleux2011QuicklyGR,\n
      author = {Olivier Breuleux and Yoshua Bengio and Pascal Vincent},\n booktitle
      = {Neural Computation},\n journal = {Neural Computation},\n pages = {2058-2073},\n
      title = {Quickly Generating Representative Samples from an RBM-Derived Process},\n
      volume = {23},\n year = {2011}\n}\n"}, "authors": [{"authorId": "1967465", "name":
      "Olivier Breuleux"}, {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId":
      "145467703", "name": "Pascal Vincent"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "0fe79ea0d4e3db83662d1123a9d73f4eba477ff4",
      "externalIds": {"DBLP": "journals/jmlr/Lacoste-JulienHG11", "MAG": "2099667085",
      "CorpusId": 8138646}, "corpusId": 8138646, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/0fe79ea0d4e3db83662d1123a9d73f4eba477ff4",
      "title": "Approximate inference for the loss-calibrated Bayesian", "abstract":
      "We consider the problem of approximate inference in the context of Bayesian
      decision theory. Traditional approaches focus on approximating general properties
      of the posterior, ignoring the decision task { and associated losses { for which
      the posterior could be used. We argue that this can be suboptimal and propose
      instead to loss-calibrate the approximate inference methods with respect to
      the decision task at hand. We present a general framework rooted in Bayesian
      decision theory to analyze approximate inference from the perspective of losses,
      opening up several research directions. As a rst loss-calibrated approximate
      inference attempt, we propose an EM-like algorithm on the Bayesian posterior
      risk and show how it can improve a standard approach to Gaussian process classication
      when losses are asymmetric.", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2011, "referenceCount": 22, "citationCount":
      64, "influentialCitationCount": 10, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2011-06-14", "journal": {"pages": "416-424"},
      "citationStyles": {"bibtex": "@Article{Lacoste-Julien2011ApproximateIF,\n author
      = {Simon Lacoste-Julien and Ferenc Husz\u00e1r and Zoubin Ghahramani},\n booktitle
      = {International Conference on Artificial Intelligence and Statistics},\n pages
      = {416-424},\n title = {Approximate inference for the loss-calibrated Bayesian},\n
      year = {2011}\n}\n"}, "authors": [{"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}, {"authorId": "3108066", "name": "Ferenc Husz\u00e1r"}, {"authorId":
      "1744700", "name": "Zoubin Ghahramani"}]}}, {"contexts": ["th undirected and
      directed models. Alternative criteria that do not approximate or bound the log-likelihood
      have also been proposed, such as score matching [18] and noise-contrastive estimation
      (NCE) [13]. Both of these require the learned probability density to be analytically
      speci\ufb01ed up to a normalization constant. Note that in many interesting
      generative models with several layers of latent variab"], "intents": [], "contextsWithIntent":
      [{"context": "th undirected and directed models. Alternative criteria that do
      not approximate or bound the log-likelihood have also been proposed, such as
      score matching [18] and noise-contrastive estimation (NCE) [13]. Both of these
      require the learned probability density to be analytically speci\ufb01ed up
      to a normalization constant. Note that in many interesting generative models
      with several layers of latent variab", "intents": []}], "isInfluential": false,
      "citedPaper": {"paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "externalIds":
      {"MAG": "2152790380", "DBLP": "journals/jmlr/GutmannH10", "CorpusId": 15816723},
      "corpusId": 15816723, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized
      statistical models", "abstract": "We present a new estimation principle for
      parameterized statistical models. The idea is to perform nonlinear logistic
      regression to discriminate between the observed data and some artificially generated
      noise, using the model log-density function in the regression nonlinearity.
      We show that this leads to a consistent (convergent) estimator of the parameters,
      and analyze the asymptotic variance. In particular, the method is shown to directly
      work for unnormalized models, i.e. models where the density function does not
      integrate to one. The normalization constant can be estimated just like any
      other parameter. For a tractable ICA model, we compare the method with other
      estimation methods that can be used to learn unnormalized models, including
      score matching, contrastive divergence, and maximum-likelihood where the normalization
      constant is estimated with importance sampling. Simulations show that noise-contrastive
      estimation offers the best trade-off between computational and statistical efficiency.
      The method is then applied to the modeling of natural images: We show that the
      method can successfully estimate a large-scale two-layer model and a Markov
      random field.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 15, "citationCount": 1881,
      "influentialCitationCount": 277, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Mathematics", "source": "s2-fos-model"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2010-03-31", "journal": {"pages": "297-304"}, "citationStyles":
      {"bibtex": "@Article{Gutmann2010NoisecontrastiveEA,\n author = {Michael U Gutmann
      and Aapo Hyv\u00e4rinen},\n booktitle = {International Conference on Artificial
      Intelligence and Statistics},\n pages = {297-304},\n title = {Noise-contrastive
      estimation: A new estimation principle for unnormalized statistical models},\n
      year = {2010}\n}\n"}, "authors": [{"authorId": "145992652", "name": "Michael
      U Gutmann"}, {"authorId": "1791548", "name": "Aapo Hyv\u00e4rinen"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "externalIds": {"MAG":
      "2964947096", "DBLP": "journals/jmlr/ErhanCBV10", "DOI": "10.5555/1756006.1756025",
      "CorpusId": 15796526}, "corpusId": 15796526, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract":
      "Much recent research has been devoted to learning algorithms for deep architectures
      such as Deep Belief Networks and stacks of auto-encoder variants, with impressive
      results obtained in several areas, mostly on vision and language data sets.
      The best results obtained on supervised learning tasks involve an unsupervised
      learning component, usually in an unsupervised pre-training phase. Even though
      these new algorithms have enabled training deep models, many questions remain
      as to the nature of this difficult learning problem. The main question investigated
      here is the following: how does unsupervised pre-training work? Answering this
      questions is important if learning in deep architectures is to be further improved.
      We propose several explanatory hypotheses and test them through extensive simulations.
      We empirically show the influence of pre-training with respect to architecture
      depth, model capacity, and number of training examples. The experiments confirm
      and clarify the advantage of unsupervised pre-training. The results suggest
      that unsupervised pre-training guides the learning towards basins of attraction
      of minima that support better generalization from the training data set; the
      evidence from these results supports a regularization explanation for the effect
      of pre-training.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 69, "citationCount": 2271,
      "influentialCitationCount": 77, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2010-03-01", "journal": {"pages": "201-208"}, "citationStyles": {"bibtex":
      "@Article{Erhan2010WhyDU,\n author = {D. Erhan and Aaron C. Courville and Yoshua
      Bengio and Pascal Vincent},\n booktitle = {International Conference on Artificial
      Intelligence and Statistics},\n pages = {201-208},\n title = {Why Does Unsupervised
      Pre-training Help Deep Learning?},\n year = {2010}\n}\n"}, "authors": [{"authorId":
      "1761978", "name": "D. Erhan"}, {"authorId": "1760871", "name": "Aaron C. Courville"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "120247189",
      "name": "Pascal Vincent"}]}}, {"contexts": ["ments We trained adversarial nets
      an a range of datasets including MNIST[23], the Toronto Face Database (TFD)
      [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10] activations. Dropout [17] was applied in training the discriminator
      net. While our theoretical framework permits the use of dropo", "at map a high-dimensional,
      rich sensory input to a class label [14, 22]. These striking successes have
      primarily been based on the backpropagation and dropout algorithms, using piecewise
      linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep
      generative models have had less of an impact, due to the dif\ufb01culty of approximating
      many intractable probabilistic computations that arise in ma", "ersarial nets
      framework does not require a Markov chain for sampling. Because adversarial
      nets do not require feedback loops during generation, they are better able to
      leverage piecewise linear units [19, 9, 10], which improve the performance of
      backpropagation but have problems with unbounded activation when used ina feedback
      loop. More recent examples of training a generative machine by back-propagating
      in"], "intents": [], "contextsWithIntent": [{"context": "ments We trained adversarial
      nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD)
      [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10] activations. Dropout [17] was applied in training the discriminator
      net. While our theoretical framework permits the use of dropo", "intents": []},
      {"context": "at map a high-dimensional, rich sensory input to a class label
      [14, 22]. These striking successes have primarily been based on the backpropagation
      and dropout algorithms, using piecewise linear units [19, 9, 10] which have
      a particularly well-behaved gradient . Deep generative models have had less
      of an impact, due to the dif\ufb01culty of approximating many intractable probabilistic
      computations that arise in ma", "intents": []}, {"context": "ersarial nets framework
      does not require a Markov chain for sampling. Because adversarial nets do not
      require feedback loops during generation, they are better able to leverage piecewise
      linear units [19, 9, 10], which improve the performance of backpropagation but
      have problems with unbounded activation when used ina feedback loop. More recent
      examples of training a generative machine by back-propagating in", "intents":
      []}], "isInfluential": false, "citedPaper": {"paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
      "externalIds": {"DBLP": "conf/iccv/JarrettKRL09", "MAG": "2546302380", "DOI":
      "10.1109/ICCV.2009.5459469", "CorpusId": 206769720}, "corpusId": 206769720,
      "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE
      International Conference on Computer Vision", "type": "conference", "alternate_names":
      ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url":
      "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
      "title": "What is the best multi-stage architecture for object recognition?",
      "abstract": "In many recent object recognition systems, feature extraction stages
      are generally composed of a filter bank, a non-linear transformation, and some
      sort of feature pooling layer. Most systems use only one stage of feature extraction
      in which the filters are hard-wired, or two stages where the filters in one
      or both stages are learned in supervised or unsupervised mode. This paper addresses
      three questions: 1. How does the non-linearities that follow the filter banks
      influence the recognition accuracy? 2. does learning the filter banks in an
      unsupervised or supervised manner improve the performance over random filters
      or hardwired filters? 3. Is there any advantage to using an architecture with
      two stages of feature extraction, rather than one? We show that using non-linearities
      that include rectification and local contrast normalization is the single most
      important ingredient for good accuracy on object recognition benchmarks. We
      show that two stages of feature extraction yield better accuracy than one. Most
      surprisingly, we show that a two-stage system with random filters can yield
      almost 63% recognition rate on Caltech-101, provided that the proper non-linearities
      and pooling layers are used. Finally, we show that with supervised refinement,
      the system achieves state-of-the-art performance on NORB dataset (5.6%) and
      unsupervised pre-training followed by supervised refinement produces good accuracy
      on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted,
      unprocessed MNIST dataset (0.53%).", "venue": "IEEE International Conference
      on Computer Vision", "year": 2009, "referenceCount": 54, "citationCount": 2268,
      "influentialCitationCount": 139, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2009-09-01", "journal": {"pages": "2146-2153", "name": "2009 IEEE 12th International
      Conference on Computer Vision"}, "citationStyles": {"bibtex": "@Article{Jarrett2009WhatIT,\n
      author = {Kevin Jarrett and K. Kavukcuoglu and Marc''Aurelio Ranzato and Yann
      LeCun},\n booktitle = {IEEE International Conference on Computer Vision},\n
      journal = {2009 IEEE 12th International Conference on Computer Vision},\n pages
      = {2146-2153},\n title = {What is the best multi-stage architecture for object
      recognition?},\n year = {2009}\n}\n"}, "authors": [{"authorId": "2077257730",
      "name": "Kevin Jarrett"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"},
      {"authorId": "1706809", "name": "Marc''Aurelio Ranzato"}, {"authorId": "1688882",
      "name": "Yann LeCun"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
      "externalIds": {"MAG": "2108598243", "DBLP": "conf/cvpr/DengDSLL009", "DOI":
      "10.1109/CVPR.2009.5206848", "CorpusId": 57246310}, "corpusId": 57246310, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
      "title": "ImageNet: A large-scale hierarchical image database", "abstract":
      "The explosion of image data on the Internet has the potential to foster more
      sophisticated and robust models and algorithms to index, retrieve, organize
      and interact with images and multimedia data. But exactly how such data can
      be harnessed and organized remains a critical problem. We introduce here a new
      database called \u201cImageNet\u201d, a large-scale ontology of images built
      upon the backbone of the WordNet structure. ImageNet aims to populate the majority
      of the 80,000 synsets of WordNet with an average of 500-1000 clean and full
      resolution images. This will result in tens of millions of annotated images
      organized by the semantic hierarchy of WordNet. This paper offers a detailed
      analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and
      3.2 million images in total. We show that ImageNet is much larger in scale and
      diversity and much more accurate than the current image datasets. Constructing
      such a large-scale database is a challenging task. We describe the data collection
      scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of
      ImageNet through three simple applications in object recognition, image classification
      and automatic object clustering. We hope that the scale, accuracy, diversity
      and hierarchical structure of ImageNet can offer unparalleled opportunities
      to researchers in the computer vision community and beyond.", "venue": "2009
      IEEE Conference on Computer Vision and Pattern Recognition", "year": 2009, "referenceCount":
      27, "citationCount": 49533, "influentialCitationCount": 9319, "isOpenAccess":
      true, "openAccessPdf": {"url": "http://www.image-net.org/papers/imagenet_cvpr09.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2009-06-20", "journal": {"pages": "248-255", "name": "2009
      IEEE Conference on Computer Vision and Pattern Recognition"}, "citationStyles":
      {"bibtex": "@Article{Deng2009ImageNetAL,\n author = {Jia Deng and Wei Dong and
      R. Socher and Li-Jia Li and K. Li and Li Fei-Fei},\n booktitle = {2009 IEEE
      Conference on Computer Vision and Pattern Recognition},\n journal = {2009 IEEE
      Conference on Computer Vision and Pattern Recognition},\n pages = {248-255},\n
      title = {ImageNet: A large-scale hierarchical image database},\n year = {2009}\n}\n"},
      "authors": [{"authorId": "153302678", "name": "Jia Deng"}, {"authorId": "144847596",
      "name": "Wei Dong"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId":
      "2040091191", "name": "Li-Jia Li"}, {"authorId": "94451829", "name": "K. Li"},
      {"authorId": "48004138", "name": "Li Fei-Fei"}]}}, {"contexts": ["rnative to
      directed graphical models with latent variables are undirected graphical models
      with latent variables, such as restricted Boltzmann machines (RBMs) [27, 16],
      deep Boltzmann machines (DBMs) [26] and their numerous variants. The interactions
      within such models are represented as the product of unnormalized potential
      functions, normalized by a global summation/integration over all states of th"],
      "intents": [], "contextsWithIntent": [{"context": "rnative to directed graphical
      models with latent variables are undirected graphical models with latent variables,
      such as restricted Boltzmann machines (RBMs) [27, 16], deep Boltzmann machines
      (DBMs) [26] and their numerous variants. The interactions within such models
      are represented as the product of unnormalized potential functions, normalized
      by a global summation/integration over all states of th", "intents": []}], "isInfluential":
      false, "citedPaper": {"paperId": "ddc45fad8d15771d9f1f8579331458785b2cdd93",
      "externalIds": {"MAG": "189596042", "DBLP": "journals/jmlr/SalakhutdinovH09",
      "CorpusId": 877639}, "corpusId": 877639, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/ddc45fad8d15771d9f1f8579331458785b2cdd93",
      "title": "Deep Boltzmann Machines", "abstract": "We present a new learning algorithm
      for Boltzmann machines that contain many layers of hidden variables. Data-dependent
      expectations are estimated using a variational approximation that tends to focus
      on a single mode, and dataindependent expectations are approximated using persistent
      Markov chains. The use of two quite different techniques for estimating the
      two types of expectation that enter into the gradient of the log-likelihood
      makes it practical to learn Boltzmann machines with multiple hidden layers and
      millions of parameters. The learning can be made more efficient by using a layer-by-layer
      \u201cpre-training\u201d phase that allows variational inference to be initialized
      with a single bottomup pass. We present results on the MNIST and NORB datasets
      showing that deep Boltzmann machines learn good generative models and perform
      well on handwritten digit and visual object recognition tasks.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2009, "referenceCount":
      22, "citationCount": 2203, "influentialCitationCount": 246, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2009-04-15", "journal":
      {"pages": "448-455"}, "citationStyles": {"bibtex": "@Article{Salakhutdinov2009DeepBM,\n
      author = {R. Salakhutdinov and Geoffrey E. Hinton},\n booktitle = {International
      Conference on Artificial Intelligence and Statistics},\n pages = {448-455},\n
      title = {Deep Boltzmann Machines},\n year = {2009}\n}\n"}, "authors": [{"authorId":
      "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1695689", "name": "Geoffrey
      E. Hinton"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9",
      "externalIds": {"MAG": "2122683976", "DBLP": "conf/nips/Lacoste-JulienSJ08",
      "CorpusId": 6500077}, "corpusId": 6500077, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9",
      "title": "DiscLDA: Discriminative Learning for Dimensionality Reduction and
      Classification", "abstract": "Probabilistic topic models have become popular
      as methods for dimensionality reduction in collections of text documents or
      images. These models are usually treated as generative models and trained using
      maximum likelihood or Bayesian methods. In this paper, we discuss an alternative:
      a discriminative framework in which we assume that supervised side information
      is present, and in which we wish to take that side information into account
      in finding a reduced dimensionality representation. Specifically, we present
      DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in
      which a class-dependent linear transformation is introduced on the topic mixture
      proportions. This parameter is estimated by maximizing the conditional likelihood.
      By using the transformed topic mixture proportions as a new representation of
      documents, we obtain a supervised dimensionality reduction algorithm that uncovers
      the latent structure in a document collection while preserving predictive power
      for the task of classification. We compare the predictive power of the latent
      structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification
      task and show how our model can identify shared topics across classes as well
      as class-dependent topics.", "venue": "Neural Information Processing Systems",
      "year": 2008, "referenceCount": 11, "citationCount": 437, "influentialCitationCount":
      33, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2008-12-08", "journal": {"pages": "897-904"}, "citationStyles": {"bibtex":
      "@Article{Lacoste-Julien2008DiscLDADL,\n author = {Simon Lacoste-Julien and
      Fei Sha and Michael I. Jordan},\n booktitle = {Neural Information Processing
      Systems},\n pages = {897-904},\n title = {DiscLDA: Discriminative Learning for
      Dimensionality Reduction and Classification},\n year = {2008}\n}\n"}, "authors":
      [{"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "145757665",
      "name": "Fei Sha"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]}},
      {"contexts": ["ive models with several layers of latent variables (such as DBNs
      and DBMs), it is not even possible to derive a tractable unnormalized probability
      density. Some models such as denoising auto-encoders [30] and contractive autoencoders
      have learning rules very similar to score matching applied to RBMs. In NCE,
      as in this work, a discriminative training criterion is employed to \ufb01t
      a generative model. How"], "intents": [], "contextsWithIntent": [{"context":
      "ive models with several layers of latent variables (such as DBNs and DBMs),
      it is not even possible to derive a tractable unnormalized probability density.
      Some models such as denoising auto-encoders [30] and contractive autoencoders
      have learning rules very similar to score matching applied to RBMs. In NCE,
      as in this work, a discriminative training criterion is employed to \ufb01t
      a generative model. How", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "843959ffdccf31c6694d135fad07425924f785b1", "externalIds": {"MAG":
      "2025768430", "DBLP": "conf/icml/VincentLBM08", "DOI": "10.1145/1390156.1390294",
      "CorpusId": 207168299}, "corpusId": 207168299, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": "Previous work has shown that the difficulties in learning deep
      generative or discriminative models can be overcome by an initial unsupervised
      learning step that maps inputs to useful intermediate representations. We introduce
      and motivate a new training principle for unsupervised learning of a representation
      based on the idea of making the learned representations robust to partial corruption
      of the input pattern. This approach can be used to train autoencoders, and these
      denoising autoencoders can be stacked to initialize deep architectures. The
      algorithm can be motivated from a manifold learning and information theoretic
      perspective or from a generative model perspective. Comparative experiments
      clearly show the surprising advantage of corrupting the input of autoencoders
      on a pattern classification benchmark suite.", "venue": "International Conference
      on Machine Learning", "year": 2008, "referenceCount": 30, "citationCount": 6684,
      "influentialCitationCount": 515, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2008-07-05", "journal": {"pages": "1096-1103"},
      "citationStyles": {"bibtex": "@Article{Vincent2008ExtractingAC,\n author = {Pascal
      Vincent and H. Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},\n
      booktitle = {International Conference on Machine Learning},\n pages = {1096-1103},\n
      title = {Extracting and composing robust features with denoising autoencoders},\n
      year = {2008}\n}\n"}, "authors": [{"authorId": "120247189", "name": "Pascal
      Vincent"}, {"authorId": "1777528", "name": "H. Larochelle"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "1798462", "name": "Pierre-Antoine Manzagol"}]}},
      {"contexts": [" ksteps of optimizing Dand one step of optimizing G. This results
      in Dbeing maintained near its optimal solution, so long as Gchanges slowly enough.
      This strategy is analogous to the way that SML/PCD [31, 29] training maintains
      samples from a Markov chain from one learning step to the next in order to avoid
      burning in a Markov chain as part of the inner loop of learning. The procedure
      is formally presente"], "intents": [], "contextsWithIntent": [{"context": "
      ksteps of optimizing Dand one step of optimizing G. This results in Dbeing maintained
      near its optimal solution, so long as Gchanges slowly enough. This strategy
      is analogous to the way that SML/PCD [31, 29] training maintains samples from
      a Markov chain from one learning step to the next in order to avoid burning
      in a Markov chain as part of the inner loop of learning. The procedure is formally
      presente", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "73d6a26f407db77506959fdf3f7b853e44f3844a", "externalIds": {"MAG": "2116825644",
      "DBLP": "conf/icml/Tieleman08", "DOI": "10.1145/1390156.1390290", "CorpusId":
      7330145}, "corpusId": 7330145, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/73d6a26f407db77506959fdf3f7b853e44f3844a",
      "title": "Training restricted Boltzmann machines using approximations to the
      likelihood gradient", "abstract": "A new algorithm for training Restricted Boltzmann
      Machines is introduced. The algorithm, named Persistent Contrastive Divergence,
      is different from the standard Contrastive Divergence algorithms in that it
      aims to draw samples from almost exactly the model distribution. It is compared
      to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on
      the tasks of modeling and classifying various types of data. The Persistent
      Contrastive Divergence algorithm outperforms the other algorithms, and is equally
      fast and simple.", "venue": "International Conference on Machine Learning",
      "year": 2008, "referenceCount": 22, "citationCount": 993, "influentialCitationCount":
      133, "isOpenAccess": true, "openAccessPdf": {"url": "http://icml2008.cs.helsinki.fi/papers/638.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2008-07-05", "journal": {"pages": "1064-1071"},
      "citationStyles": {"bibtex": "@Article{Tieleman2008TrainingRB,\n author = {T.
      Tieleman},\n booktitle = {International Conference on Machine Learning},\n pages
      = {1064-1071},\n title = {Training restricted Boltzmann machines using approximations
      to the likelihood gradient},\n year = {2008}\n}\n"}, "authors": [{"authorId":
      "2957517", "name": "T. Tieleman"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "7669c8e2edb9409dec9911ff20a5ba2d53d48ac0",
      "externalIds": {"MAG": "2100564780", "DBLP": "journals/jmlr/TaskarLJ06", "DOI":
      "10.5555/1248547.1248607", "CorpusId": 14842644}, "corpusId": 14842644, "publicationVenue":
      {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine learning
      research", "type": "journal", "alternate_names": ["Journal of Machine Learning
      Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435", "alternate_issns":
      ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/", "alternate_urls":
      ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/7669c8e2edb9409dec9911ff20a5ba2d53d48ac0",
      "title": "Structured Prediction, Dual Extragradient and Bregman Projections",
      "abstract": "We present a simple and scalable algorithm for maximum-margin estimation
      of structured output models, including an important class of Markov networks
      and combinatorial models. We formulate the estimation problem as a convex-concave
      saddle-point problem that allows us to use simple projection methods based on
      the dual extragradient algorithm (Nesterov, 2003). The projection step can be
      solved using dynamic programming or combinatorial algorithms for min-cost convex
      flow, depending on the structure of the problem. We show that this approach
      provides a memory-efficient alternative to formulations based on reductions
      to a quadratic program (QP). We analyze the convergence of the method and present
      experiments on two very different structured prediction tasks: 3D image segmentation
      and word alignment, illustrating the favorable scaling properties of our algorithm.",
      "venue": "Journal of machine learning research", "year": 2006, "referenceCount":
      44, "citationCount": 129, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2006-12-01", "journal":
      {"volume": "7", "pages": "1627-1653", "name": "J. Mach. Learn. Res."}, "citationStyles":
      {"bibtex": "@Article{Taskar2006StructuredPD,\n author = {B. Taskar and Simon
      Lacoste-Julien and Michael I. Jordan},\n booktitle = {Journal of machine learning
      research},\n journal = {J. Mach. Learn. Res.},\n pages = {1627-1653},\n title
      = {Structured Prediction, Dual Extragradient and Bregman Projections},\n volume
      = {7},\n year = {2006}\n}\n"}, "authors": [{"authorId": "1685978", "name": "B.
      Taskar"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "1694621", "name": "Michael I. Jordan"}]}}, {"contexts": ["ins are necessary.
      2 Related work An alternative to directed graphical models with latent variables
      are undirected graphical models with latent variables, such as restricted Boltzmann
      machines (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous
      variants. The interactions within such models are represented as the product
      of unnormalized potential functions, normalized by a global summat", "l instances,
      although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing
      poses a signi\ufb01cant problem for learning algorithms that rely on MCMC [3,
      5]. Deep belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the computational dif\ufb01culties associated "],
      "intents": [], "contextsWithIntent": [{"context": "ins are necessary. 2 Related
      work An alternative to directed graphical models with latent variables are undirected
      graphical models with latent variables, such as restricted Boltzmann machines
      (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous variants.
      The interactions within such models are represented as the product of unnormalized
      potential functions, normalized by a global summat", "intents": []}, {"context":
      "l instances, although they can be estimated by Markov chain Monte Carlo (MCMC)
      methods. Mixing poses a signi\ufb01cant problem for learning algorithms that
      rely on MCMC [3, 5]. Deep belief networks (DBNs) [16] are hybrid models containing
      a single undirected layer and several directed layers. While a fast approximate
      layer-wise training criterion exists, DBNs incur the computational dif\ufb01culties
      associated ", "intents": []}], "isInfluential": false, "citedPaper": {"paperId":
      "8978cf7574ceb35f4c3096be768c7547b28a35d0", "externalIds": {"DBLP": "journals/neco/HintonOT06",
      "MAG": "2136922672", "DOI": "10.1162/neco.2006.18.7.1527", "CorpusId": 2309950,
      "PubMed": "16764513"}, "corpusId": 2309950, "publicationVenue": {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
      "name": "Neural Computation", "type": "journal", "alternate_names": ["Neural
      Comput"], "issn": "0899-7667", "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
      "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
      "http://www.mitpressjournals.org/loi/neco", "https://www.mitpressjournals.org/loi/neco"]},
      "url": "https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0",
      "title": "A Fast Learning Algorithm for Deep Belief Nets", "abstract": "We show
      how to use complementary priors to eliminate the explaining-away effects that
      make inference difficult in densely connected belief nets that have many hidden
      layers. Using complementary priors, we derive a fast, greedy algorithm that
      can learn deep, directed belief networks one layer at a time, provided the top
      two layers form an undirected associative memory. The fast, greedy algorithm
      is used to initialize a slower learning procedure that fine-tunes the weights
      using a contrastive version of the wake-sleep algorithm. After fine-tuning,
      a network with three hidden layers forms a very good generative model of the
      joint distribution of handwritten digit images and their labels. This generative
      model gives better digit classification than the best discriminative learning
      algorithms. The low-dimensional manifolds on which the digits lie are modeled
      by long ravines in the free-energy landscape of the top-level associative memory,
      and it is easy to explore these ravines by using the directed connections to
      display what the associative memory has in mind.", "venue": "Neural Computation",
      "year": 2006, "referenceCount": 32, "citationCount": 15204, "influentialCitationCount":
      1271, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
      "Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2006-07-01", "journal": {"volume": "18", "pages": "1527-1554", "name": "Neural
      Computation"}, "citationStyles": {"bibtex": "@Article{Hinton2006AFL,\n author
      = {Geoffrey E. Hinton and Simon Osindero and Y. Teh},\n booktitle = {Neural
      Computation},\n journal = {Neural Computation},\n pages = {1527-1554},\n title
      = {A Fast Learning Algorithm for Deep Belief Nets},\n volume = {18},\n year
      = {2006}\n}\n"}, "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"},
      {"authorId": "2217144", "name": "Simon Osindero"}, {"authorId": "1725303", "name":
      "Y. Teh"}]}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "ddddd9be714919db194a371070a1e13a6a0546b4",
      "externalIds": {"MAG": "2122244345", "DBLP": "conf/naacl/Lacoste-JulienTKJ06",
      "ACL": "N06-1015", "DOI": "10.3115/1220835.1220850", "CorpusId": 5409635}, "corpusId":
      5409635, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/ddddd9be714919db194a371070a1e13a6a0546b4",
      "title": "Word Alignment via Quadratic Assignment", "abstract": "Recently, discriminative
      word alignment methods have achieved state-of-the-art accuracies by extending
      the range of information sources that can be easily incorporated into aligners.
      The chief advantage of a discriminative framework is the ability to score alignments
      based on arbitrary features of the matching word tokens, including orthographic
      form, predictions of other models, lexical context and so on. However, the proposed
      bipartite matching model of Taskar et al. (2005), despite being tractable and
      effective, has two important limitations. First, it is limited by the restriction
      that words have fertility of at most one. More importantly, first order correlations
      between consecutive words cannot be directly captured by the model. In this
      work, we address these limitations by enriching the model form. We give estimation
      and inference algorithms for these enhancements. Our best model achieves a relative
      AER reduction of 25% over the basic matching formulation, outperforming intersected
      IBM Model 4 without using any overly compute-intensive features. By including
      predictions of other models as features, we achieve AER of 3.8 on the standard
      Hansards dataset.", "venue": "North American Chapter of the Association for
      Computational Linguistics", "year": 2006, "referenceCount": 11, "citationCount":
      100, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://dl.acm.org/doi/pdf/10.3115/1220835.1220850", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2006-06-04", "journal":
      {"volume": "", "pages": "112-119", "name": ""}, "citationStyles": {"bibtex":
      "@Article{Lacoste-Julien2006WordAV,\n author = {Simon Lacoste-Julien and B.
      Taskar and D. Klein and Michael I. Jordan},\n booktitle = {North American Chapter
      of the Association for Computational Linguistics},\n pages = {112-119},\n title
      = {Word Alignment via Quadratic Assignment},\n year = {2006}\n}\n"}, "authors":
      [{"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1685978",
      "name": "B. Taskar"}, {"authorId": "38666915", "name": "D. Klein"}, {"authorId":
      "1694621", "name": "Michael I. Jordan"}]}}, {"contexts": [], "intents": [],
      "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "fd5590d9696be6d9e0807c6660826f5351093790", "externalIds": {"MAG": "2161002641",
      "DBLP": "conf/nips/TaskarLJ05", "CorpusId": 2298202}, "corpusId": 2298202, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/fd5590d9696be6d9e0807c6660826f5351093790",
      "title": "Structured Prediction via the Extragradient Method", "abstract": "We
      present a simple and scalable algorithm for large-margin estimation of structured
      models, including an important class of Markov networks and combinatorial models.
      We formulate the estimation problem as a convex-concave saddle-point problem
      and apply the extragradient method, yielding an algorithm with linear convergence
      using simple gradient and projection calculations. The projection step can be
      solved using combinatorial algorithms for min-cost quadratic flow. This makes
      the approach an efficient alternative to formulations based on reductions to
      a quadratic program (QP). We present experiments on two very different structured
      prediction tasks: 3D image segmentation and word alignment, illustrating the
      favorable scaling properties of our algorithm.", "venue": "Neural Information
      Processing Systems", "year": 2005, "referenceCount": 33, "citationCount": 64,
      "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2005-12-05", "journal": {"pages": "1345-1352"},
      "citationStyles": {"bibtex": "@Article{Taskar2005StructuredPV,\n author = {B.
      Taskar and Simon Lacoste-Julien and Michael I. Jordan},\n booktitle = {Neural
      Information Processing Systems},\n pages = {1345-1352},\n title = {Structured
      Prediction via the Extragradient Method},\n year = {2005}\n}\n"}, "authors":
      [{"authorId": "1685978", "name": "B. Taskar"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "64a007a07cbeab1b6949f196e58fdbe93ef1a297", "externalIds":
      {"DBLP": "conf/naacl/TaskarLK05", "MAG": "2119224513", "ACL": "H05-1010", "DOI":
      "10.3115/1220575.1220585", "CorpusId": 2379886}, "corpusId": 2379886, "publicationVenue":
      {"id": "f8e3f8d0-0f40-48c0-b3c0-0c540237b859", "name": "Human Language Technology
      - The Baltic Perspectiv", "type": "conference", "alternate_names": ["Human Language
      Technology", "HLT", "Hum Lang Technol", "Hum Lang Technol  Balt Perspect"]},
      "url": "https://www.semanticscholar.org/paper/64a007a07cbeab1b6949f196e58fdbe93ef1a297",
      "title": "A Discriminative Matching Approach to Word Alignment", "abstract":
      "We present a discriminative, large-margin approach to feature-based matching
      for word alignment. In this framework, pairs of word tokens receive a matching
      score, which is based on features of that pair, including measures of association
      between the words, distortion between their positions, similarity of the orthographic
      form, and so on. Even with only 100 labeled training examples and simple features
      which incorporate counts from a large unlabeled corpus, we achieve AER performance
      close to IBM Model 4, in much less time. Including Model 4 predictions as features,
      we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
      "venue": "Human Language Technology - The Baltic Perspectiv", "year": 2005,
      "referenceCount": 18, "citationCount": 188, "influentialCitationCount": 17,
      "isOpenAccess": true, "openAccessPdf": {"url": "http://dl.acm.org/ft_gateway.cfm?id=1220585&type=pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2005-10-06", "journal": {"pages": "73-80"}, "citationStyles": {"bibtex": "@Article{Taskar2005ADM,\n
      author = {B. Taskar and Simon Lacoste-Julien and D. Klein},\n booktitle = {Human
      Language Technology - The Baltic Perspectiv},\n pages = {73-80},\n title = {A
      Discriminative Matching Approach to Word Alignment},\n year = {2005}\n}\n"},
      "authors": [{"authorId": "1685978", "name": "B. Taskar"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "38666915", "name": "D. Klein"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": "a70563b755ee309b11c49062af53020214046fcd", "externalIds":
      {"MAG": "2540599577", "DOI": "10.1109/CACSD.2004.1393852", "CorpusId": 1899687},
      "corpusId": 1899687, "publicationVenue": {"id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
      "name": "IEEE International Conference on Robotics and Automation", "type":
      "conference", "alternate_names": ["International Conference on Robotics and
      Automation", "Int Conf Robot Autom", "ICRA", "IEEE Int Conf Robot Autom"], "issn":
      "2152-4092", "alternate_issns": ["2379-9544"], "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
      "alternate_urls": ["http://www.ncsu.edu/IEEE-RAS/"]}, "url": "https://www.semanticscholar.org/paper/a70563b755ee309b11c49062af53020214046fcd",
      "title": "Meta-modelling hybrid formalisms", "abstract": "This article demonstrates
      how meta-modelling can simplify the construction of domain-and formalism-specific
      modelling environments. Using AToM3 (a tool for multi-formalism and meta-modelling
      developed at McGill University), a model is constructed of a hybrid formalism,
      HS, that combines event scheduling constructs with ordinary differential equations.
      From this specification, an HS-specific visual modelling environment is synthesized.
      For the purpose of this demonstration, a simple hybrid model of a bouncing ball
      is modelled in this environment. It is envisioned that the future of modelling
      and simulation in general, and more specifically in hybrid dynamic systems design
      lies in domain-specific computer automated multi-paradigm modelling (CAMPaM)
      which combines multi-abstraction, multi-formalism, and meta-modelling. The small
      example presented in this article demonstrates the feasibility of this approach",
      "venue": "IEEE International Conference on Robotics and Automation", "year":
      2004, "referenceCount": 26, "citationCount": 13, "influentialCitationCount":
      0, "isOpenAccess": true, "openAccessPdf": {"url": "http://msdl.cs.mcgill.ca/people/mosterman/campam/cacsd04/index.html/lacoste-julien.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}],
      "publicationTypes": ["Conference"], "publicationDate": "2004-09-02", "journal":
      {"pages": "65-70", "name": "2004 IEEE International Conference on Robotics and
      Automation (IEEE Cat. No.04CH37508)"}, "citationStyles": {"bibtex": "@Conference{Lacoste-Julien2004MetamodellingHF,\n
      author = {Simon Lacoste-Julien and H. Vangheluwe and Juan de Lara and P. Mosterman},\n
      booktitle = {IEEE International Conference on Robotics and Automation},\n journal
      = {2004 IEEE International Conference on Robotics and Automation (IEEE Cat.
      No.04CH37508)},\n pages = {65-70},\n title = {Meta-modelling hybrid formalisms},\n
      year = {2004}\n}\n"}, "authors": [{"authorId": "2239746115", "name": "Simon
      Lacoste-Julien"}, {"authorId": "1762640", "name": "H. Vangheluwe"}, {"authorId":
      "2239747160", "name": "Juan de Lara"}, {"authorId": "1779212", "name": "P. Mosterman"}]}},
      {"contexts": ["If your task is to determine P(Y | X ), then why would you want
      to model P(X ,Y ) as an intermediate step? Ng and Jordan (2001) shows that generative
      models can be useful even for traditionally discriminative problems."], "intents":
      [], "contextsWithIntent": [{"context": "If your task is to determine P(Y | X
      ), then why would you want to model P(X ,Y ) as an intermediate step? Ng and
      Jordan (2001) shows that generative models can be useful even for traditionally
      discriminative problems.", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "90929a6aa901ba958eb4960aeeb594c752e08369", "externalIds": {"MAG":
      "2163614729", "DBLP": "conf/nips/NgJ01", "CorpusId": 296750}, "corpusId": 296750,
      "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural
      Information Processing Systems", "type": "conference", "alternate_names": ["Neural
      Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url":
      "https://www.semanticscholar.org/paper/90929a6aa901ba958eb4960aeeb594c752e08369",
      "title": "On Discriminative vs. Generative Classifiers: A comparison of logistic
      regression and naive Bayes", "abstract": "We compare discriminative and generative
      learning as typified by logistic regression and naive Bayes. We show, contrary
      to a widely-held belief that discriminative classifiers are almost always to
      be preferred, that there can often be two distinct regimes of performance as
      the training set size is increased, one in which each algorithm does better.
      This stems from the observation\u2014which is borne out in repeated experiments\u2014that
      while discriminative learning has lower asymptotic error, a generative classifier
      may also approach its (higher) asymptotic error much faster.", "venue": "Neural
      Information Processing Systems", "year": 2001, "referenceCount": 7, "citationCount":
      2394, "influentialCitationCount": 86, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2001-01-03", "journal":
      {"pages": "841-848"}, "citationStyles": {"bibtex": "@Article{Ng2001OnDV,\n author
      = {A. Ng and Michael I. Jordan},\n booktitle = {Neural Information Processing
      Systems},\n pages = {841-848},\n title = {On Discriminative vs. Generative Classifiers:
      A comparison of logistic regression and naive Bayes},\n year = {2001}\n}\n"},
      "authors": [{"authorId": "34699434", "name": "A. Ng"}, {"authorId": "1694621",
      "name": "Michael I. Jordan"}]}}, {"contexts": [" ksteps of optimizing Dand one
      step of optimizing G. This results in Dbeing maintained near its optimal solution,
      so long as Gchanges slowly enough. This strategy is analogous to the way that
      SML/PCD [31, 29] training maintains samples from a Markov chain from one learning
      step to the next in order to avoid burning in a Markov chain as part of the
      inner loop of learning. The procedure is formally presente"], "intents": [],
      "contextsWithIntent": [{"context": " ksteps of optimizing Dand one step of optimizing
      G. This results in Dbeing maintained near its optimal solution, so long as Gchanges
      slowly enough. This strategy is analogous to the way that SML/PCD [31, 29] training
      maintains samples from a Markov chain from one learning step to the next in
      order to avoid burning in a Markov chain as part of the inner loop of learning.
      The procedure is formally presente", "intents": []}], "isInfluential": false,
      "citedPaper": {"paperId": "ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e", "externalIds":
      {"MAG": "1990838964", "DOI": "10.1080/17442509908834179", "CorpusId": 15419929},
      "corpusId": 15419929, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e",
      "title": "On the convergence of markovian stochastic algorithms with rapidly
      decreasing ergodicity rates", "abstract": "We analyse the convergence of stochastic
      algorithms with Markovian noise when the ergodicity of the Markov chain governing
      the noise rapidly decreases as the control parameter tends to infinity. In such
      a case, there may be a positive probability of divergence of the algorithm in
      the classic Robbins-Monro form. We provide sufficient condition which ensure
      convergence. Moreover, we analyse the asymptotic behaviour of these algorithms
      and state a diffusion approximation theorem", "venue": "", "year": 1999, "referenceCount":
      24, "citationCount": 162, "influentialCitationCount": 23, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "1999-02-01",
      "journal": {"volume": "65", "pages": "177-228", "name": "Stochastics and Stochastics
      Reports"}, "citationStyles": {"bibtex": "@Article{Younes1999OnTC,\n author =
      {L. Younes},\n journal = {Stochastics and Stochastics Reports},\n pages = {177-228},\n
      title = {On the convergence of markovian stochastic algorithms with rapidly
      decreasing ergodicity rates},\n volume = {65},\n year = {1999}\n}\n"}, "authors":
      [{"authorId": "1721284", "name": "L. Younes"}]}}, {"contexts": [], "intents":
      [], "contextsWithIntent": [], "isInfluential": false, "citedPaper": {"paperId":
      "629cc74dcaf655feea40f64cd74617ac884ed0f8", "externalIds": {"MAG": "2150218618",
      "DOI": "10.1016/s0898-1221(99)90389-9", "CorpusId": 62488180}, "corpusId": 62488180,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/629cc74dcaf655feea40f64cd74617ac884ed0f8",
      "title": "Graphical Models for Machine Learning and Digital Communication",
      "abstract": null, "venue": "", "year": 1998, "referenceCount": 6, "citationCount":
      632, "influentialCitationCount": 35, "isOpenAccess": true, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": "1998-06-26", "journal": {"volume": "", "name": ""},
      "citationStyles": {"bibtex": "@Inproceedings{Frey1998GraphicalMF,\n author =
      {B. Frey},\n title = {Graphical Models for Machine Learning and Digital Communication},\n
      year = {1998}\n}\n"}, "authors": [{"authorId": "1749650", "name": "B. Frey"}]}},
      {"contexts": ["input to both Gand D. 2. Learned approximate inference can be
      performed by training an auxiliary network to predict z given x. This is similar
      to the inference net trained by the wake-sleep algorithm [15] but with the advantage
      that the inference net may be trained for a \ufb01xed generator net after the
      generator net has \ufb01nished training. 7 3.One can approximately model all
      conditionals p(x S jx 6S) where"], "intents": [], "contextsWithIntent": [{"context":
      "input to both Gand D. 2. Learned approximate inference can be performed by
      training an auxiliary network to predict z given x. This is similar to the inference
      net trained by the wake-sleep algorithm [15] but with the advantage that the
      inference net may be trained for a \ufb01xed generator net after the generator
      net has \ufb01nished training. 7 3.One can approximately model all conditionals
      p(x S jx 6S) where", "intents": []}], "isInfluential": false, "citedPaper":
      {"paperId": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea", "externalIds": {"MAG":
      "1993845689", "DOI": "10.1126/SCIENCE.7761831", "CorpusId": 871473, "PubMed":
      "7761831"}, "corpusId": 871473, "publicationVenue": {"id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
      "name": "Science", "type": "journal", "issn": "0193-4511", "alternate_issns":
      ["0036-8075"], "url": "https://www.jstor.org/journal/science", "alternate_urls":
      ["https://www.sciencemag.org/", "http://www.sciencemag.org/", "http://www.jstor.org/journals/00368075.html",
      "http://www.sciencemag.org/archive/"]}, "url": "https://www.semanticscholar.org/paper/6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
      "title": "The \"wake-sleep\" algorithm for unsupervised neural networks.", "abstract":
      "An unsupervised learning algorithm for a multilayer network of stochastic neurons
      is described. Bottom-up \"recognition\" connections convert the input into representations
      in successive hidden layers, and top-down \"generative\" connections reconstruct
      the representation in one layer from the representation in the layer above.
      In the \"wake\" phase, neurons are driven by recognition connections, and generative
      connections are adapted to increase the probability that they would reconstruct
      the correct activity vector in the layer below. In the \"sleep\" phase, neurons
      are driven by generative connections, and recognition connections are adapted
      to increase the probability that they would produce the correct activity vector
      in the layer above.", "venue": "Science", "year": 1995, "referenceCount": 22,
      "citationCount": 1075, "influentialCitationCount": 52, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "1995-05-26", "journal":
      {"volume": "268 5214", "pages": "\n          1158-61\n        ", "name": "Science"},
      "citationStyles": {"bibtex": "@Article{Hinton1995TheA,\n author = {Geoffrey
      E. Hinton and P. Dayan and B. Frey and R. Neal},\n booktitle = {Science},\n
      journal = {Science},\n pages = {\n          1158-61\n        },\n title = {The
      \"wake-sleep\" algorithm for unsupervised neural networks.},\n volume = {268
      5214},\n year = {1995}\n}\n"}, "authors": [{"authorId": "1695689", "name": "Geoffrey
      E. Hinton"}, {"authorId": "1790646", "name": "P. Dayan"}, {"authorId": "1749650",
      "name": "B. Frey"}, {"authorId": "145572884", "name": "R. Neal"}]}}, {"contexts":
      ["ins are necessary. 2 Related work An alternative to directed graphical models
      with latent variables are undirected graphical models with latent variables,
      such as restricted Boltzmann machines (RBMs) [27, 16], deep Boltzmann machines
      (DBMs) [26] and their numerous variants. The interactions within such models
      are represented as the product of unnormalized potential functions, normalized
      by a global summat"], "intents": [], "contextsWithIntent": [{"context": "ins
      are necessary. 2 Related work An alternative to directed graphical models with
      latent variables are undirected graphical models with latent variables, such
      as restricted Boltzmann machines (RBMs) [27, 16], deep Boltzmann machines (DBMs)
      [26] and their numerous variants. The interactions within such models are represented
      as the product of unnormalized potential functions, normalized by a global summat",
      "intents": []}], "isInfluential": false, "citedPaper": {"paperId": "4f7476037408ac3d993f5088544aab427bc319c1",
      "externalIds": {"MAG": "1820494964", "CorpusId": 533055}, "corpusId": 533055,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4f7476037408ac3d993f5088544aab427bc319c1",
      "title": "Information processing in dynamical systems: foundations of harmony
      theory", "abstract": "Abstract : At this early stage in the development of cognitive
      science, methodological issues are both open and central. There may have been
      times when developments in neuroscience, artificial intelligence, or cognitive
      psychology seduced researchers into believing that their discipline was on the
      verge of discovering the secret of intelligence. But a humbling history of hopes
      disappointed has produced the realization that understanding the mind will challenge
      the power of all these methodologies combined. The work reported in this chapter
      rests on the conviction that a methodology that has a crucial role to play in
      the development of cognitive science is mathematical analysis. The success of
      cognitive science, like that of many other sciences, will, I believe, depend
      upon the construction of a solid body of theoretical results: results that express
      in a mathematical language the conceptual insights of the field; results that
      squeeze all possible implications out of those insights by exploiting powerful
      mathematical techniques. This body of results, which I will call the theory
      of information processing, exists because information is a concept that lends
      itself to mathematical formalization. One part of the theory of information
      processing is already well-developed. The classical theory of computation provides
      powerful and elegant results about the notion of effective procedure, including
      languages for precisely expressing them and theoretical machines for realizing
      them.", "venue": "", "year": 1986, "referenceCount": 18, "citationCount": 2089,
      "influentialCitationCount": 189, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": "1986-01-03", "journal": {"volume": "", "pages": "194-281",
      "name": ""}, "citationStyles": {"bibtex": "@Inproceedings{Smolensky1986InformationPI,\n
      author = {P. Smolensky},\n pages = {194-281},\n title = {Information processing
      in dynamical systems: foundations of harmony theory},\n year = {1986}\n}\n"},
      "authors": [{"authorId": "1748557", "name": "P. Smolensky"}]}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": "69d76d8a89ab1ad8a988f320dc424ee6f9e67288", "externalIds": {"DBLP":
      "journals/corr/abs-2211-14666", "DOI": "10.48550/arXiv.2211.14666", "CorpusId":
      254044391}, "corpusId": 254044391, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/69d76d8a89ab1ad8a988f320dc424ee6f9e67288",
      "title": "Synergies Between Disentanglement and Sparsity: a Multi-Task Learning
      Perspective", "abstract": "Although disentangled representations are often said
      to be beneficial for downstream tasks, current empirical and theoretical understanding
      is limited. In this work, we provide evidence that disentangled representations
      coupled with sparse base-predictors improve generalization. In the context of
      multi-task learning, we prove a new identifiability result that provides conditions
      under which maximally sparse base-predictors yield disentangled representations.
      Motivated by this theoretical result, we propose a practical approach to learn
      disentangled representations based on a sparsity-promoting bi-level optimization
      problem. Finally, we explore a meta-learning version of this algorithm based
      on group Lasso multiclass SVM base-predictors, for which we derive a tractable
      dual formulation. It obtains competitive results on standard few-shot classification
      benchmarks, while each task is using only a fraction of the learned representations.",
      "venue": "arXiv.org", "year": 2022, "referenceCount": 92, "citationCount": 4,
      "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://arxiv.org/pdf/2211.14666", "status": null}, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": {"volume": "abs/2211.14666", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Lachapelle2022SynergiesBD,\n author = {S\u00e9bastien Lachapelle
      and T. Deleu and Divyat Mahajan and Ioannis Mitliagkas and Y. Bengio and Simon
      Lacoste-Julien and Quentin Bertrand},\n booktitle = {arXiv.org},\n journal =
      {ArXiv},\n title = {Synergies Between Disentanglement and Sparsity: a Multi-Task
      Learning Perspective},\n volume = {abs/2211.14666},\n year = {2022}\n}\n"},
      "authors": [{"authorId": "134730235", "name": "S\u00e9bastien Lachapelle"},
      {"authorId": "7636193", "name": "T. Deleu"}, {"authorId": "133841722", "name":
      "Divyat Mahajan"}, {"authorId": "2065139188", "name": "Ioannis Mitliagkas"},
      {"authorId": "1865800402", "name": "Y. Bengio"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "14205549", "name": "Quentin Bertrand"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "D Scieur", "abstract": null, "venue": "Affine Invariant
      Analysis of Frank-Wolfe on Strongly Convex Sets, ICML,", "year": 2021, "referenceCount":
      null, "citationCount": null, "influentialCitationCount": null, "isOpenAccess":
      null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": null,
      "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": null, "externalIds": null,
      "corpusId": null, "publicationVenue": null, "url": null, "title": "S Lacoste-Julien",
      "abstract": null, "venue": "Structured Convolutional Kernel Networks for Airline
      Crew Scheduling, ICML,", "year": 2021, "referenceCount": null, "citationCount":
      null, "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf":
      null, "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null,
      "publicationDate": null, "journal": null, "citationStyles": null, "authors":
      []}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
      "externalIds": {"CorpusId": 10319744}, "corpusId": 10319744, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
      "title": "GENERATIVE ADVERSARIAL NETS", "abstract": "Estimating individualized
      treatment effects (ITE) is a challenging task due to the need for an individual\u2019s
      potential outcomes to be learned from biased data and without having access
      to the counterfactuals. We propose a novel method for inferring ITE based on
      the Generative Adversarial Nets (GANs) framework. Our method, termed Generative
      Adversarial Nets for inference of Individualized Treatment Effects (GANITE),
      is motivated by the possibility that we can capture the uncertainty in the counterfactual
      distributions by attempting to learn them using a GAN. We generate proxies of
      the counterfactual outcomes using a counterfactual generator, G, and then pass
      these proxies to an ITE generator, I, in order to train it. By modeling both
      of these using the GAN framework, we are able to infer based on the factual
      data, while still accounting for the unseen counterfactuals. We test our method
      on three real-world datasets (with both binary and multiple treatments) and
      show that GANITE outperforms state-of-the-art methods.", "venue": "", "year":
      2018, "referenceCount": 24, "citationCount": 8796, "influentialCitationCount":
      1319, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": null, "citationStyles": {"bibtex":
      "@Inproceedings{Treat2018GENERATIVEAN,\n author = {Individualized Treat and
      Jinsung Yoon},\n title = {GENERATIVE ADVERSARIAL NETS},\n year = {2018}\n}\n"},
      "authors": [{"authorId": "2262142027", "name": "Individualized Treat"}, {"authorId":
      "2262204742", "name": "Jinsung Yoon"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "0edc142b51581a358055d7eddada8a4d0f9d021b",
      "externalIds": {"DBLP": "journals/corr/abs-1802-10551", "CorpusId": 260497042},
      "corpusId": 260497042, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/0edc142b51581a358055d7eddada8a4d0f9d021b",
      "title": "A Variational Inequality Perspective on Generative Adversarial Nets",
      "abstract": "Stability has been a recurrent issue in training generative adversarial
      networks (GANs). One common way to tackle this issue has been to propose new
      formulations of the GAN objective. Yet, surprisingly few studies have looked
      at optimization methods specifically designed for this adversarial training.
      In this work, we review the \u201cvariational inequality\u201d framework which
      contains most formulations of the GAN objective introduced so far. Taping into
      the mathematical programming literature, we counter some common misconceptions
      about the difficulties of saddle point optimization and propose to extend standard
      methods designed for variational inequalities to GANs training, such as a stochastic
      version of the extragradient method, and empirically investigate their behavior
      on GANs.", "venue": "arXiv.org", "year": 2018, "referenceCount": 38, "citationCount":
      18, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Review"], "publicationDate": null, "journal": {"volume":
      "abs/1802.10551", "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Gidel2018AVI,\n
      author = {Gauthier Gidel and Hugo Berard and Pascal Vincent and Simon Lacoste-Julien},\n
      booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Variational Inequality
      Perspective on Generative Adversarial Nets},\n volume = {abs/1802.10551},\n
      year = {2018}\n}\n"}, "authors": [{"authorId": "8150760", "name": "Gauthier
      Gidel"}, {"authorId": "40201329", "name": "Hugo Berard"}, {"authorId": "145467703",
      "name": "Pascal Vincent"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "ASAGA: AsynchronousParallel Saga", "abstract":
      null, "venue": "", "year": 2017, "referenceCount": null, "citationCount": null,
      "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf": null,
      "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null, "publicationDate":
      null, "journal": null, "citationStyles": null, "authors": []}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "State of the art model for speech: Dilated convolutions
      increase the receptive field: kernel only touches the signal at every 2 d entries",
      "abstract": null, "venue": "Recall: WaveNet", "year": 2016, "referenceCount":
      null, "citationCount": null, "influentialCitationCount": null, "isOpenAccess":
      null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": null,
      "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
      "externalIds": {"MAG": "2978753875", "CorpusId": 208098497}, "corpusId": 208098497,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
      "title": "A large-scale hierarchical image database", "abstract": null, "venue":
      "Computer Vision and Pattern Recognition", "year": 2009, "referenceCount": 0,
      "citationCount": 950, "influentialCitationCount": 68, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": null, "journal": {"volume": "",
      "name": ""}, "citationStyles": {"bibtex": "@Inproceedings{Deng2009ALH,\n author
      = {Jia Deng},\n booktitle = {Computer Vision and Pattern Recognition},\n title
      = {A large-scale hierarchical image database},\n year = {2009}\n}\n"}, "authors":
      [{"authorId": "153302678", "name": "Jia Deng"}]}}, {"contexts": ["monstrate
      the potential of the framework through qualitative and quantitative evaluation
      of the generated samples. 1 Introduction The promise of deep learning is to
      discover rich, hierarchical models [2] that represent probability distributions
      over the kinds of data encountered in arti\ufb01cial intelligence applications,
      such as natural images, audio waveforms containing speech, and symbols in natural
      l"], "intents": [], "contextsWithIntent": [{"context": "monstrate the potential
      of the framework through qualitative and quantitative evaluation of the generated
      samples. 1 Introduction The promise of deep learning is to discover rich, hierarchical
      models [2] that represent probability distributions over the kinds of data encountered
      in arti\ufb01cial intelligence applications, such as natural images, audio waveforms
      containing speech, and symbols in natural l", "intents": []}], "isInfluential":
      false, "citedPaper": {"paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
      "externalIds": {"MAG": "2072128103", "DBLP": "journals/ftml/Bengio09", "DOI":
      "10.1561/2200000006", "CorpusId": 207178999}, "corpusId": 207178999, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
      "title": "Learning Deep Architectures for AI", "abstract": "Theoretical results
      strongly suggest that in order to learn the kind of complicated functions that
      can represent high-level abstractions (e.g. in vision, language, and other AI-level
      tasks), one needs deep architectures. Deep architectures are composed of multiple
      levels of non-linear operations, such as in neural nets with many hidden layers
      or in complicated propositional formulae re-using many sub-formulae. Searching
      the parameter space of deep architectures is a difficult optimization task,
      but learning algorithms such as those for Deep Belief Networks have recently
      been proposed to tackle this problem with notable success, beating the state-of-the-art
      in certain areas. This paper discusses the motivations and principles regarding
      learning algorithms for deep architectures, in particular those exploiting as
      building blocks unsupervised learning of single-layer models such as Restricted
      Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
      "venue": "Found. Trends Mach. Learn.", "year": 2007, "referenceCount": 248,
      "citationCount": 8254, "influentialCitationCount": 516, "isOpenAccess": true,
      "openAccessPdf": {"url": "http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": {"volume": "2", "pages": "1-127", "name": "Found. Trends Mach.
      Learn."}, "citationStyles": {"bibtex": "@Article{Bengio2007LearningDA,\n author
      = {Yoshua Bengio},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found.
      Trends Mach. Learn.},\n pages = {1-127},\n title = {Learning Deep Architectures
      for AI},\n volume = {2},\n year = {2007}\n}\n"}, "authors": [{"authorId": "1751762",
      "name": "Yoshua Bengio"}]}}, {"contexts": [" perceptrons in practice suggests
      that they are a reasonable model to use despite their lack of theoretical guarantees.
      5 Experiments We trained adversarial nets an a range of datasets including MNIST[23],
      the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets
      used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid activations,
      while the discriminator net used maxout [10"], "intents": [], "contextsWithIntent":
      [{"context": " perceptrons in practice suggests that they are a reasonable model
      to use despite their lack of theoretical guarantees. 5 Experiments We trained
      adversarial nets an a range of datasets including MNIST[23], the Toronto Face
      Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of
      recti\ufb01er linear activations [19, 9] and sigmoid activations, while the
      discriminator net used maxout [10", "intents": []}], "isInfluential": false,
      "citedPaper": {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "externalIds":
      {"MAG": "2112796928", "DBLP": "journals/pieee/LeCunBBH98", "DOI": "10.1109/5.726791",
      "CorpusId": 14542261}, "corpusId": 14542261, "publicationVenue": {"id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
      "name": "Proceedings of the IEEE", "type": "journal", "alternate_names": ["Proc
      IEEE"], "issn": "0018-9219", "alternate_issns": ["1558-2256"], "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
      "alternate_urls": ["http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
      "https://ieeexplore.ieee.org/servlet/opac?punumber=5", "http://proceedingsoftheieee.ieee.org/"]},
      "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
      "title": "Gradient-based learning applied to document recognition", "abstract":
      "Multilayer neural networks trained with the back-propagation algorithm constitute
      the best example of a successful gradient based learning technique. Given an
      appropriate network architecture, gradient-based learning algorithms can be
      used to synthesize a complex decision surface that can classify high-dimensional
      patterns, such as handwritten characters, with minimal preprocessing. This paper
      reviews various methods applied to handwritten character recognition and compares
      them on a standard handwritten digit recognition task. Convolutional neural
      networks, which are specifically designed to deal with the variability of 2D
      shapes, are shown to outperform all other techniques. Real-life document recognition
      systems are composed of multiple modules including field extraction, segmentation
      recognition, and language modeling. A new learning paradigm, called graph transformer
      networks (GTN), allows such multimodule systems to be trained globally using
      gradient-based methods so as to minimize an overall performance measure. Two
      systems for online handwriting recognition are described. Experiments demonstrate
      the advantage of global training, and the flexibility of graph transformer networks.
      A graph transformer network for reading a bank cheque is also described. It
      uses convolutional neural network character recognizers combined with global
      training techniques to provide record accuracy on business and personal cheques.
      It is deployed commercially and reads several million cheques per day.", "venue":
      "Proceedings of the IEEE", "year": 1998, "referenceCount": 138, "citationCount":
      45191, "influentialCitationCount": 6728, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://hal.science/hal-03926082/document", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Review"], "publicationDate": null, "journal": {"volume":
      "86", "pages": "2278-2324", "name": "Proc. IEEE"}, "citationStyles": {"bibtex":
      "@Article{LeCun1998GradientbasedLA,\n author = {Yann LeCun and L. Bottou and
      Yoshua Bengio and P. Haffner},\n booktitle = {Proceedings of the IEEE},\n journal
      = {Proc. IEEE},\n pages = {2278-2324},\n title = {Gradient-based learning applied
      to document recognition},\n volume = {86},\n year = {1998}\n}\n"}, "authors":
      [{"authorId": "1688882", "name": "Yann LeCun"}, {"authorId": "52184096", "name":
      "L. Bottou"}, {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId":
      "1721248", "name": "P. Haffner"}]}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
      "externalIds": {"CorpusId": 2877073}, "corpusId": 2877073, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
      "title": "In Advances in Neural Information Processing Systems", "abstract":
      "The Bayesian analysis of neural networks is diicult because a simple prior
      over weights implies a complex prior distribution over functions. In this paper
      we investigate the use of Gaussian process priors over functions, which permit
      the predictive Bayesian analysis for xed values of hyperparameters to be carried
      out exactly using matrix operations. Two methods, using optimization and averaging
      (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of
      challenging problems and have produced excellent results.", "venue": "", "year":
      1996, "referenceCount": 10, "citationCount": 3606, "influentialCitationCount":
      298, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
      "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
      "journal": null, "citationStyles": {"bibtex": "@Inproceedings{Touretzky1996InAI,\n
      author = {D. Touretzky and M. C. Mozer and M. Hasselmo and RegressionChristopher
      and I. K. and WilliamsNeural and GroupAston and UniversityBirmingham},\n title
      = {In Advances in Neural Information Processing Systems},\n year = {1996}\n}\n"},
      "authors": [{"authorId": "1516909860", "name": "D. Touretzky"}, {"authorId":
      "2265537075", "name": "M. C. Mozer"}, {"authorId": "4493719", "name": "M. Hasselmo"},
      {"authorId": "2265534467", "name": "RegressionChristopher"}, {"authorId": "2265534300",
      "name": "I. K."}, {"authorId": "2265534429", "name": "WilliamsNeural"}, {"authorId":
      "2265538050", "name": "GroupAston"}, {"authorId": "2265534468", "name": "UniversityBirmingham"}]}},
      {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential": false,
      "citedPaper": {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "\u2022 Concurrently learning the generator and
      discriminator is challenging due to \u2022 Vanishing Gradients, \u2022 Non-convergence
      due to oscilliation \u2022 Mode collapse and mode dropping", "abstract": null,
      "venue": "", "year": null, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": null, "externalIds": null,
      "corpusId": null, "publicationVenue": null, "url": null, "title": "Introducing
      adversial discriminator networks allows GANs to learn by minimizing the Jensen-Shannon
      divergence", "abstract": null, "venue": "", "year": null, "referenceCount":
      null, "citationCount": null, "influentialCitationCount": null, "isOpenAccess":
      null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": null,
      "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"contexts": [], "intents": [], "contextsWithIntent":
      [], "isInfluential": false, "citedPaper": {"paperId": null, "externalIds": null,
      "corpusId": null, "publicationVenue": null, "url": null, "title": "References
      IV", "abstract": null, "venue": "", "year": null, "referenceCount": null, "citationCount":
      null, "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf":
      null, "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null,
      "publicationDate": null, "journal": null, "citationStyles": null, "authors":
      []}}, {"contexts": [], "intents": [], "contextsWithIntent": [], "isInfluential":
      false, "citedPaper": {"paperId": null, "externalIds": null, "corpusId": null,
      "publicationVenue": null, "url": null, "title": "GANs are a class of density-free
      generative models with (mostly) unrestricted generator functions", "abstract":
      null, "venue": "", "year": null, "referenceCount": null, "citationCount": null,
      "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf": null,
      "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null, "publicationDate":
      null, "journal": null, "citationStyles": null, "authors": []}}, {"contexts":
      [], "intents": [], "contextsWithIntent": [], "isInfluential": false, "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "On the Convergence of Continuous Constrained Optimization
      for Bayesian Network Structure Learning", "abstract": null, "venue": "", "year":
      null, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '162517'
      Content-Type:
      - application/json
      Date:
      - Wed, 27 Dec 2023 21:47:36 GMT
      Via:
      - 1.1 c8c6b28a99983d138ae2215ef1bb424a.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - w_Xc75YqYurTrDUV87kB2F0vavL__aA4jCzO_yypqF_jiaO9RhTCtg==
      X-Amz-Cf-Pop:
      - GRU3-P4
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - Qn062GbdvHcEC1A=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '162517'
      x-amzn-Remapped-Date:
      - Wed, 27 Dec 2023 21:47:36 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 4155f9e9-a57d-443f-820b-c7005316c65f
    http_version: HTTP/1.1
    status_code: 200
version: 1
