interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.25.2
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:49313245/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100
  response:
    content: '{"offset": 0, "data": [{"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "928f9dccb806a3278d20d82cc53781c5f44e2bb1",
      "externalIds": {"MAG": "2950268825", "ArXiv": "1805.01052", "DBLP": "conf/acl/KleinK18",
      "ACL": "P18-1249", "DOI": "10.18653/v1/P18-1249", "CorpusId": 19206893}, "corpusId":
      19206893, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
      of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
      Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
      "https://www.semanticscholar.org/paper/928f9dccb806a3278d20d82cc53781c5f44e2bb1",
      "title": "Constituency Parsing with a Self-Attentive Encoder", "abstract": "We
      demonstrate that replacing an LSTM encoder with a self-attentive architecture
      can lead to improvements to a state-of-the-art discriminative constituency parser.
      The use of attention makes explicit the manner in which information is propagated
      between different locations in the sentence, which we use to both analyze our
      model and propose potential improvements. For example, we find that separating
      positional and content information in the encoder can lead to improved parsing
      accuracy. Additionally, we evaluate different approaches for lexical representation.
      Our parser achieves new state-of-the-art results for single models trained on
      the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13
      F1 when using pre-trained word representations. Our parser also outperforms
      the previous best-published accuracy figures on 8 of the 9 languages in the
      SPMRL dataset.", "venue": "Annual Meeting of the Association for Computational
      Linguistics", "year": 2018, "referenceCount": 21, "citationCount": 448, "influentialCitationCount":
      62, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P18-1249.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-05-02", "journal": {"pages": "2676-2686"}, "citationStyles":
      {"bibtex": "@Article{Kitaev2018ConstituencyPW,\n author = {Nikita Kitaev and
      D. Klein},\n booktitle = {Annual Meeting of the Association for Computational
      Linguistics},\n pages = {2676-2686},\n title = {Constituency Parsing with a
      Self-Attentive Encoder},\n year = {2018}\n}\n"}, "authors": [{"authorId": "143808231",
      "name": "Nikita Kitaev"}, {"authorId": "38666915", "name": "D. Klein"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Natural language inference SNLI [5],
      MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE
      [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora
      Question Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment
      Treebank-2 [54], CoLA [65] but is\u2026", "intents": ["methodology"]}], "intents":
      ["methodology"], "contexts": ["Natural language inference SNLI [5], MultiNLI
      [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is\u2026"], "citedPaper": {"paperId": "cf8c493079702ec420ab4fc9c0fabb56b2a16c84",
      "externalIds": {"DBLP": "conf/aaai/KhotSC18", "MAG": "2788496822", "DOI": "10.1609/aaai.v32i1.12022",
      "CorpusId": 24462950}, "corpusId": 24462950, "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence", "type": "conference",
      "alternate_names": ["National Conference on Artificial Intelligence", "National
      Conf Artif Intell", "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"},
      "url": "https://www.semanticscholar.org/paper/cf8c493079702ec420ab4fc9c0fabb56b2a16c84",
      "title": "SciTaiL: A Textual Entailment Dataset from Science Question Answering",
      "abstract": "\n \n We present a new dataset and model for textual entailment,
      derived from treating multiple-choice question-answering as an entailment problem.
      SciTail is the first entailment set that is created solely from natural sentences
      that already exist independently ``in the wild'''' rather than sentences authored
      specifically for the entailment task. Different from existing entailment datasets,
      we create hypotheses from science questions and the corresponding answer candidates,\u00a0and
      premises from relevant web sentences retrieved from a large corpus. These sentences
      are often linguistically challenging. This, combined with the high lexical similarity
      of premise and hypothesis for both entailed and non-entailed pairs, makes this
      new entailment task particularly difficult.\u00a0The resulting challenge is
      evidenced by state-of-the-art textual entailment systems achieving mediocre
      performance on SciTail, especially in comparison to a simple majority class
      baseline. As a step forward, we demonstrate that one can improve accuracy on
      SciTail by 5% using a new neural model that exploits linguistic structure.\n
      \n", "venue": "AAAI Conference on Artificial Intelligence", "year": 2018, "referenceCount":
      36, "citationCount": 386, "influentialCitationCount": 85, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/12022/11881",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-04-27", "journal": {"pages": "5189-5197"}, "citationStyles":
      {"bibtex": "@Article{Khot2018SciTaiLAT,\n author = {Tushar Khot and Ashish Sabharwal
      and Peter Clark},\n booktitle = {AAAI Conference on Artificial Intelligence},\n
      pages = {5189-5197},\n title = {SciTaiL: A Textual Entailment Dataset from Science
      Question Answering},\n year = {2018}\n}\n"}, "authors": [{"authorId": "2236429",
      "name": "Tushar Khot"}, {"authorId": "48229640", "name": "Ashish Sabharwal"},
      {"authorId": "48323507", "name": "Peter Clark"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "Although there has been a lot of recent
      interest [58, 35, 44], the task remains challenging due to the presence of a
      wide variety of phenomena like lexical entailment, coreference, and lexical
      and syntactic ambiguity.", "intents": ["background"]}], "intents": ["background"],
      "contexts": ["Although there has been a lot of recent interest [58, 35, 44],
      the task remains challenging due to the presence of a wide variety of phenomena
      like lexical entailment, coreference, and lexical and syntactic ambiguity."],
      "citedPaper": {"paperId": "6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39", "externalIds":
      {"DBLP": "journals/corr/abs-1804-07888", "MAG": "2798459010", "ArXiv": "1804.07888",
      "CorpusId": 5058361}, "corpusId": 5058361, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39",
      "title": "Stochastic Answer Networks for Natural Language Inference", "abstract":
      "We propose a stochastic answer network (SAN) to explore multi-step inference
      strategies in Natural Language Inference. Rather than directly predicting the
      results given the inputs, the model maintains a state and iteratively refines
      its predictions. Our experiments show that SAN achieves the state-of-the-art
      results on three benchmarks: Stanford Natural Language Inference (SNLI) dataset,
      MultiGenre Natural Language Inference (MultiNLI) dataset and Quora Question
      Pairs dataset.", "venue": "arXiv.org", "year": 2018, "referenceCount": 29, "citationCount":
      43, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-04-21", "journal": {"volume": "abs/1804.07888",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Liu2018StochasticAN,\n
      author = {Xiaodong Liu and Kevin Duh and Jianfeng Gao},\n booktitle = {arXiv.org},\n
      journal = {ArXiv},\n title = {Stochastic Answer Networks for Natural Language
      Inference},\n volume = {abs/1804.07888},\n year = {2018}\n}\n"}, "authors":
      [{"authorId": "2108860856", "name": "Xiaodong Liu"}, {"authorId": "1800354",
      "name": "Kevin Duh"}, {"authorId": "1800422", "name": "Jianfeng Gao"}]}}, {"isInfluential":
      true, "contextsWithIntent": [{"context": "For instance, we achieve absolute
      improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7%
      on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66]
      and 5.5% on the recently introduced GLUE multi-task benchmark [64].", "intents":
      ["background"]}, {"context": "Some of these tasks are available as part of the
      recently released GLUE multi-task benchmark [64], which we make use of.", "intents":
      ["background"]}, {"context": "Natural language inference SNLI [5], MultiNLI
      [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is\u2026", "intents": ["methodology"]}], "intents": ["methodology",
      "background"], "contexts": ["For instance, we achieve absolute improvements
      of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question
      answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on
      the recently introduced GLUE multi-task benchmark [64].", "Some of these tasks
      are available as part of the recently released GLUE multi-task benchmark [64],
      which we make use of.", "Natural language inference SNLI [5], MultiNLI [66],
      Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story
      Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs
      [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54],
      CoLA [65] but is\u2026"], "citedPaper": {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "externalIds": {"MAG": "2963310665", "DBLP": "conf/emnlp/WangSMHLB18", "ACL":
      "W18-5446", "ArXiv": "1804.07461", "DOI": "10.18653/v1/W18-5446", "CorpusId":
      5034059}, "corpusId": 5034059, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
      Understanding", "abstract": "Human ability to understand language is general,
      flexible, and robust. In contrast, most NLU models above the word level are
      designed for a specific task and struggle with out-of-domain data. If we aspire
      to develop models with understanding beyond the detection of superficial correspondences
      between inputs and outputs, then it is critical to develop a unified model that
      can execute a range of linguistic tasks across different domains. To facilitate
      research in this direction, we present the General Language Understanding Evaluation
      (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary
      dataset for probing models for understanding of specific linguistic phenomena,
      and an online platform for evaluating and comparing models. For some benchmark
      tasks, training data is plentiful, but for others it is limited or does not
      match the genre of the test set. GLUE thus favors models that can represent
      linguistic knowledge in a way that facilitates sample-efficient learning and
      effective knowledge-transfer across tasks. While none of the datasets in GLUE
      were created from scratch for the benchmark, four of them feature privately-held
      test data, which is used to ensure that the benchmark is used fairly. We evaluate
      baselines that use ELMo (Peters et al., 2018), a powerful transfer learning
      technique, as well as state-of-the-art sentence representation models. The best
      models still achieve fairly low absolute scores. Analysis with our diagnostic
      dataset yields similarly weak performance over all phenomena tested, with some
      exceptions.", "venue": "BlackboxNLP@EMNLP", "year": 2018, "referenceCount":
      77, "citationCount": 4890, "influentialCitationCount": 1133, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W18-5446.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-04-20", "journal":
      {"pages": "353-355"}, "citationStyles": {"bibtex": "@Article{Wang2018GLUEAM,\n
      author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and
      Omer Levy and Samuel R. Bowman},\n booktitle = {BlackboxNLP@EMNLP},\n pages
      = {353-355},\n title = {GLUE: A Multi-Task Benchmark and Analysis Platform for
      Natural Language Understanding},\n year = {2018}\n}\n"}, "authors": [{"authorId":
      "144906624", "name": "Alex Wang"}, {"authorId": "50286460", "name": "Amanpreet
      Singh"}, {"authorId": "38614754", "name": "Julian Michael"}, {"authorId": "145783676",
      "name": "Felix Hill"}, {"authorId": "39455775", "name": "Omer Levy"}, {"authorId":
      "3644767", "name": "Samuel R. Bowman"}]}}, {"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "7b29f45df975ed1e4c3864b6ab4483f11086aa76",
      "externalIds": {"MAG": "2798081680", "DBLP": "conf/naacl/QiSFPN18", "ArXiv":
      "1804.06323", "ACL": "N18-2084", "DOI": "10.18653/v1/N18-2084", "CorpusId":
      4929974}, "corpusId": 4929974, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/7b29f45df975ed1e4c3864b6ab4483f11086aa76",
      "title": "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine
      Translation?", "abstract": "The performance of Neural Machine Translation (NMT)
      systems often suffers in low-resource scenarios where sufficiently large-scale
      parallel corpora cannot be obtained. Pre-trained word embeddings have proven
      to be invaluable for improving performance in natural language analysis tasks,
      which often suffer from paucity of data. However, their utility for NMT has
      not been extensively explored. In this work, we perform five sets of experiments
      that analyze when we can expect pre-trained word embeddings to help in NMT tasks.
      We show that such embeddings can be surprisingly effective in some cases \u2013
      providing gains of up to 20 BLEU points in the most favorable setting.", "venue":
      "North American Chapter of the Association for Computational Linguistics", "year":
      2018, "referenceCount": 30, "citationCount": 302, "influentialCitationCount":
      35, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/N18-2084.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-04-17", "journal": {"volume": "abs/1804.06323", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Qi2018WhenAW,\n author = {Ye
      Qi and Devendra Singh Sachan and Matthieu Felix and Sarguna Padmanabhan and
      Graham Neubig},\n booktitle = {North American Chapter of the Association for
      Computational Linguistics},\n journal = {ArXiv},\n title = {When and Why Are
      Pre-Trained Word Embeddings Useful for Neural Machine Translation?},\n volume
      = {abs/1804.06323},\n year = {2018}\n}\n"}, "authors": [{"authorId": "145270896",
      "name": "Ye Qi"}, {"authorId": "39670454", "name": "Devendra Singh Sachan"},
      {"authorId": "40895015", "name": "Matthieu Felix"}, {"authorId": "51177454",
      "name": "Sarguna Padmanabhan"}, {"authorId": "1700325", "name": "Graham Neubig"}]}},
      {"isInfluential": false, "contextsWithIntent": [], "intents": [], "contexts":
      [], "citedPaper": {"paperId": "d66ad3628c11c45bde5d4b65b9c1109a95d364d4", "externalIds":
      {"DBLP": "journals/corr/abs-1803-09074", "ArXiv": "1803.09074", "MAG": "2794554529",
      "CorpusId": 4417366}, "corpusId": 4417366, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/d66ad3628c11c45bde5d4b65b9c1109a95d364d4",
      "title": "Multi-range Reasoning for Machine Comprehension", "abstract": "We
      propose MRU (Multi-Range Reasoning Units), a new fast compositional encoder
      for machine comprehension (MC). Our proposed MRU encoders are characterized
      by multi-ranged gating, executing a series of parameterized contract-and-expand
      layers for learning gating vectors that benefit from long and short-term dependencies.
      The aims of our approach are as follows: (1) learning representations that are
      concurrently aware of long and short-term context, (2) modeling relationships
      between intra-document blocks and (3) fast and efficient sequence encoding.
      We show that our proposed encoder demonstrates promising results both as a standalone
      encoder and as well as a complementary building block. We conduct extensive
      experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA,
      achieving highly competitive performance on all. On the RACE benchmark, our
      model outperforms DFN (Dynamic Fusion Networks) by 1.5%-6% without using any
      recurrent or convolution layers. Similarly, we achieve competitive performance
      relative to AMANDA on the SearchQA benchmark and BiDAF on the NarrativeQA benchmark
      without using any LSTM/GRU layers. Finally, incorporating MRU encoders with
      standard BiLSTM architectures further improves performance, achieving state-of-the-art
      results.", "venue": "arXiv.org", "year": 2018, "referenceCount": 40, "citationCount":
      33, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-03-24", "journal": {"volume": "abs/1803.09074", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Tay2018MultirangeRF,\n author = {Yi Tay and Anh Tuan Luu
      and S. Hui},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Multi-range
      Reasoning for Machine Comprehension},\n volume = {abs/1803.09074},\n year =
      {2018}\n}\n"}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId":
      "26336902", "name": "Anh Tuan Luu"}, {"authorId": "144194328", "name": "S. Hui"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "Method Story Cloze
      RACE-m RACE-h RACE val-LS-skip [55] 76.", "intents": ["methodology"]}], "intents":
      ["methodology"], "contexts": ["Method Story Cloze RACE-m RACE-h RACE val-LS-skip
      [55] 76."], "citedPaper": {"paperId": "72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31",
      "externalIds": {"DBLP": "journals/corr/abs-1803-05547", "ArXiv": "1803.05547",
      "MAG": "2962714848", "ACL": "N18-2015", "DOI": "10.18653/v1/N18-2015", "CorpusId":
      3929706}, "corpusId": 3929706, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31",
      "title": "A Simple and Effective Approach to the Story Cloze Test", "abstract":
      "In the Story Cloze Test, a system is presented with a 4-sentence prompt to
      a story, and must determine which one of two potential endings is the \u2018right\u2019
      ending to the story. Previous work has shown that ignoring the training set
      and training a model on the validation set can achieve high accuracy on this
      task due to stylistic differences between the story endings in the training
      set and validation and test sets. Following this approach, we present a simpler
      fully-neural approach to the Story Cloze Test using skip-thought embeddings
      of the stories in a feed-forward network that achieves close to state-of-the-art
      performance on this task without any feature engineering. We also find that
      considering just the last sentence of the prompt instead of the whole prompt
      yields higher accuracy with our approach.", "venue": "North American Chapter
      of the Association for Computational Linguistics", "year": 2018, "referenceCount":
      11, "citationCount": 29, "influentialCitationCount": 3, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://www.aclweb.org/anthology/N18-2015.pdf", "status":
      null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-03-15", "journal": {"volume": "abs/1803.05547", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Srinivasan2018ASA,\n author
      = {Siddarth Srinivasan and Richa Arora and Mark O. Riedl},\n booktitle = {North
      American Chapter of the Association for Computational Linguistics},\n journal
      = {ArXiv},\n title = {A Simple and Effective Approach to the Story Cloze Test},\n
      volume = {abs/1803.05547},\n year = {2018}\n}\n"}, "authors": [{"authorId":
      "50106987", "name": "Siddarth Srinivasan"}, {"authorId": "2066379254", "name":
      "Richa Arora"}, {"authorId": "2757194", "name": "Mark O. Riedl"}]}}, {"isInfluential":
      true, "contextsWithIntent": [{"context": "Previous work proposed learning task
      speci\ufb01c architectures on top of transferred representations [44].", "intents":
      ["background"]}, {"context": "An alternative dataset, the 1B Word Benchmark,
      which is used by a similar approach, ELMo [44], is approximately the same size
      Table 1: A list of the different tasks and datasets used in our experiments.",
      "intents": ["methodology"]}, {"context": "Recent research has looked at various
      objectives such as language modeling [44], machine translation [38], and discourse
      coherence [22], with each method outperforming the others on different tasks.",
      "intents": ["methodology"]}, {"context": "Existing techniques involve a combination
      of making task-speci\ufb01c changes to the model architecture [43, 44], using
      intricate learning schemes [21] and adding auxiliary learning objectives [50].",
      "intents": ["methodology"]}, {"context": "Although there has been a lot of recent
      interest [58, 35, 44], the task remains challenging due to the presence of a
      wide variety of phenomena like lexical entailment, coreference, and lexical
      and syntactic ambiguity.", "intents": ["background"]}, {"context": "Other approaches
      [43, 44, 38] use hidden representations from a pre-trained language or machine
      translation model as auxiliary features while training a supervised model on
      the target task.", "intents": ["methodology"]}], "intents": ["methodology",
      "background"], "contexts": ["Previous work proposed learning task speci\ufb01c
      architectures on top of transferred representations [44].", "An alternative
      dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44],
      is approximately the same size Table 1: A list of the different tasks and datasets
      used in our experiments.", "Recent research has looked at various objectives
      such as language modeling [44], machine translation [38], and discourse coherence
      [22], with each method outperforming the others on different tasks.", "Existing
      techniques involve a combination of making task-speci\ufb01c changes to the
      model architecture [43, 44], using intricate learning schemes [21] and adding
      auxiliary learning objectives [50].", "Although there has been a lot of recent
      interest [58, 35, 44], the task remains challenging due to the presence of a
      wide variety of phenomena like lexical entailment, coreference, and lexical
      and syntactic ambiguity.", "Other approaches [43, 44, 38] use hidden representations
      from a pre-trained language or machine translation model as auxiliary features
      while training a supervised model on the target task."], "citedPaper": {"paperId":
      "3febb2bed8865945e7fddc99efd791887bb7e14f", "externalIds": {"DBLP": "conf/naacl/PetersNIGCLZ18",
      "ArXiv": "1802.05365", "MAG": "2949856395", "ACL": "N18-1202", "DOI": "10.18653/v1/N18-1202",
      "CorpusId": 3626819}, "corpusId": 3626819, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f",
      "title": "Deep Contextualized Word Representations", "abstract": "We introduce
      a new type of deep contextualized word representation that models both (1) complex
      characteristics of word use (e.g., syntax and semantics), and (2) how these
      uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors
      are learned functions of the internal states of a deep bidirectional language
      model (biLM), which is pre-trained on a large text corpus. We show that these
      representations can be easily added to existing models and significantly improve
      the state of the art across six challenging NLP problems, including question
      answering, textual entailment and sentiment analysis. We also present an analysis
      showing that exposing the deep internals of the pre-trained network is crucial,
      allowing downstream models to mix different types of semi-supervision signals.",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "year": 2018, "referenceCount": 64, "citationCount": 10267, "influentialCitationCount":
      1559, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/N18-1202.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-02-15",
      "journal": {"volume": "abs/1802.05365", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Peters2018DeepCW,\n author = {Matthew E. Peters and Mark
      Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee
      and Luke Zettlemoyer},\n booktitle = {North American Chapter of the Association
      for Computational Linguistics},\n journal = {ArXiv},\n title = {Deep Contextualized
      Word Representations},\n volume = {abs/1802.05365},\n year = {2018}\n}\n"},
      "authors": [{"authorId": "39139825", "name": "Matthew E. Peters"}, {"authorId":
      "50043859", "name": "Mark Neumann"}, {"authorId": "2136562", "name": "Mohit
      Iyyer"}, {"authorId": "40642935", "name": "Matt Gardner"}, {"authorId": "143997772",
      "name": "Christopher Clark"}, {"authorId": "2544107", "name": "Kenton Lee"},
      {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "Phrase-level or sentence-level embeddings,
      which can be trained using an unlabeled corpus, have been used to encode text
      into suitable vector representations for various target tasks [28, 32, 1, 36,
      22, 12, 56, 31].", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31]."],
      "citedPaper": {"paperId": "afc2850945a871e72c245818f9bc141bd659b453", "externalIds":
      {"MAG": "2786685006", "ArXiv": "1804.00079", "DBLP": "conf/iclr/SubramanianTBP18",
      "CorpusId": 4567927}, "corpusId": 4567927, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/afc2850945a871e72c245818f9bc141bd659b453",
      "title": "Learning General Purpose Distributed Sentence Representations via
      Large Scale Multi-task Learning", "abstract": "A lot of the recent success in
      natural language processing (NLP) has been driven by distributed vector representations
      of words trained on large amounts of text in an unsupervised manner. These representations
      are typically used as general purpose features for words across a range of NLP
      problems. However, extending this success to learning representations of sequences
      of words, such as sentences, remains an open problem. Recent work has explored
      unsupervised as well as supervised learning techniques with different training
      objectives to learn general purpose fixed-length sentence representations. In
      this work, we present a simple, effective multi-task learning framework for
      sentence representations that combines the inductive biases of diverse training
      objectives in a single model. We train this model on several data sources with
      multiple training objectives on over 100 million sentences. Extensive experiments
      demonstrate that sharing a single recurrent sentence encoder across weakly related
      tasks leads to consistent improvements over previous methods. We present substantial
      improvements in the context of transfer learning and low-resource settings using
      our learned general-purpose representations.", "venue": "International Conference
      on Learning Representations", "year": 2018, "referenceCount": 58, "citationCount":
      317, "influentialCitationCount": 38, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-02-15", "journal": {"volume": "abs/1804.00079", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Subramanian2018LearningGP,\n author = {Sandeep Subramanian
      and A. Trischler and Yoshua Bengio and C. Pal},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning
      General Purpose Distributed Sentence Representations via Large Scale Multi-task
      Learning},\n volume = {abs/1804.00079},\n year = {2018}\n}\n"}, "authors": [{"authorId":
      "50324141", "name": "Sandeep Subramanian"}, {"authorId": "3382568", "name":
      "A. Trischler"}, {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId":
      "1972076", "name": "C. Pal"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31].",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Phrase-level
      or sentence-level embeddings, which can be trained using an unlabeled corpus,
      have been used to encode text into suitable vector representations for various
      target tasks [28, 32, 1, 36, 22, 12, 56, 31]."], "citedPaper": {"paperId": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
      "externalIds": {"ArXiv": "1803.02893", "MAG": "2963644595", "DBLP": "journals/corr/abs-1803-02893",
      "CorpusId": 3525802}, "corpusId": 3525802, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
      "title": "An efficient framework for learning sentence representations", "abstract":
      "In this work we propose a simple and efficient framework for learning sentence
      representations from unlabelled data. Drawing inspiration from the distributional
      hypothesis and recent work on learning sentence representations, we reformulate
      the problem of predicting the context in which a sentence appears as a classification
      problem. Given a sentence and its context, a classifier distinguishes context
      sentences from other contrastive sentences based on their vector representations.
      This allows us to efficiently learn different types of encoding functions, and
      we show that the model learns high-quality sentence representations. We demonstrate
      that our sentence representations outperform state-of-the-art unsupervised and
      supervised representation learning methods on several downstream NLP tasks that
      involve understanding sentence semantics while achieving an order of magnitude
      speedup in training time.", "venue": "International Conference on Learning Representations",
      "year": 2018, "referenceCount": 50, "citationCount": 471, "influentialCitationCount":
      39, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-02-15", "journal": {"volume": "abs/1803.02893",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Logeswaran2018AnEF,\n
      author = {Lajanugen Logeswaran and Honglak Lee},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {An
      efficient framework for learning sentence representations},\n volume = {abs/1803.02893},\n
      year = {2018}\n}\n"}, "authors": [{"authorId": "2876316", "name": "Lajanugen
      Logeswaran"}, {"authorId": "1697141", "name": "Honglak Lee"}]}}, {"isInfluential":
      false, "contextsWithIntent": [], "intents": [], "contexts": [], "citedPaper":
      {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "externalIds": {"ArXiv":
      "1801.10198", "MAG": "2950355077", "DBLP": "conf/iclr/LiuSPGSKS18", "CorpusId":
      3608234}, "corpusId": 3608234, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/8691706ad0cf5e83969658b2e6bfffdc379440c9",
      "title": "Generating Wikipedia by Summarizing Long Sequences", "abstract": "We
      show that generating English Wikipedia articles can be approached as a multi-
      document summarization of source documents. We use extractive summarization
      to coarsely identify salient information and a neural abstractive model to generate
      the article. For the abstractive model, we introduce a decoder-only architecture
      that can scalably attend to very long sequences, much longer than typical encoder-
      decoder architectures used in sequence transduction. We show that this model
      can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia
      articles. When given reference documents, we show it can extract relevant factual
      information as reflected in perplexity, ROUGE scores and human evaluations.",
      "venue": "International Conference on Learning Representations", "year": 2018,
      "referenceCount": 22, "citationCount": 669, "influentialCitationCount": 95,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-01-30", "journal": {"volume": "abs/1801.10198",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Liu2018GeneratingWB,\n
      author = {Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and
      Ryan Sepassi and Lukasz Kaiser and Noam M. Shazeer},\n booktitle = {International
      Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generating
      Wikipedia by Summarizing Long Sequences},\n volume = {abs/1801.10198},\n year
      = {2018}\n}\n"}, "authors": [{"authorId": "35025299", "name": "Peter J. Liu"},
      {"authorId": "144413479", "name": "Mohammad Saleh"}, {"authorId": "38627717",
      "name": "Etienne Pot"}, {"authorId": "2065067542", "name": "Ben Goodrich"},
      {"authorId": "35474601", "name": "Ryan Sepassi"}, {"authorId": "40527594", "name":
      "Lukasz Kaiser"}, {"authorId": "1846258", "name": "Noam M. Shazeer"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Dai et al. [13] and Howard and Ruder
      [21] follow this method to improve text classi\ufb01cation.", "intents": []},
      {"context": "[13] and Howard and Ruder [21] follow this method to improve text
      classification.", "intents": ["methodology"]}, {"context": "Existing techniques
      involve a combination of making task-specific changes to the model architecture
      [43, 44], using intricate learning schemes [21] and adding auxiliary learning
      objectives [50].", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Dai et al. [13] and Howard and Ruder [21] follow this method to
      improve text classi\ufb01cation.", "[13] and Howard and Ruder [21] follow this
      method to improve text classification.", "Existing techniques involve a combination
      of making task-specific changes to the model architecture [43, 44], using intricate
      learning schemes [21] and adding auxiliary learning objectives [50]."], "citedPaper":
      {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "externalIds": {"MAG":
      "2798812533", "DBLP": "conf/acl/RuderH18", "ACL": "P18-1031", "DOI": "10.18653/v1/P18-1031",
      "CorpusId": 40100965}, "corpusId": 40100965, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
      of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
      Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
      "https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a",
      "title": "Universal Language Model Fine-tuning for Text Classification", "abstract":
      "Inductive transfer learning has greatly impacted computer vision, but existing
      approaches in NLP still require task-specific modifications and training from
      scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective
      transfer learning method that can be applied to any task in NLP, and introduce
      techniques that are key for fine-tuning a language model. Our method significantly
      outperforms the state-of-the-art on six text classification tasks, reducing
      the error by 18-24% on the majority of datasets. Furthermore, with only 100
      labeled examples, it matches the performance of training from scratch on 100
      times more data. We open-source our pretrained models and code.", "venue": "Annual
      Meeting of the Association for Computational Linguistics", "year": 2018, "referenceCount":
      57, "citationCount": 3112, "influentialCitationCount": 295, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P18-1031.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-01-18", "journal": {"pages": "328-339"}, "citationStyles":
      {"bibtex": "@Article{Howard2018UniversalLM,\n author = {Jeremy Howard and Sebastian
      Ruder},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n
      pages = {328-339},\n title = {Universal Language Model Fine-tuning for Text
      Classification},\n year = {2018}\n}\n"}, "authors": [{"authorId": "2093348519",
      "name": "Jeremy Howard"}, {"authorId": "2884561", "name": "Sebastian Ruder"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "Although there
      has been a lot of recent interest [58, 35, 44], the task remains challenging
      due to the presence of a wide variety of phenomena like lexical entailment,
      coreference, and lexical and syntactic ambiguity.", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["Although there has been a lot of recent
      interest [58, 35, 44], the task remains challenging due to the presence of a
      wide variety of phenomena like lexical entailment, coreference, and lexical
      and syntactic ambiguity."], "citedPaper": {"paperId": "f14dc3dba4a58fd380cce41e44552fd5f7812a4c",
      "externalIds": {"MAG": "2782363479", "DBLP": "journals/corr/abs-1801-00102",
      "CorpusId": 195346747}, "corpusId": 195346747, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/f14dc3dba4a58fd380cce41e44552fd5f7812a4c",
      "title": "A Compare-Propagate Architecture with Alignment Factorization for
      Natural Language Inference", "abstract": "This paper presents a new deep learning
      architecture for Natural Language Inference (NLI). Firstly, we introduce a new
      compare-propagate architecture where alignments pairs are compared and then
      propagated to upper layers for enhanced representation learning. Secondly, we
      adopt novel factorization layers for efficient compression of alignment vectors
      into scalar valued features, which are then be used to augment the base word
      representations. The design of our approach is aimed to be conceptually simple,
      compact and yet powerful. We conduct experiments on three popular benchmarks,
      SNLI, MultiNLI and SciTail, achieving state-of-the-art performance on all. A
      lightweight parameterization of our model enjoys a $\\approx 300\\%$ reduction
      in parameter size compared to the ESIM and DIIN, while maintaining competitive
      performance. Visual analysis shows that our propagated features are highly interpretable,
      opening new avenues to explainability in neural NLI models.", "venue": "arXiv.org",
      "year": 2017, "referenceCount": 28, "citationCount": 62, "influentialCitationCount":
      6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2017-12-30", "journal": {"volume": "abs/1801.00102",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Tay2017ACA,\n author
      = {Yi Tay and Anh Tuan Luu and S. Hui},\n booktitle = {arXiv.org},\n journal
      = {ArXiv},\n title = {A Compare-Propagate Architecture with Alignment Factorization
      for Natural Language Inference},\n volume = {abs/1801.00102},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "26336902",
      "name": "Anh Tuan Luu"}, {"authorId": "144194328", "name": "S. Hui"}]}}, {"isInfluential":
      false, "contextsWithIntent": [], "intents": [], "contexts": [], "citedPaper":
      {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "externalIds": {"DBLP":
      "journals/corr/abs-1711-05101", "MAG": "2768282280", "ArXiv": "1711.05101",
      "CorpusId": 3312944}, "corpusId": 3312944, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/45dfef0cc1ed96558c1c650432ce39d6a1050b6a",
      "title": "Fixing Weight Decay Regularization in Adam", "abstract": "We note
      that common implementations of adaptive gradient algorithms, such as Adam, limit
      the potential benefit of weight decay regularization, because the weights do
      not decay multiplicatively (as would be expected for standard weight decay)
      but by an additive constant factor. We propose a simple way to resolve this
      issue by decoupling weight decay and the optimization steps taken w.r.t. the
      loss function. We provide empirical evidence that our proposed modification
      (i) decouples the optimal choice of weight decay factor from the setting of
      the learning rate for both standard SGD and Adam, and (ii) substantially improves
      Adam''s generalization performance, allowing it to compete with SGD with momentum
      on image classification datasets (on which it was previously typically outperformed
      by the latter). We also demonstrate that longer optimization runs require smaller
      weight decay values for optimal results and introduce a normalized variant of
      weight decay to reduce this dependence. Finally, we propose a version of Adam
      with warm restarts (AdamWR) that has strong anytime performance while achieving
      state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will
      become available after the review process.", "venue": "arXiv.org", "year": 2017,
      "referenceCount": 23, "citationCount": 1396, "influentialCitationCount": 217,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate":
      "2017-11-14", "journal": {"volume": "abs/1711.05101", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Loshchilov2017FixingWD,\n author = {I. Loshchilov and F.
      Hutter},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fixing Weight
      Decay Regularization in Adam},\n volume = {abs/1711.05101},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "1678656", "name": "I. Loshchilov"}, {"authorId": "144661829",
      "name": "F. Hutter"}]}}, {"isInfluential": false, "contextsWithIntent": [],
      "intents": [], "contexts": [], "citedPaper": {"paperId": "4e013e6c800666c5bc611ca820ae437a7139cbb6",
      "externalIds": {"MAG": "2768331480", "ArXiv": "1711.04964", "DBLP": "journals/corr/abs-1711-04964",
      "CorpusId": 195346709}, "corpusId": 195346709, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/4e013e6c800666c5bc611ca820ae437a7139cbb6",
      "title": "Towards Human-level Machine Reading Comprehension: Reasoning and Inference
      with Multiple Strategies", "abstract": "This paper presents a new MRC model
      that is capable of three key comprehension skills: 1) handling rich variations
      in question types; 2) understanding potential answer choices; and 3) drawing
      inference through multiple sentences. The model is based on the proposed MUlti-Strategy
      Inference for Comprehension (MUSIC) architecture, which is able to dynamically
      apply different attention strategies to different types of questions on the
      fly. By incorporating a multi-step inference engine analogous to ReasoNet (Shen
      et al., 2017), MUSIC can also effectively perform multi-sentence inference in
      generating answers. Evaluation on the RACE dataset shows that the proposed method
      significantly outperforms previous state-of-the-art models by 7.5% in relative
      accuracy.", "venue": "arXiv.org", "year": 2017, "referenceCount": 20, "citationCount":
      27, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-11-14", "journal": {"volume": "abs/1711.04964", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Xu2017TowardsHM,\n author = {Yichong Xu and Jingjing Liu
      and Jianfeng Gao and Yelong Shen and Xiaodong Liu},\n booktitle = {arXiv.org},\n
      journal = {ArXiv},\n title = {Towards Human-level Machine Reading Comprehension:
      Reasoning and Inference with Multiple Strategies},\n volume = {abs/1711.04964},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "2636690", "name": "Yichong Xu"},
      {"authorId": "46700348", "name": "Jingjing Liu"}, {"authorId": "1800422", "name":
      "Jianfeng Gao"}, {"authorId": "1752875", "name": "Yelong Shen"}, {"authorId":
      "2108860856", "name": "Xiaodong Liu"}]}}, {"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
      "externalIds": {"MAG": "2949520888", "ArXiv": "1711.00043", "DBLP": "conf/iclr/LampleCDR18",
      "CorpusId": 3518190}, "corpusId": 3518190, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134",
      "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
      "abstract": "Machine translation has recently achieved impressive performance
      thanks to recent advances in deep learning and the availability of large-scale
      parallel corpora. There have been numerous attempts to extend these successes
      to low-resource language pairs, yet requiring tens of thousands of parallel
      sentences. In this work, we take this research direction to the extreme and
      investigate whether it is possible to learn to translate even without any parallel
      data. We propose a model that takes sentences from monolingual corpora in two
      different languages and maps them into the same latent space. By learning to
      reconstruct in both languages from this shared feature space, the model effectively
      learns to translate without using any labeled data. We demonstrate our model
      on two widely used datasets and two language pairs, reporting BLEU scores of
      32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using
      even a single parallel sentence at training time.", "venue": "International
      Conference on Learning Representations", "year": 2017, "referenceCount": 43,
      "citationCount": 994, "influentialCitationCount": 176, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-10-31", "journal":
      {"volume": "abs/1711.00043", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Lample2017UnsupervisedMT,\n author = {Guillaume Lample and Ludovic
      Denoyer and Marc''Aurelio Ranzato},\n booktitle = {International Conference
      on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Machine
      Translation Using Monolingual Corpora Only},\n volume = {abs/1711.00043},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "1830914", "name": "Guillaume
      Lample"}, {"authorId": "8905591", "name": "Ludovic Denoyer"}, {"authorId": "1706809",
      "name": "Marc''Aurelio Ranzato"}]}}, {"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "2cdc28b4f34410ff70099ae845daaaa25813f0e9",
      "externalIds": {"ACL": "D17-1168", "MAG": "2758815496", "DBLP": "conf/emnlp/ChaturvediPR17",
      "DOI": "10.18653/v1/D17-1168", "CorpusId": 27249900}, "corpusId": 27249900,
      "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference
      on Empirical Methods in Natural Language Processing", "type": "conference",
      "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural
      Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url":
      "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/2cdc28b4f34410ff70099ae845daaaa25813f0e9",
      "title": "Story Comprehension for Predicting What Happens Next", "abstract":
      "Automatic story comprehension is a fundamental challenge in Natural Language
      Understanding, and can enable computers to learn about social norms, human behavior
      and commonsense. In this paper, we present a story comprehension model that
      explores three distinct semantic aspects: (i) the sequence of events described
      in the story, (ii) its emotional trajectory, and (iii) its plot consistency.
      We judge the model\u2019s understanding of real-world stories by inquiring if,
      like humans, it can develop an expectation of what will happen next in a given
      story. Specifically, we use it to predict the correct ending of a given short
      story from possible alternatives. The model uses a hidden variable to weigh
      the semantic aspects in the context of the story. Our experiments demonstrate
      the potential of our approach to characterize these semantic aspects, and the
      strength of the hidden variable based approach. The model outperforms the state-of-the-art
      approaches and achieves best results on a publicly available dataset.", "venue":
      "Conference on Empirical Methods in Natural Language Processing", "year": 2017,
      "referenceCount": 75, "citationCount": 81, "influentialCitationCount": 9, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1168.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-09-01",
      "journal": {"pages": "1603-1614"}, "citationStyles": {"bibtex": "@Article{Chaturvedi2017StoryCF,\n
      author = {Snigdha Chaturvedi and Haoruo Peng and D. Roth},\n booktitle = {Conference
      on Empirical Methods in Natural Language Processing},\n pages = {1603-1614},\n
      title = {Story Comprehension for Predicting What Happens Next},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "37202877", "name": "Snigdha Chaturvedi"}, {"authorId":
      "1981962", "name": "Haoruo Peng"}, {"authorId": "144590225", "name": "D. Roth"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "0 - - ECNU (mixed
      ensemble) [60] - - - 81.", "intents": []}], "intents": [], "contexts": ["0 -
      - ECNU (mixed ensemble) [60] - - - 81."], "citedPaper": {"paperId": "caa8a41d58e386c56f56d46bbe79df9cb1087338",
      "externalIds": {"DBLP": "conf/semeval/TianZLW17", "ACL": "S17-2028", "MAG":
      "2752289368", "DOI": "10.18653/v1/S17-2028", "CorpusId": 31826507}, "corpusId":
      31826507, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/caa8a41d58e386c56f56d46bbe79df9cb1087338",
      "title": "ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP
      features and Neural Networks to Build a Universal Model for Multilingual and
      Cross-lingual Semantic Textual Similarity", "abstract": "To address semantic
      similarity on multilingual and cross-lingual sentences, we firstly translate
      other foreign languages into English, and then feed our monolingual English
      system with various interactive features. Our system is further supported by
      combining with deep learning semantic similarity and our best run achieves the
      mean Pearson correlation 73.16% in primary track.", "venue": "SemEval@ACL",
      "year": 2017, "referenceCount": 17, "citationCount": 67, "influentialCitationCount":
      10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/S17-2028.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-08-01", "journal":
      {"pages": "191-197"}, "citationStyles": {"bibtex": "@Article{Tian2017ECNUAS,\n
      author = {Junfeng Tian and Zhi-Min Zhou and Man Lan and Yuanbin Wu},\n booktitle
      = {SemEval@ACL},\n pages = {191-197},\n title = {ECNU at SemEval-2017 Task 1:
      Leverage Kernel-based Traditional NLP features and Neural Networks to Build
      a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "2338841", "name": "Junfeng Tian"},
      {"authorId": "1800668", "name": "Zhi-Min Zhou"}, {"authorId": "143745020", "name":
      "Man Lan"}, {"authorId": "3174675", "name": "Yuanbin Wu"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Other approaches [43, 44, 38] use
      hidden representations from a pre-trained language or machine translation model
      as auxiliary features while training a supervised model on the target task.",
      "intents": ["methodology"]}, {"context": "Recent research has looked at various
      objectives such as language modeling [44], machine translation [38], and discourse
      coherence [22], with each method outperforming the others on different tasks.",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Other
      approaches [43, 44, 38] use hidden representations from a pre-trained language
      or machine translation model as auxiliary features while training a supervised
      model on the target task.", "Recent research has looked at various objectives
      such as language modeling [44], machine translation [38], and discourse coherence
      [22], with each method outperforming the others on different tasks."], "citedPaper":
      {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "externalIds": {"ArXiv":
      "1708.00107", "DBLP": "conf/nips/McCannBXS17", "MAG": "2949553999", "CorpusId":
      9447219}, "corpusId": 9447219, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/bc8fa64625d9189f5801837e7b133e7fe3c581f7",
      "title": "Learned in Translation: Contextualized Word Vectors", "abstract":
      "Computer vision has benefited from initializing multiple deep layers with weights
      pretrained on large supervised training sets like ImageNet. Natural language
      processing (NLP) typically sees initialization of only the lowest layer of deep
      models with pretrained word vectors. In this paper, we use a deep LSTM encoder
      from an attentional sequence-to-sequence model trained for machine translation
      (MT) to contextualize word vectors. We show that adding these context vectors
      (CoVe) improves performance over using only unsupervised word and character
      vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb),
      question classification (TREC), entailment (SNLI), and question answering (SQuAD).
      For fine-grained sentiment analysis and entailment, CoVe improves performance
      of our baseline models to the state of the art.", "venue": "Neural Information
      Processing Systems", "year": 2017, "referenceCount": 73, "citationCount": 849,
      "influentialCitationCount": 78, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-08-01", "journal":
      {"pages": "6294-6305"}, "citationStyles": {"bibtex": "@Article{McCann2017LearnedIT,\n
      author = {Bryan McCann and James Bradbury and Caiming Xiong and R. Socher},\n
      booktitle = {Neural Information Processing Systems},\n pages = {6294-6305},\n
      title = {Learned in Translation: Contextualized Word Vectors},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "143775536", "name": "Bryan McCann"}, {"authorId":
      "40518045", "name": "James Bradbury"}, {"authorId": "2228109", "name": "Caiming
      Xiong"}, {"authorId": "2166511", "name": "R. Socher"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "\u2026[64], RTE [4], SciTail [25]
      Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase
      Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuf\ufb02ed at a sentence
      level - destroying long-range structure.", "intents": ["background"]}, {"context":
      "We use three datasets for this task \u2013 the Microsoft Paraphrase corpus
      (MRPC) [14] (collected from news sources), the Quora Question Pairs (QQP) dataset
      [9], and the Semantic Textual Similarity benchmark (STS-B) [6].", "intents":
      ["methodology"]}], "intents": ["methodology", "background"], "contexts": ["\u2026[64],
      RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is
      shuf\ufb02ed at a sentence level - destroying long-range structure.", "We use
      three datasets for this task \u2013 the Microsoft Paraphrase corpus (MRPC) [14]
      (collected from news sources), the Quora Question Pairs (QQP) dataset [9], and
      the Semantic Textual Similarity benchmark (STS-B) [6]."], "citedPaper": {"paperId":
      "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "externalIds": {"DBLP": "journals/corr/abs-1708-00055",
      "ACL": "S17-2001", "MAG": "3104033643", "ArXiv": "1708.00055", "DOI": "10.18653/v1/S17-2001",
      "CorpusId": 4421747}, "corpusId": 4421747, "publicationVenue": {"id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
      "name": "International Workshop on Semantic Evaluation", "type": "conference",
      "alternate_names": ["SemEval ", "Int Workshop Semantic Evaluation"]}, "url":
      "https://www.semanticscholar.org/paper/a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096",
      "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and
      Crosslingual Focused Evaluation", "abstract": "Semantic Textual Similarity (STS)
      measures the meaning similarity of sentences. Applications include machine translation
      (MT), summarization, generation, question answering (QA), short answer grading,
      semantic search, dialog and conversational systems. The STS shared task is a
      venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual
      and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE)
      data. The task obtained strong participation from 31 teams, with 17 participating
      in all language tracks. We summarize performance and review a selection of well
      performing methods. Analysis highlights common errors, providing insight into
      the limitations of existing models. To support ongoing work on semantic representations,
      the STS Benchmark is introduced as a new shared training and evaluation set
      carefully selected from the corpus of English STS shared task data (2012-2017).",
      "venue": "International Workshop on Semantic Evaluation", "year": 2017, "referenceCount":
      91, "citationCount": 1458, "influentialCitationCount": 273, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/S17-2001.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2017-07-31",
      "journal": {"pages": "1-14"}, "citationStyles": {"bibtex": "@Article{Cer2017SemEval2017T1,\n
      author = {Daniel Matthew Cer and Mona T. Diab and Eneko Agirre and I. Lopez-Gazpio
      and Lucia Specia},\n booktitle = {International Workshop on Semantic Evaluation},\n
      pages = {1-14},\n title = {SemEval-2017 Task 1: Semantic Textual Similarity
      Multilingual and Crosslingual Focused Evaluation},\n year = {2017}\n}\n"}, "authors":
      [{"authorId": "46724030", "name": "Daniel Matthew Cer"}, {"authorId": "1700007",
      "name": "Mona T. Diab"}, {"authorId": "1733049", "name": "Eneko Agirre"}, {"authorId":
      "1405529953", "name": "I. Lopez-Gazpio"}, {"authorId": "1702974", "name": "Lucia
      Specia"}]}}, {"isInfluential": true, "contextsWithIntent": [{"context": "Model
      speci\ufb01cations Our model largely follows the original transformer work [62].",
      "intents": ["methodology"]}, {"context": "For our model architecture, we use
      the Transformer [62], which has been shown to perform strongly on various tasks
      such as machine translation [62], document generation [34], and syntactic parsing
      [29].", "intents": ["methodology"]}, {"context": "In our experiments, we use
      a multi-layer Transformer decoder [34] for the language model, which is a variant
      of the transformer [62].", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Model speci\ufb01cations Our model largely follows the original
      transformer work [62].", "For our model architecture, we use the Transformer
      [62], which has been shown to perform strongly on various tasks such as machine
      translation [62], document generation [34], and syntactic parsing [29].", "In
      our experiments, we use a multi-layer Transformer decoder [34] for the language
      model, which is a variant of the transformer [62]."], "citedPaper": {"paperId":
      "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "externalIds": {"MAG": "2950858113",
      "DBLP": "conf/nips/VaswaniSPUJGKP17", "ArXiv": "1706.03762", "CorpusId": 13756489},
      "corpusId": 13756489, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "title": "Attention is All you Need", "abstract": "The dominant sequence transduction
      models are based on complex recurrent or convolutional neural networks in an
      encoder-decoder configuration. The best performing models also connect the encoder
      and decoder through an attention mechanism. We propose a new simple network
      architecture, the Transformer, based solely on attention mechanisms, dispensing
      with recurrence and convolutions entirely. Experiments on two machine translation
      tasks show these models to be superior in quality while being more parallelizable
      and requiring significantly less time to train. Our model achieves 28.4 BLEU
      on the WMT 2014 English-to-German translation task, improving over the existing
      best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
      translation task, our model establishes a new single-model state-of-the-art
      BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
      of the training costs of the best models from the literature. We show that the
      Transformer generalizes well to other tasks by applying it successfully to English
      constituency parsing both with large and limited training data.", "venue": "Neural
      Information Processing Systems", "year": 2017, "referenceCount": 42, "citationCount":
      78035, "influentialCitationCount": 14378, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-06-12", "journal": {"pages": "5998-6008"}, "citationStyles":
      {"bibtex": "@Article{Vaswani2017AttentionIA,\n author = {Ashish Vaswani and
      Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan
      N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n booktitle = {Neural Information
      Processing Systems},\n pages = {5998-6008},\n title = {Attention is All you
      Need},\n year = {2017}\n}\n"}, "authors": [{"authorId": "40348417", "name":
      "Ashish Vaswani"}, {"authorId": "1846258", "name": "Noam M. Shazeer"}, {"authorId":
      "3877127", "name": "Niki Parmar"}, {"authorId": "39328010", "name": "Jakob Uszkoreit"},
      {"authorId": "145024664", "name": "Llion Jones"}, {"authorId": "19177000", "name":
      "Aidan N. Gomez"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId":
      "3443442", "name": "Illia Polosukhin"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31].",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Phrase-level
      or sentence-level embeddings, which can be trained using an unlabeled corpus,
      have been used to encode text into suitable vector representations for various
      target tasks [28, 32, 1, 36, 22, 12, 56, 31]."], "citedPaper": {"paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
      "externalIds": {"ArXiv": "1705.02364", "DBLP": "conf/emnlp/ConneauKSBB17", "MAG":
      "2612953412", "ACL": "D17-1070", "DOI": "10.18653/v1/D17-1070", "CorpusId":
      28971531}, "corpusId": 28971531, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
      "title": "Supervised Learning of Universal Sentence Representations from Natural
      Language Inference Data", "abstract": "Many modern NLP systems rely on word
      embeddings, previously trained in an unsupervised manner on large corpora, as
      base features. Efforts to obtain embeddings for larger chunks of text, such
      as sentences, have however not been so successful. Several attempts at learning
      unsupervised representations of sentences have not reached satisfactory enough
      performance to be widely adopted. In this paper, we show how universal sentence
      representations trained using the supervised data of the Stanford Natural Language
      Inference datasets can consistently outperform unsupervised methods like SkipThought
      vectors on a wide range of transfer tasks. Much like how computer vision uses
      ImageNet to obtain features, which can then be transferred to other tasks, our
      work tends to indicate the suitability of natural language inference for transfer
      learning to other NLP tasks. Our encoder is publicly available.", "venue": "Conference
      on Empirical Methods in Natural Language Processing", "year": 2017, "referenceCount":
      45, "citationCount": 1886, "influentialCitationCount": 356, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1070.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-05-05", "journal": {"volume": "abs/1705.02364", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Conneau2017SupervisedLO,\n
      author = {Alexis Conneau and Douwe Kiela and Holger Schwenk and Lo\u00efc Barrault
      and Antoine Bordes},\n booktitle = {Conference on Empirical Methods in Natural
      Language Processing},\n journal = {ArXiv},\n title = {Supervised Learning of
      Universal Sentence Representations from Natural Language Inference Data},\n
      volume = {abs/1705.02364},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "2480903", "name": "Alexis Conneau"}, {"authorId": "1743722", "name": "Douwe
      Kiela"}, {"authorId": "144518416", "name": "Holger Schwenk"}, {"authorId": "2934336",
      "name": "Lo\u00efc Barrault"}, {"authorId": "1713934", "name": "Antoine Bordes"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "More recently,
      Rei [50] added an auxiliary language modeling objective to their target task
      objective and demonstrated performance gains on sequence labeling tasks.", "intents":
      ["background"]}, {"context": "Existing techniques involve a combination of making
      task-speci\ufb01c changes to the model architecture [43, 44], using intricate
      learning schemes [21] and adding auxiliary learning objectives [50].", "intents":
      ["methodology"]}, {"context": "This is in line with prior work [50, 43], who
      also observed improved performance with such an auxiliary objective.", "intents":
      ["result"]}], "intents": ["methodology", "result", "background"], "contexts":
      ["More recently, Rei [50] added an auxiliary language modeling objective to
      their target task objective and demonstrated performance gains on sequence labeling
      tasks.", "Existing techniques involve a combination of making task-speci\ufb01c
      changes to the model architecture [43, 44], using intricate learning schemes
      [21] and adding auxiliary learning objectives [50].", "This is in line with
      prior work [50, 43], who also observed improved performance with such an auxiliary
      objective."], "citedPaper": {"paperId": "ac17cfa150d802750b46220084d850cfdb64d1c1",
      "externalIds": {"ACL": "P17-1194", "MAG": "2609130030", "ArXiv": "1704.07156",
      "DBLP": "journals/corr/Rei17", "DOI": "10.18653/v1/P17-1194", "CorpusId": 16386838},
      "corpusId": 16386838, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
      of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
      Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
      "https://www.semanticscholar.org/paper/ac17cfa150d802750b46220084d850cfdb64d1c1",
      "title": "Semi-supervised Multitask Learning for Sequence Labeling", "abstract":
      "We propose a sequence labeling framework with a secondary training objective,
      learning to predict surrounding words for every word in the dataset. This language
      modeling objective incentivises the system to learn general-purpose patterns
      of semantic and syntactic composition, which are also useful for improving accuracy
      on different sequence labeling tasks. The architecture was evaluated on a range
      of datasets, covering the tasks of error detection in learner texts, named entity
      recognition, chunking and POS-tagging. The novel language modeling objective
      provided consistent performance improvements on every benchmark, without requiring
      any additional annotated or unannotated data.", "venue": "Annual Meeting of
      the Association for Computational Linguistics", "year": 2017, "referenceCount":
      37, "citationCount": 237, "influentialCitationCount": 17, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P17-1194.pdf", "status":
      null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-04-24", "journal": {"volume": "abs/1704.07156", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Rei2017SemisupervisedML,\n
      author = {Marek Rei},\n booktitle = {Annual Meeting of the Association for Computational
      Linguistics},\n journal = {ArXiv},\n title = {Semi-supervised Multitask Learning
      for Sequence Labeling},\n volume = {abs/1704.07156},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "145687301", "name": "Marek Rei"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Phrase-level or sentence-level embeddings,
      which can be trained using an unlabeled corpus, have been used to encode text
      into suitable vector representations for various target tasks [28, 32, 1, 36,
      22, 12, 56, 31].", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31]."],
      "citedPaper": {"paperId": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "externalIds":
      {"DBLP": "conf/iclr/AroraLM17", "MAG": "2752172973", "CorpusId": 64908139},
      "corpusId": 64908139, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/3f1802d3f4f5f6d66875dac09112f978f12e1e1e",
      "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings", "abstract":
      null, "venue": "International Conference on Learning Representations", "year":
      2017, "referenceCount": 0, "citationCount": 1188, "influentialCitationCount":
      188, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-04-24", "journal": {"volume": "", "name": ""}, "citationStyles": {"bibtex":
      "@Article{Arora2017ASB,\n author = {Sanjeev Arora and Yingyu Liang and Tengyu
      Ma},\n booktitle = {International Conference on Learning Representations},\n
      title = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},\n year
      = {2017}\n}\n"}, "authors": [{"authorId": "145563465", "name": "Sanjeev Arora"},
      {"authorId": "40609253", "name": "Yingyu Liang"}, {"authorId": "1901958", "name":
      "Tengyu Ma"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "Recent research has looked at various objectives such as language modeling
      [44], machine translation [38], and discourse coherence [22], with each method
      outperforming the others on different tasks.", "intents": ["methodology"]},
      {"context": "Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31].",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Recent
      research has looked at various objectives such as language modeling [44], machine
      translation [38], and discourse coherence [22], with each method outperforming
      the others on different tasks.", "Phrase-level or sentence-level embeddings,
      which can be trained using an unlabeled corpus, have been used to encode text
      into suitable vector representations for various target tasks [28, 32, 1, 36,
      22, 12, 56, 31]."], "citedPaper": {"paperId": "a97dc52807d80454e78d255f9fbd7b0fab56bd03",
      "externalIds": {"ArXiv": "1705.00557", "MAG": "2610858497", "DBLP": "journals/corr/JerniteBS17",
      "CorpusId": 6694822}, "corpusId": 6694822, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/a97dc52807d80454e78d255f9fbd7b0fab56bd03",
      "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation
      Learning", "abstract": "This work presents a novel objective function for the
      unsupervised training of neural network sentence encoders. It exploits signals
      from paragraph-level discourse coherence to train these models to understand
      text. Our objective is purely discriminative, allowing us to train models many
      times faster than was possible under prior methods, and it yields models which
      perform well in extrinsic evaluations.", "venue": "arXiv.org", "year": 2017,
      "referenceCount": 33, "citationCount": 107, "influentialCitationCount": 9, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-04-23", "journal":
      {"volume": "abs/1705.00557", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Jernite2017DiscourseBasedOF,\n author = {Yacine Jernite and Samuel
      R. Bowman and D. Sontag},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n
      title = {Discourse-Based Objectives for Fast Unsupervised Sentence Representation
      Learning},\n volume = {abs/1705.00557},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "2262249", "name": "Yacine Jernite"}, {"authorId": "3644767", "name": "Samuel
      R. Bowman"}, {"authorId": "1746662", "name": "D. Sontag"}]}}, {"isInfluential":
      true, "contextsWithIntent": [{"context": "For instance, we achieve absolute
      improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7%
      on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66]
      and 5.5% on the recently introduced GLUE multi-task benchmark [64].", "intents":
      ["background"]}, {"context": "Natural language inference SNLI [5], MultiNLI
      [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is\u2026", "intents": ["methodology"]}], "intents": ["methodology",
      "background"], "contexts": ["For instance, we achieve absolute improvements
      of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question
      answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on
      the recently introduced GLUE multi-task benchmark [64].", "Natural language
      inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]
      Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase
      Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026"], "citedPaper":
      {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "externalIds": {"DBLP":
      "journals/corr/WilliamsNB17", "MAG": "2963846996", "ArXiv": "1704.05426", "ACL":
      "N18-1101", "DOI": "10.18653/v1/N18-1101", "CorpusId": 3432876}, "corpusId":
      3432876, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
      "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through
      Inference", "abstract": "This paper introduces the Multi-Genre Natural Language
      Inference (MultiNLI) corpus, a dataset designed for use in the development and
      evaluation of machine learning models for sentence understanding. At 433k examples,
      this resource is one of the largest corpora available for natural language inference
      (a.k.a. recognizing textual entailment), improving upon available resources
      in both its coverage and difficulty. MultiNLI accomplishes this by offering
      data from ten distinct genres of written and spoken English, making it possible
      to evaluate systems on nearly the full complexity of the language, while supplying
      an explicit setting for evaluating cross-genre domain adaptation. In addition,
      an evaluation using existing machine learning models designed for the Stanford
      NLI corpus shows that it represents a substantially more difficult task than
      does that corpus, despite the two showing similar levels of inter-annotator
      agreement.", "venue": "North American Chapter of the Association for Computational
      Linguistics", "year": 2017, "referenceCount": 55, "citationCount": 3384, "influentialCitationCount":
      686, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/N18-1101.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-04-18",
      "journal": {"pages": "1112-1122"}, "citationStyles": {"bibtex": "@Article{Williams2017ABC,\n
      author = {Adina Williams and Nikita Nangia and Samuel R. Bowman},\n booktitle
      = {North American Chapter of the Association for Computational Linguistics},\n
      pages = {1112-1122},\n title = {A Broad-Coverage Challenge Corpus for Sentence
      Understanding through Inference},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "81840293", "name": "Adina Williams"}, {"authorId": "10666396", "name": "Nikita
      Nangia"}, {"authorId": "3644767", "name": "Samuel R. Bowman"}]}}, {"isInfluential":
      true, "contextsWithIntent": [{"context": "For instance, we achieve absolute
      improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7%
      on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66]
      and 5.5% on the recently introduced GLUE multi-task benchmark [64].", "intents":
      ["background"]}, {"context": "We use the recently released RACE dataset [30],
      consisting of English passages with associated questions from middle and high
      school exams.", "intents": ["methodology"]}, {"context": "Natural language inference
      SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering
      RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14],
      Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment
      Treebank-2 [54], CoLA [65] but is\u2026", "intents": ["methodology"]}], "intents":
      ["methodology", "background"], "contexts": ["For instance, we achieve absolute
      improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7%
      on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66]
      and 5.5% on the recently introduced GLUE multi-task benchmark [64].", "We use
      the recently released RACE dataset [30], consisting of English passages with
      associated questions from middle and high school exams.", "Natural language
      inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]
      Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase
      Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026"], "citedPaper":
      {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "externalIds": {"DBLP":
      "journals/corr/LaiXLYH17", "ArXiv": "1704.04683", "ACL": "D17-1082", "MAG":
      "2950501607", "DOI": "10.18653/v1/D17-1082", "CorpusId": 6826032}, "corpusId":
      6826032, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8",
      "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
      "abstract": "We present RACE, a new dataset for benchmark evaluation of methods
      in the reading comprehension task. Collected from the English exams for middle
      and high school Chinese students in the age range between 12 to 18, RACE consists
      of near 28,000 passages and near 100,000 questions generated by human experts
      (English instructors), and covers a variety of topics which are carefully designed
      for evaluating the students\u2019 ability in understanding and reasoning. In
      particular, the proportion of questions that requires reasoning is much larger
      in RACE than that in other benchmark datasets for reading comprehension, and
      there is a significant gap between the performance of the state-of-the-art models
      (43%) and the ceiling human performance (95%). We hope this new dataset can
      serve as a valuable resource for research and evaluation in machine comprehension.
      The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and
      the code is available at https://github.com/qizhex/RACE_AR_baselines.", "venue":
      "Conference on Empirical Methods in Natural Language Processing", "year": 2017,
      "referenceCount": 22, "citationCount": 997, "influentialCitationCount": 240,
      "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1082.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Education", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2017-04-15", "journal": {"volume": "abs/1704.04683", "name": "ArXiv"}, "citationStyles":
      {"bibtex": "@Article{Lai2017RACELR,\n author = {Guokun Lai and Qizhe Xie and
      Hanxiao Liu and Yiming Yang and E. Hovy},\n booktitle = {Conference on Empirical
      Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {RACE:
      Large-scale ReAding Comprehension Dataset From Examinations},\n volume = {abs/1704.04683},\n
      year = {2017}\n}\n"}, "authors": [{"authorId": "1857734", "name": "Guokun Lai"},
      {"authorId": "1912046", "name": "Qizhe Xie"}, {"authorId": "2391802", "name":
      "Hanxiao Liu"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId":
      "144547315", "name": "E. Hovy"}]}}, {"isInfluential": true, "contextsWithIntent":
      [{"context": "In addition, we evaluate on the Story Cloze Test [40], which involves
      selecting the correct ending to multi-sentence stories from two options.", "intents":
      ["methodology"]}, {"context": "For instance, we achieve absolute improvements
      of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question
      answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on
      the recently introduced GLUE multi-task benchmark [64].", "intents": ["background"]},
      {"context": "Natural language inference SNLI [5], MultiNLI [66], Question NLI
      [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026",
      "intents": ["methodology"]}], "intents": ["methodology", "background"], "contexts":
      ["In addition, we evaluate on the Story Cloze Test [40], which involves selecting
      the correct ending to multi-sentence stories from two options.", "For instance,
      we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze
      Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment
      (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark
      [64].", "Natural language inference SNLI [5], MultiNLI [66], Question NLI [64],
      RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026"],
      "citedPaper": {"paperId": "97394554eb5a74c3160c6bd743fcd3e4bd6cbe28", "externalIds":
      {"MAG": "2692059227", "ACL": "W17-0906", "DBLP": "conf/eacl/MostafazadehRLC17",
      "DOI": "10.18653/v1/W17-0906", "CorpusId": 13746570}, "corpusId": 13746570,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/97394554eb5a74c3160c6bd743fcd3e4bd6cbe28",
      "title": "LSDSem 2017 Shared Task: The Story Cloze Test", "abstract": "The LSDSem\u201917
      shared task is the Story Cloze Test, a new evaluation for story understanding
      and script learning. This test provides a system with a four-sentence story
      and two possible endings, and the system must choose the correct ending to the
      story. Successful narrative understanding (getting closer to human performance
      of 100%) requires systems to link various levels of semantics to commonsense
      knowledge. A total of eight systems participated in the shared task, with a
      variety of approaches including.", "venue": "LSDSem@EACL", "year": 2017, "referenceCount":
      17, "citationCount": 144, "influentialCitationCount": 21, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W17-0906.pdf", "status":
      null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-04-03", "journal":
      {"pages": "46-51"}, "citationStyles": {"bibtex": "@Article{Mostafazadeh2017LSDSem2S,\n
      author = {N. Mostafazadeh and Michael Roth and Annie Louis and Nathanael Chambers
      and James F. Allen},\n booktitle = {LSDSem@EACL},\n pages = {46-51},\n title
      = {LSDSem 2017 Shared Task: The Story Cloze Test},\n year = {2017}\n}\n"}, "authors":
      [{"authorId": "2400138", "name": "N. Mostafazadeh"}, {"authorId": "46617131",
      "name": "Michael Roth"}, {"authorId": "1767336", "name": "Annie Louis"}, {"authorId":
      "1729918", "name": "Nathanael Chambers"}, {"authorId": "145844737", "name":
      "James F. Allen"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "Other approaches [43, 44, 38] use hidden representations from a pre-trained
      language or machine translation model as auxiliary features while training a
      supervised model on the target task.", "intents": ["methodology"]}, {"context":
      "Existing techniques involve a combination of making task-speci\ufb01c changes
      to the model architecture [43, 44], using intricate learning schemes [21] and
      adding auxiliary learning objectives [50].", "intents": ["methodology"]}, {"context":
      "This is in line with prior work [50, 43], who also observed improved performance
      with such an auxiliary objective.", "intents": ["result"]}], "intents": ["methodology",
      "result"], "contexts": ["Other approaches [43, 44, 38] use hidden representations
      from a pre-trained language or machine translation model as auxiliary features
      while training a supervised model on the target task.", "Existing techniques
      involve a combination of making task-speci\ufb01c changes to the model architecture
      [43, 44], using intricate learning schemes [21] and adding auxiliary learning
      objectives [50].", "This is in line with prior work [50, 43], who also observed
      improved performance with such an auxiliary objective."], "citedPaper": {"paperId":
      "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "externalIds": {"MAG": "2963563735",
      "DBLP": "journals/corr/PetersABP17", "ACL": "P17-1161", "ArXiv": "1705.00108",
      "DOI": "10.18653/v1/P17-1161", "CorpusId": 7197241}, "corpusId": 7197241, "publicationVenue":
      {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the
      Association for Computational Linguistics", "type": "conference", "alternate_names":
      ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational
      Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"},
      "url": "https://www.semanticscholar.org/paper/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38",
      "title": "Semi-supervised sequence tagging with bidirectional language models",
      "abstract": "Pre-trained word embeddings learned from unlabeled text have become
      a standard component of neural network architectures for NLP tasks. However,
      in most cases, the recurrent network that operates on word-level representations
      to produce context sensitive representations is trained on relatively little
      labeled data. In this paper, we demonstrate a general semi-supervised approach
      for adding pretrained context embeddings from bidirectional language models
      to NLP systems and apply it to sequence labeling tasks. We evaluate our model
      on two standard datasets for named entity recognition (NER) and chunking, and
      in both cases achieve state of the art results, surpassing previous systems
      that use other forms of transfer or joint learning with additional labeled data
      and task specific gazetteers.", "venue": "Annual Meeting of the Association
      for Computational Linguistics", "year": 2017, "referenceCount": 46, "citationCount":
      584, "influentialCitationCount": 56, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://www.aclweb.org/anthology/P17-1161.pdf", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-04-01", "journal":
      {"volume": "abs/1705.00108", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Peters2017SemisupervisedST,\n author = {Matthew E. Peters and Waleed
      Ammar and Chandra Bhagavatula and Russell Power},\n booktitle = {Annual Meeting
      of the Association for Computational Linguistics},\n journal = {ArXiv},\n title
      = {Semi-supervised sequence tagging with bidirectional language models},\n volume
      = {abs/1705.00108},\n year = {2017}\n}\n"}, "authors": [{"authorId": "39139825",
      "name": "Matthew E. Peters"}, {"authorId": "145585097", "name": "Waleed Ammar"},
      {"authorId": "1857797", "name": "Chandra Bhagavatula"}, {"authorId": "39071178",
      "name": "Russell Power"}]}}, {"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "62f3d3015cee122bd147d7d878c85f70cc15680d",
      "externalIds": {"MAG": "2949532563", "DBLP": "conf/cvpr/ZhangIE17", "ArXiv":
      "1611.09842", "DOI": "10.1109/CVPR.2017.76", "CorpusId": 9658690}, "corpusId":
      9658690, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/62f3d3015cee122bd147d7d878c85f70cc15680d",
      "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction",
      "abstract": "We propose split-brain autoencoders, a straightforward modification
      of the traditional autoencoder architecture, for unsupervised representation
      learning. The method adds a split to the network, resulting in two disjoint
      sub-networks. Each sub-network is trained to perform a difficult task &#x2013;
      predicting one subset of the data channels from another. Together, the sub-networks
      extract features from the entire input signal. By forcing the network to solve
      cross-channel prediction tasks, we induce a representation within the network
      which transfers well to other, unseen tasks. This method achieves state-of-the-art
      performance on several large-scale transfer learning benchmarks.", "venue":
      "Computer Vision and Pattern Recognition", "year": 2016, "referenceCount": 57,
      "citationCount": 623, "influentialCitationCount": 36, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://arxiv.org/pdf/1611.09842", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-11-29", "journal": {"pages": "645-654", "name": "2017 IEEE Conference
      on Computer Vision and Pattern Recognition (CVPR)"}, "citationStyles": {"bibtex":
      "@Article{Zhang2016SplitBrainAU,\n author = {Richard Zhang and Phillip Isola
      and Alexei A. Efros},\n booktitle = {Computer Vision and Pattern Recognition},\n
      journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n
      pages = {645-654},\n title = {Split-Brain Autoencoders: Unsupervised Learning
      by Cross-Channel Prediction},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "2844849", "name": "Richard Zhang"}, {"authorId": "2094770", "name": "Phillip
      Isola"}, {"authorId": "1763086", "name": "Alexei A. Efros"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "In recent work, the method has been
      used to help train deep neural networks on various tasks like image classi\ufb01cation
      [69], speech recognition [68], entity disambiguation [17] and machine translation
      [48].", "intents": ["methodology"]}], "intents": ["methodology"], "contexts":
      ["In recent work, the method has been used to help train deep neural networks
      on various tasks like image classi\ufb01cation [69], speech recognition [68],
      entity disambiguation [17] and machine translation [48]."], "citedPaper": {"paperId":
      "85f94d8098322f8130512b4c6c4627548ce4a6cc", "externalIds": {"DBLP": "journals/corr/RamachandranLL16",
      "ArXiv": "1611.02683", "ACL": "D17-1039", "MAG": "2951991713", "DOI": "10.18653/v1/D17-1039",
      "CorpusId": 3488076}, "corpusId": 3488076, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/85f94d8098322f8130512b4c6c4627548ce4a6cc",
      "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract":
      "This work presents a general unsupervised learning method to improve the accuracy
      of sequence to sequence (seq2seq) models. In our method, the weights of the
      encoder and decoder of a seq2seq model are initialized with the pretrained weights
      of two language models and then fine-tuned with labeled data. We apply this
      method to challenging benchmarks in machine translation and abstractive summarization
      and find that it significantly improves the subsequent supervised models. Our
      main result is that pretraining improves the generalization of seq2seq models.
      We achieve state-of-the-art results on the WMT English\u2192German task, surpassing
      a range of methods using both phrase-based machine translation and neural machine
      translation. Our method achieves a significant improvement of 1.3 BLEU from
      th previous best models on both WMT\u201914 and WMT\u201915 English\u2192German.
      We also conduct human evaluations on abstractive summarization and find that
      our method outperforms a purely supervised learning baseline in a statistically
      significant manner.", "venue": "Conference on Empirical Methods in Natural Language
      Processing", "year": 2016, "referenceCount": 63, "citationCount": 272, "influentialCitationCount":
      11, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1039.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-11-08", "journal": {"volume": "abs/1611.02683", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Ramachandran2016UnsupervisedPF,\n
      author = {Prajit Ramachandran and Peter J. Liu and Quoc V. Le},\n booktitle
      = {Conference on Empirical Methods in Natural Language Processing},\n journal
      = {ArXiv},\n title = {Unsupervised Pretraining for Sequence to Sequence Learning},\n
      volume = {abs/1611.02683},\n year = {2016}\n}\n"}, "authors": [{"authorId":
      "3377142", "name": "Prajit Ramachandran"}, {"authorId": "35025299", "name":
      "Peter J. Liu"}, {"authorId": "2827616", "name": "Quoc V. Le"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Since layernorm [2] is used extensively
      throughout the model, a simple weight initialization of N (0 , 0 .", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["Since layernorm
      [2] is used extensively throughout the model, a simple weight initialization
      of N (0 , 0 ."], "citedPaper": {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
      "externalIds": {"MAG": "3037932933", "ArXiv": "1607.06450", "DBLP": "journals/corr/BaKH16",
      "CorpusId": 8236317}, "corpusId": 8236317, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/97fb4e3d45bb098e27e0071448b6152217bd35a5",
      "title": "Layer Normalization", "abstract": "Training state-of-the-art, deep
      neural networks is computationally expensive. One way to reduce the training
      time is to normalize the activities of the neurons. A recently introduced technique
      called batch normalization uses the distribution of the summed input to a neuron
      over a mini-batch of training cases to compute a mean and variance which are
      then used to normalize the summed input to that neuron on each training case.
      This significantly reduces the training time in feed-forward neural networks.
      However, the effect of batch normalization is dependent on the mini-batch size
      and it is not obvious how to apply it to recurrent neural networks. In this
      paper, we transpose batch normalization into layer normalization by computing
      the mean and variance used for normalization from all of the summed inputs to
      the neurons in a layer on a single training case. Like batch normalization,
      we also give each neuron its own adaptive bias and gain which are applied after
      the normalization but before the non-linearity. Unlike batch normalization,
      layer normalization performs exactly the same computation at training and test
      times. It is also straightforward to apply to recurrent neural networks by computing
      the normalization statistics separately at each time step. Layer normalization
      is very effective at stabilizing the hidden state dynamics in recurrent networks.
      Empirically, we show that layer normalization can substantially reduce the training
      time compared with previously published techniques.", "venue": "arXiv.org",
      "year": 2016, "referenceCount": 33, "citationCount": 7134, "influentialCitationCount":
      337, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-07-21", "journal": {"volume": "abs/1607.06450", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Ba2016LayerN,\n author = {Jimmy
      Ba and J. Kiros and Geoffrey E. Hinton},\n booktitle = {arXiv.org},\n journal
      = {ArXiv},\n title = {Layer Normalization},\n volume = {abs/1607.06450},\n year
      = {2016}\n}\n"}, "authors": [{"authorId": "2503659", "name": "Jimmy Ba"}, {"authorId":
      "51131802", "name": "J. Kiros"}, {"authorId": "1695689", "name": "Geoffrey E.
      Hinton"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context": "For
      the activation function, we used the Gaussian Error Linear Unit (GELU) [18].",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["For
      the activation function, we used the Gaussian Error Linear Unit (GELU) [18]."],
      "citedPaper": {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "externalIds":
      {"MAG": "2462831000", "DBLP": "journals/corr/HendrycksG16", "CorpusId": 2359786},
      "corpusId": 2359786, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian
      Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit
      (GELU), a high-performing neural network activation function. The GELU nonlinearity
      is the expected transformation of a stochastic regularizer which randomly applies
      the identity or zero map, combining the intuitions of dropout and zoneout while
      respecting neuron values. This connection suggests a new probabilistic understanding
      of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity
      against the ReLU and ELU activations and find performance improvements across
      all tasks.", "venue": "arXiv.org", "year": 2016, "referenceCount": 24, "citationCount":
      517, "influentialCitationCount": 71, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2016-06-27", "journal":
      {"volume": "abs/1606.08415", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Hendrycks2016BridgingNA,\n author = {Dan Hendrycks and Kevin Gimpel},\n
      booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Bridging Nonlinearities
      and Stochastic Regularizers with Gaussian Error Linear Units},\n volume = {abs/1606.08415},\n
      year = {2016}\n}\n"}, "authors": [{"authorId": "3422872", "name": "Dan Hendrycks"},
      {"authorId": "1700980", "name": "Kevin Gimpel"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "This corpus has been shown to contain more
      reasoning type questions that other datasets like CNN [19] or SQuaD [47], providing
      the perfect evaluation for our model which is trained to handle long-range contexts.",
      "intents": ["background"]}], "intents": ["background"], "contexts": ["This corpus
      has been shown to contain more reasoning type questions that other datasets
      like CNN [19] or SQuaD [47], providing the perfect evaluation for our model
      which is trained to handle long-range contexts."], "citedPaper": {"paperId":
      "05dd7254b632376973f3a1b4d39485da17814df5", "externalIds": {"DBLP": "journals/corr/RajpurkarZLL16",
      "MAG": "2963748441", "ACL": "D16-1264", "ArXiv": "1606.05250", "DOI": "10.18653/v1/D16-1264",
      "CorpusId": 11816014}, "corpusId": 11816014, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5",
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "abstract":
      "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension
      dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia
      articles, where the answer to each question is a segment of text from the corresponding
      reading passage. We analyze the dataset to understand the types of reasoning
      required to answer the questions, leaning heavily on dependency and constituency
      trees. We build a strong logistic regression model, which achieves an F1 score
      of 51.0%, a significant improvement over a simple baseline (20%). However, human
      performance (86.8%) is much higher, indicating that the dataset presents a good
      challenge problem for future research. \nThe dataset is freely available at
      this https URL", "venue": "Conference on Empirical Methods in Natural Language
      Processing", "year": 2016, "referenceCount": 29, "citationCount": 6306, "influentialCitationCount":
      1479, "isOpenAccess": true, "openAccessPdf": {"url": "https://doi.org/10.18653/v1/d16-1264",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-06-16", "journal": {"pages": "2383-2392"}, "citationStyles":
      {"bibtex": "@Article{Rajpurkar2016SQuAD1Q,\n author = {Pranav Rajpurkar and
      Jian Zhang and Konstantin Lopyrev and Percy Liang},\n booktitle = {Conference
      on Empirical Methods in Natural Language Processing},\n pages = {2383-2392},\n
      title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},\n year
      = {2016}\n}\n"}, "authors": [{"authorId": "2706258", "name": "Pranav Rajpurkar"},
      {"authorId": "2151810148", "name": "Jian Zhang"}, {"authorId": "2787620", "name":
      "Konstantin Lopyrev"}, {"authorId": "145419642", "name": "Percy Liang"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "Dai et al. [13]
      and Howard and Ruder [21] follow this method to improve text classi\ufb01cation.",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Dai
      et al. [13] and Howard and Ruder [21] follow this method to improve text classi\ufb01cation."],
      "citedPaper": {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "externalIds":
      {"ArXiv": "1511.01432", "MAG": "2952729433", "DBLP": "conf/nips/DaiL15", "CorpusId":
      7138078}, "corpusId": 7138078, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
      "title": "Semi-supervised Sequence Learning", "abstract": "We present two approaches
      to use unlabeled data to improve Sequence Learning with recurrent networks.
      The first approach is to predict what comes next in a sequence, which is a language
      model in NLP. The second approach is to use a sequence autoencoder, which reads
      the input sequence into a vector and predicts the input sequence again. These
      two algorithms can be used as a \"pretraining\" algorithm for a later supervised
      sequence learning algorithm. In other words, the parameters obtained from the
      pretraining step can then be used as a starting point for other supervised training
      models. In our experiments, we find that long short term memory recurrent networks
      after pretrained with the two approaches become more stable to train and generalize
      better. With pretraining, we were able to achieve strong performance in many
      classification tasks, such as text classification with IMDB, DBpedia or image
      recognition in CIFAR-10.", "venue": "Neural Information Processing Systems",
      "year": 2015, "referenceCount": 41, "citationCount": 1101, "influentialCitationCount":
      72, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-11-04", "journal":
      {"volume": "abs/1511.01432", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Dai2015SemisupervisedSL,\n author = {Andrew M. Dai and Quoc V. Le},\n
      booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n
      title = {Semi-supervised Sequence Learning},\n volume = {abs/1511.01432},\n
      year = {2015}\n}\n"}, "authors": [{"authorId": "2555924", "name": "Andrew M.
      Dai"}, {"authorId": "2827616", "name": "Quoc V. Le"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "Instead, we use a traversal-style approach
      [52], where we convert structured inputs into an ordered sequence that our pre-trained
      model can process.", "intents": ["methodology"]}, {"context": "During transfer,
      we utilize task-speci\ufb01c input adaptations derived from traversal-style
      approaches [52], which process structured text input as a single contiguous
      sequence of tokens.", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Instead, we use a traversal-style approach [52], where we convert
      structured inputs into an ordered sequence that our pre-trained model can process.",
      "During transfer, we utilize task-speci\ufb01c input adaptations derived from
      traversal-style approaches [52], which process structured text input as a single
      contiguous sequence of tokens."], "citedPaper": {"paperId": "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45",
      "externalIds": {"MAG": "2950162130", "DBLP": "journals/corr/RocktaschelGHKB15",
      "ArXiv": "1509.06664", "CorpusId": 2135897}, "corpusId": 2135897, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45",
      "title": "Reasoning about Entailment with Neural Attention", "abstract": "While
      most approaches to automatically recognizing entailment relations have used
      classifiers employing hand engineered features derived from complex natural
      language processing pipelines, in practice their performance has been only slightly
      better than bag-of-word pair classifiers using only lexical similarity. The
      only attempt so far to build an end-to-end differentiable neural network for
      entailment failed to outperform such a simple similarity classifier. In this
      paper, we propose a neural model that reads two sentences to determine entailment
      using long short-term memory units. We extend this model with a word-by-word
      neural attention mechanism that encourages reasoning over entailments of pairs
      of words and phrases. Furthermore, we present a qualitative analysis of attention
      weights produced by this model, demonstrating such reasoning capabilities. On
      a large entailment dataset this model outperforms the previous best neural model
      and a classifier with engineered features by a substantial margin. It is the
      first generic end-to-end differentiable system that achieves state-of-the-art
      accuracy on a textual entailment dataset.", "venue": "International Conference
      on Learning Representations", "year": 2015, "referenceCount": 34, "citationCount":
      729, "influentialCitationCount": 85, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2015-09-22", "journal": {"volume": "abs/1509.06664", "name": "CoRR"}, "citationStyles":
      {"bibtex": "@Article{Rockt\u00e4schel2015ReasoningAE,\n author = {Tim Rockt\u00e4schel
      and Edward Grefenstette and K. Hermann and Tom\u00e1s Kocisk\u00fd and Phil
      Blunsom},\n booktitle = {International Conference on Learning Representations},\n
      journal = {CoRR},\n title = {Reasoning about Entailment with Neural Attention},\n
      volume = {abs/1509.06664},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "2620211", "name": "Tim Rockt\u00e4schel"}, {"authorId": "1864353", "name":
      "Edward Grefenstette"}, {"authorId": "2910877", "name": "K. Hermann"}, {"authorId":
      "2367821", "name": "Tom\u00e1s Kocisk\u00fd"}, {"authorId": "1685771", "name":
      "Phil Blunsom"}]}}, {"isInfluential": false, "contextsWithIntent": [], "intents":
      [], "contexts": [], "citedPaper": {"paperId": "1518039b5001f1836565215eb047526b3ac7f462",
      "externalIds": {"DBLP": "conf/acl/SennrichHB16a", "ACL": "P16-1162", "MAG":
      "1816313093", "ArXiv": "1508.07909", "DOI": "10.18653/v1/P16-1162", "CorpusId":
      1114678}, "corpusId": 1114678, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
      of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
      Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
      "https://www.semanticscholar.org/paper/1518039b5001f1836565215eb047526b3ac7f462",
      "title": "Neural Machine Translation of Rare Words with Subword Units", "abstract":
      "Neural machine translation (NMT) models typically operate with a fixed vocabulary,
      but translation is an open-vocabulary problem. Previous work addresses the translation
      of out-of-vocabulary words by backing off to a dictionary. In this paper, we
      introduce a simpler and more effective approach, making the NMT model capable
      of open-vocabulary translation by encoding rare and unknown words as sequences
      of subword units. This is based on the intuition that various word classes are
      translatable via smaller units than words, for instance names (via character
      copying or transliteration), compounds (via compositional translation), and
      cognates and loanwords (via phonological and morphological transformations).
      We discuss the suitability of different word segmentation techniques, including
      simple character n-gram models and a segmentation based on the byte pair encoding
      compression algorithm, and empirically show that subword models improve over
      a back-off dictionary baseline for the WMT 15 translation tasks English-German
      and English-Russian by 1.1 and 1.3 BLEU, respectively.", "venue": "Annual Meeting
      of the Association for Computational Linguistics", "year": 2015, "referenceCount":
      41, "citationCount": 6420, "influentialCitationCount": 1019, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P16-1162.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2015-08-31", "journal": {"volume": "abs/1508.07909", "name":
      "ArXiv"}, "citationStyles": {"bibtex": "@Article{Sennrich2015NeuralMT,\n author
      = {Rico Sennrich and B. Haddow and Alexandra Birch},\n booktitle = {Annual Meeting
      of the Association for Computational Linguistics},\n journal = {ArXiv},\n title
      = {Neural Machine Translation of Rare Words with Subword Units},\n volume =
      {abs/1508.07909},\n year = {2015}\n}\n"}, "authors": [{"authorId": "2082372",
      "name": "Rico Sennrich"}, {"authorId": "2259100", "name": "B. Haddow"}, {"authorId":
      "2539211", "name": "Alexandra Birch"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Natural language inference SNLI [5], MultiNLI [66], Question NLI
      [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Natural
      language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail
      [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR
      Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026"], "citedPaper":
      {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "externalIds": {"MAG":
      "2953084091", "DBLP": "conf/emnlp/BowmanAPM15", "ACL": "D15-1075", "ArXiv":
      "1508.05326", "DOI": "10.18653/v1/D15-1075", "CorpusId": 14604520}, "corpusId":
      14604520, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1",
      "title": "A large annotated corpus for learning natural language inference",
      "abstract": "Understanding entailment and contradiction is fundamental to understanding
      natural language, and inference about entailment and contradiction is a valuable
      testing ground for the development of semantic representations. However, machine
      learning research in this area has been dramatically limited by the lack of
      large-scale resources. To address this, we introduce the Stanford Natural Language
      Inference corpus, a new, freely available collection of labeled sentence pairs,
      written by humans doing a novel grounded task based on image captioning. At
      570K pairs, it is two orders of magnitude larger than all other resources of
      its type. This increase in scale allows lexicalized classifiers to outperform
      some sophisticated existing entailment models, and it allows a neural network-based
      model to perform competitively on natural language inference benchmarks for
      the first time.", "venue": "Conference on Empirical Methods in Natural Language
      Processing", "year": 2015, "referenceCount": 40, "citationCount": 3562, "influentialCitationCount":
      906, "isOpenAccess": true, "openAccessPdf": {"url": "https://doi.org/10.18653/v1/d15-1075",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-21",
      "journal": {"pages": "632-642"}, "citationStyles": {"bibtex": "@Article{Bowman2015ALA,\n
      author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher
      D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language
      Processing},\n pages = {632-642},\n title = {A large annotated corpus for learning
      natural language inference},\n year = {2015}\n}\n"}, "authors": [{"authorId":
      "3644767", "name": "Samuel R. Bowman"}, {"authorId": "32301760", "name": "Gabor
      Angeli"}, {"authorId": "144922861", "name": "Christopher Potts"}, {"authorId":
      "144783904", "name": "Christopher D. Manning"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "Phrase-level or sentence-level embeddings,
      which can be trained using an unlabeled corpus, have been used to encode text
      into suitable vector representations for various target tasks [28, 32, 1, 36,
      22, 12, 56, 31].", "intents": ["methodology"]}], "intents": ["methodology"],
      "contexts": ["Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31]."],
      "citedPaper": {"paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "externalIds":
      {"MAG": "1486649854", "ArXiv": "1506.06726", "DBLP": "journals/corr/KirosZSZTUF15",
      "CorpusId": 9126867}, "corpusId": 9126867, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/6e795c6e9916174ae12349f5dc3f516570c17ce8",
      "title": "Skip-Thought Vectors", "abstract": "We describe an approach for unsupervised
      learning of a generic, distributed sentence encoder. Using the continuity of
      text from books, we train an encoder-decoder model that tries to reconstruct
      the surrounding sentences of an encoded passage. Sentences that share semantic
      and syntactic properties are thus mapped to similar vector representations.
      We next introduce a simple vocabulary expansion method to encode words that
      were not seen as part of training, allowing us to expand our vocabulary to a
      million words. After training our model, we extract and evaluate our vectors
      with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence
      ranking, question-type classification and 4 benchmark sentiment and subjectivity
      datasets. The end result is an off-the-shelf encoder that can produce highly
      generic sentence representations that are robust and perform well in practice.",
      "venue": "Neural Information Processing Systems", "year": 2015, "referenceCount":
      43, "citationCount": 2226, "influentialCitationCount": 283, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2015-06-22", "journal": {"pages": "3294-3302"},
      "citationStyles": {"bibtex": "@Article{Kiros2015SkipThoughtV,\n author = {Ryan
      Kiros and Yukun Zhu and R. Salakhutdinov and R. Zemel and R. Urtasun and A.
      Torralba and S. Fidler},\n booktitle = {Neural Information Processing Systems},\n
      pages = {3294-3302},\n title = {Skip-Thought Vectors},\n year = {2015}\n}\n"},
      "authors": [{"authorId": "3450996", "name": "Ryan Kiros"}, {"authorId": "1844940337",
      "name": "Yukun Zhu"}, {"authorId": "145124475", "name": "R. Salakhutdinov"},
      {"authorId": "1804104", "name": "R. Zemel"}, {"authorId": "2422559", "name":
      "R. Urtasun"}, {"authorId": "143805211", "name": "A. Torralba"}, {"authorId":
      "37895334", "name": "S. Fidler"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Unsupervised pre-training We use the BooksCorpus dataset [71]
      for training the language model.", "intents": ["methodology"]}], "intents":
      ["methodology"], "contexts": ["Unsupervised pre-training We use the BooksCorpus
      dataset [71] for training the language model."], "citedPaper": {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
      "externalIds": {"ArXiv": "1506.06724", "DBLP": "journals/corr/ZhuKZSUTF15",
      "MAG": "1566289585", "DOI": "10.1109/ICCV.2015.11", "CorpusId": 6866988}, "corpusId":
      6866988, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045",
      "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations
      by Watching Movies and Reading Books", "abstract": "Books are a rich source
      of both fine-grained information, how a character, an object or a scene looks
      like, as well as high-level semantics, what someone is thinking, feeling and
      how these states evolve through a story. This paper aims to align books to their
      movie releases in order to provide rich descriptive explanations for visual
      content that go semantically far beyond the captions available in the current
      datasets. To align movies and books we propose a neural sentence embedding that
      is trained in an unsupervised way from a large corpus of books, as well as a
      video-text neural embedding for computing similarities between movie clips and
      sentences in the book. We propose a context-aware CNN to combine information
      from multiple sources. We demonstrate good quantitative performance for movie/book
      alignment and show several qualitative examples that showcase the diversity
      of tasks our model can be used for.", "venue": "IEEE International Conference
      on Computer Vision", "year": 2015, "referenceCount": 44, "citationCount": 2122,
      "influentialCitationCount": 244, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://www.cs.toronto.edu/%7Ezemel/documents/align.pdf", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-06-22", "journal":
      {"pages": "19-27", "name": "2015 IEEE International Conference on Computer Vision
      (ICCV)"}, "citationStyles": {"bibtex": "@Article{Zhu2015AligningBA,\n author
      = {Yukun Zhu and Ryan Kiros and R. Zemel and R. Salakhutdinov and R. Urtasun
      and A. Torralba and S. Fidler},\n booktitle = {IEEE International Conference
      on Computer Vision},\n journal = {2015 IEEE International Conference on Computer
      Vision (ICCV)},\n pages = {19-27},\n title = {Aligning Books and Movies: Towards
      Story-Like Visual Explanations by Watching Movies and Reading Books},\n year
      = {2015}\n}\n"}, "authors": [{"authorId": "1844940337", "name": "Yukun Zhu"},
      {"authorId": "3450996", "name": "Ryan Kiros"}, {"authorId": "1804104", "name":
      "R. Zemel"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId":
      "2422559", "name": "R. Urtasun"}, {"authorId": "143805211", "name": "A. Torralba"},
      {"authorId": "37895334", "name": "S. Fidler"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "This corpus has been shown to contain more reasoning type questions
      that other datasets like CNN [19] or SQuaD [47], providing the perfect evaluation
      for our model which is trained to handle long-range contexts.", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["This corpus has been shown to contain
      more reasoning type questions that other datasets like CNN [19] or SQuaD [47],
      providing the perfect evaluation for our model which is trained to handle long-range
      contexts."], "citedPaper": {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
      "externalIds": {"MAG": "2949615363", "DBLP": "conf/nips/HermannKGEKSB15", "ArXiv":
      "1506.03340", "CorpusId": 6203757}, "corpusId": 6203757, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72",
      "title": "Teaching Machines to Read and Comprehend", "abstract": "Teaching machines
      to read natural language documents remains an elusive challenge. Machine reading
      systems can be tested on their ability to answer questions posed on the contents
      of documents that they have seen, but until now large scale training and test
      datasets have been missing for this type of evaluation. In this work we define
      a new methodology that resolves this bottleneck and provides large scale supervised
      reading comprehension data. This allows us to develop a class of attention based
      deep neural networks that learn to read real documents and answer complex questions
      with minimal prior knowledge of language structure.", "venue": "Neural Information
      Processing Systems", "year": 2015, "referenceCount": 22, "citationCount": 3097,
      "influentialCitationCount": 490, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-06-10", "journal":
      {"volume": "abs/1506.03340", "name": "ArXiv"}, "citationStyles": {"bibtex":
      "@Article{Hermann2015TeachingMT,\n author = {K. Hermann and Tom\u00e1s Kocisk\u00fd
      and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman
      and Phil Blunsom},\n booktitle = {Neural Information Processing Systems},\n
      journal = {ArXiv},\n title = {Teaching Machines to Read and Comprehend},\n volume
      = {abs/1506.03340},\n year = {2015}\n}\n"}, "authors": [{"authorId": "2910877",
      "name": "K. Hermann"}, {"authorId": "2367821", "name": "Tom\u00e1s Kocisk\u00fd"},
      {"authorId": "1864353", "name": "Edward Grefenstette"}, {"authorId": "2311318",
      "name": "Lasse Espeholt"}, {"authorId": "2062879616", "name": "Will Kay"}, {"authorId":
      "2573615", "name": "Mustafa Suleyman"}, {"authorId": "1685771", "name": "Phil
      Blunsom"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context": "We
      used the Adam optimization scheme [27] with a max learning rate of 2.", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["We used the Adam
      optimization scheme [27] with a max learning rate of 2."], "citedPaper": {"paperId":
      "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "externalIds": {"MAG": "2964121744",
      "DBLP": "journals/corr/KingmaB14", "ArXiv": "1412.6980", "CorpusId": 6628106},
      "corpusId": 6628106, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
      "title": "Adam: A Method for Stochastic Optimization", "abstract": "We introduce
      Adam, an algorithm for first-order gradient-based optimization of stochastic
      objective functions, based on adaptive estimates of lower-order moments. The
      method is straightforward to implement, is computationally efficient, has little
      memory requirements, is invariant to diagonal rescaling of the gradients, and
      is well suited for problems that are large in terms of data and/or parameters.
      The method is also appropriate for non-stationary objectives and problems with
      very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations
      and typically require little tuning. Some connections to related algorithms,
      on which Adam was inspired, are discussed. We also analyze the theoretical convergence
      properties of the algorithm and provide a regret bound on the convergence rate
      that is comparable to the best known results under the online convex optimization
      framework. Empirical results demonstrate that Adam works well in practice and
      compares favorably to other stochastic optimization methods. Finally, we discuss
      AdaMax, a variant of Adam based on the infinity norm.", "venue": "International
      Conference on Learning Representations", "year": 2014, "referenceCount": 26,
      "citationCount": 128945, "influentialCitationCount": 20928, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2014-12-22", "journal":
      {"volume": "abs/1412.6980", "name": "CoRR"}, "citationStyles": {"bibtex": "@Article{Kingma2014AdamAM,\n
      author = {Diederik P. Kingma and Jimmy Ba},\n booktitle = {International Conference
      on Learning Representations},\n journal = {CoRR},\n title = {Adam: A Method
      for Stochastic Optimization},\n volume = {abs/1412.6980},\n year = {2014}\n}\n"},
      "authors": [{"authorId": "1726807", "name": "Diederik P. Kingma"}, {"authorId":
      "2503659", "name": "Jimmy Ba"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "The most compelling evidence for this so far has been the extensive
      use of pre-trained word embeddings [10, 39, 42] to improve performance on a
      range of NLP tasks [8, 11, 26, 45].", "intents": ["background"]}, {"context":
      "Over the last few years, researchers have demonstrated the bene\ufb01ts of
      using word embeddings [11, 39, 42], which are trained on unlabeled corpora,
      to improve performance on a variety of tasks [8, 11, 26, 45].", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["The most compelling evidence for this
      so far has been the extensive use of pre-trained word embeddings [10, 39, 42]
      to improve performance on a range of NLP tasks [8, 11, 26, 45].", "Over the
      last few years, researchers have demonstrated the bene\ufb01ts of using word
      embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve
      performance on a variety of tasks [8, 11, 26, 45]."], "citedPaper": {"paperId":
      "f37e1b62a767a307c046404ca96bc140b3e68cb5", "externalIds": {"ACL": "D14-1162",
      "MAG": "2250539671", "DBLP": "conf/emnlp/PenningtonSM14", "DOI": "10.3115/v1/D14-1162",
      "CorpusId": 1957433}, "corpusId": 1957433, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
      "title": "GloVe: Global Vectors for Word Representation", "abstract": "Recent
      methods for learning vector space representations of words have succeeded in
      capturing fine-grained semantic and syntactic regularities using vector arithmetic,
      but the origin of these regularities has remained opaque. We analyze and make
      explicit the model properties needed for such regularities to emerge in word
      vectors. The result is a new global logbilinear regression model that combines
      the advantages of the two major model families in the literature: global matrix
      factorization and local context window methods. Our model efficiently leverages
      statistical information by training only on the nonzero elements in a word-word
      cooccurrence matrix, rather than on the entire sparse matrix or on individual
      context windows in a large corpus. The model produces a vector space with meaningful
      substructure, as evidenced by its performance of 75% on a recent word analogy
      task. It also outperforms related models on similarity tasks and named entity
      recognition.", "venue": "Conference on Empirical Methods in Natural Language
      Processing", "year": 2014, "referenceCount": 32, "citationCount": 28580, "influentialCitationCount":
      4356, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2014-10-01", "journal": {"pages": "1532-1543"}, "citationStyles":
      {"bibtex": "@Article{Pennington2014GloVeGV,\n author = {Jeffrey Pennington and
      R. Socher and Christopher D. Manning},\n booktitle = {Conference on Empirical
      Methods in Natural Language Processing},\n pages = {1532-1543},\n title = {GloVe:
      Global Vectors for Word Representation},\n year = {2014}\n}\n"}, "authors":
      [{"authorId": "143845796", "name": "Jeffrey Pennington"}, {"authorId": "2166511",
      "name": "R. Socher"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "Over the last few
      years, researchers have demonstrated the benefits of using word embeddings [11,
      39, 42], which are trained on unlabeled corpora, to improve performance on a
      variety of tasks [8, 11, 26, 45].", "intents": ["background"]}, {"context":
      "The most compelling evidence for this so far has been the extensive use of
      pretrained word embeddings [10, 39, 42] to improve performance on a range of
      NLP tasks [8, 11, 26, 45].", "intents": ["background"]}], "intents": ["background"],
      "contexts": ["Over the last few years, researchers have demonstrated the benefits
      of using word embeddings [11, 39, 42], which are trained on unlabeled corpora,
      to improve performance on a variety of tasks [8, 11, 26, 45].", "The most compelling
      evidence for this so far has been the extensive use of pretrained word embeddings
      [10, 39, 42] to improve performance on a range of NLP tasks [8, 11, 26, 45]."],
      "citedPaper": {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "externalIds":
      {"MAG": "2949541494", "DBLP": "journals/corr/Kim14f", "ArXiv": "1408.5882",
      "ACL": "D14-1181", "DOI": "10.3115/v1/D14-1181", "CorpusId": 9672033}, "corpusId":
      9672033, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
      "title": "Convolutional Neural Networks for Sentence Classification", "abstract":
      "We report on a series of experiments with convolutional neural networks (CNN)
      trained on top of pre-trained word vectors for sentence-level classification
      tasks. We show that a simple CNN with little hyperparameter tuning and static
      vectors achieves excellent results on multiple benchmarks. Learning task-specific
      vectors through fine-tuning offers further gains in performance. We additionally
      propose a simple modification to the architecture to allow for the use of both
      task-specific and static vectors. The CNN models discussed herein improve upon
      the state of the art on 4 out of 7 tasks, which include sentiment analysis and
      question classification.", "venue": "Conference on Empirical Methods in Natural
      Language Processing", "year": 2014, "referenceCount": 33, "citationCount": 12277,
      "influentialCitationCount": 2040, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://aclanthology.org/D14-1181.pdf", "status": null}, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2014-08-25", "journal":
      {"pages": "1746-1751"}, "citationStyles": {"bibtex": "@Article{Kim2014ConvolutionalNN,\n
      author = {Yoon Kim},\n booktitle = {Conference on Empirical Methods in Natural
      Language Processing},\n pages = {1746-1751},\n title = {Convolutional Neural
      Networks for Sentence Classification},\n year = {2014}\n}\n"}, "authors": [{"authorId":
      "38367242", "name": "Yoon Kim"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Phrase-level or sentence-level embeddings, which can be trained
      using an unlabeled corpus, have been used to encode text into suitable vector
      representations for various target tasks [28, 32, 1, 36, 22, 12, 56, 31].",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Phrase-level
      or sentence-level embeddings, which can be trained using an unlabeled corpus,
      have been used to encode text into suitable vector representations for various
      target tasks [28, 32, 1, 36, 22, 12, 56, 31]."], "citedPaper": {"paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
      "externalIds": {"DBLP": "conf/icml/LeM14", "ArXiv": "1405.4053", "MAG": "2949547296",
      "CorpusId": 2407601}, "corpusId": 2407601, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/f3de86aeb442216a8391befcacb49e58b478f512",
      "title": "Distributed Representations of Sentences and Documents", "abstract":
      "Many machine learning algorithms require the input to be represented as a fixed-length
      feature vector. When it comes to texts, one of the most common fixed-length
      features is bag-of-words. Despite their popularity, bag-of-words features have
      two major weaknesses: they lose the ordering of the words and they also ignore
      semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\"
      are equally distant. In this paper, we propose Paragraph Vector, an unsupervised
      algorithm that learns fixed-length feature representations from variable-length
      pieces of texts, such as sentences, paragraphs, and documents. Our algorithm
      represents each document by a dense vector which is trained to predict words
      in the document. Its construction gives our algorithm the potential to overcome
      the weaknesses of bag-of-words models. Empirical results show that Paragraph
      Vectors outperforms bag-of-words models as well as other techniques for text
      representations. Finally, we achieve new state-of-the-art results on several
      text classification and sentiment analysis tasks.", "venue": "International
      Conference on Machine Learning", "year": 2014, "referenceCount": 42, "citationCount":
      8501, "influentialCitationCount": 1174, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2014-05-16", "journal": {"pages": "1188-1196"}, "citationStyles":
      {"bibtex": "@Article{Le2014DistributedRO,\n author = {Quoc V. Le and Tomas Mikolov},\n
      booktitle = {International Conference on Machine Learning},\n pages = {1188-1196},\n
      title = {Distributed Representations of Sentences and Documents},\n year = {2014}\n}\n"},
      "authors": [{"authorId": "2827616", "name": "Quoc V. Le"}, {"authorId": "2047446108",
      "name": "Tomas Mikolov"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "The most compelling evidence for this so far has been the extensive
      use of pre-trained word embeddings [10, 39, 42] to improve performance on a
      range of NLP tasks [8, 11, 26, 45].", "intents": ["background"]}, {"context":
      "Over the last few years, researchers have demonstrated the bene\ufb01ts of
      using word embeddings [11, 39, 42], which are trained on unlabeled corpora,
      to improve performance on a variety of tasks [8, 11, 26, 45].", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["The most compelling evidence for this
      so far has been the extensive use of pre-trained word embeddings [10, 39, 42]
      to improve performance on a range of NLP tasks [8, 11, 26, 45].", "Over the
      last few years, researchers have demonstrated the bene\ufb01ts of using word
      embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve
      performance on a variety of tasks [8, 11, 26, 45]."], "citedPaper": {"paperId":
      "87f40e6f3022adbc1f1905e3e506abad05a9964f", "externalIds": {"MAG": "2950133940",
      "ArXiv": "1310.4546", "DBLP": "conf/nips/MikolovSCCD13", "CorpusId": 16447573},
      "corpusId": 16447573, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "abstract": "The recently introduced continuous Skip-gram model is an efficient
      method for learning high-quality distributed vector representations that capture
      a large number of precise syntactic and semantic word relationships. In this
      paper we present several extensions that improve both the quality of the vectors
      and the training speed. By subsampling of the frequent words we obtain significant
      speedup and also learn more regular word representations. We also describe a
      simple alternative to the hierarchical softmax called negative sampling. \n
      \nAn inherent limitation of word representations is their indifference to word
      order and their inability to represent idiomatic phrases. For example, the meanings
      of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".
      Motivated by this example, we present a simple method for finding phrases in
      text, and show that learning good vector representations for millions of phrases
      is possible.", "venue": "Neural Information Processing Systems", "year": 2013,
      "referenceCount": 24, "citationCount": 30855, "influentialCitationCount": 4175,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-10-16", "journal": {"pages": "3111-3119"},
      "citationStyles": {"bibtex": "@Article{Mikolov2013DistributedRO,\n author =
      {Tomas Mikolov and I. Sutskever and Kai Chen and G. Corrado and J. Dean},\n
      booktitle = {Neural Information Processing Systems},\n pages = {3111-3119},\n
      title = {Distributed Representations of Words and Phrases and their Compositionality},\n
      year = {2013}\n}\n"}, "authors": [{"authorId": "2047446108", "name": "Tomas
      Mikolov"}, {"authorId": "1701686", "name": "I. Sutskever"}, {"authorId": "2118440152",
      "name": "Kai Chen"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId":
      "49959210", "name": "J. Dean"}]}}, {"isInfluential": false, "contextsWithIntent":
      [], "intents": [], "contexts": [], "citedPaper": {"paperId": "0e5fa90e28fab414c8ef3ac6ca937c6195c2860e",
      "externalIds": {"MAG": "2250565861", "ACL": "D13-1090", "DBLP": "conf/emnlp/JiE13",
      "CorpusId": 14169402}, "corpusId": 14169402, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/0e5fa90e28fab414c8ef3ac6ca937c6195c2860e",
      "title": "Discriminative Improvements to Distributional Sentence Similarity",
      "abstract": "Matrix and tensor factorization have been applied to a number of
      semantic relatedness tasks, including paraphrase identification. The key idea
      is that similarity in the latent space implies semantic relatedness. We describe
      three ways in which labeled data can improve the accuracy of these approaches
      on paraphrase classification. First, we design a new discriminative term-weighting
      metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the
      latent representation from matrix factorization as features in a classification
      algorithm substantially improves accuracy. Finally, we combine latent features
      with fine-grained n-gram overlap features, yielding performance that is 3% more
      accurate than the prior state-of-the-art.", "venue": "Conference on Empirical
      Methods in Natural Language Processing", "year": 2013, "referenceCount": 26,
      "citationCount": 157, "influentialCitationCount": 25, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-10-01", "journal": {"pages": "891-896"},
      "citationStyles": {"bibtex": "@Article{Ji2013DiscriminativeIT,\n author = {Yangfeng
      Ji and Jacob Eisenstein},\n booktitle = {Conference on Empirical Methods in
      Natural Language Processing},\n pages = {891-896},\n title = {Discriminative
      Improvements to Distributional Sentence Similarity},\n year = {2013}\n}\n"},
      "authors": [{"authorId": "40608686", "name": "Yangfeng Ji"}, {"authorId": "144154709",
      "name": "Jacob Eisenstein"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "\u2026[64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is shuf\ufb02ed at a sentence level - destroying long-range
      structure.", "intents": ["background"]}, {"context": "The Stanford Sentiment
      Treebank (SST-2) [54], on the other hand, is a standard binary classi\ufb01cation
      task.", "intents": ["background"]}], "intents": ["background"], "contexts":
      ["\u2026[64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze
      [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9],
      STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA
      [65] but is shuf\ufb02ed at a sentence level - destroying long-range structure.",
      "The Stanford Sentiment Treebank (SST-2) [54], on the other hand, is a standard
      binary classi\ufb01cation task."], "citedPaper": {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
      "externalIds": {"ACL": "D13-1170", "DBLP": "conf/emnlp/SocherPWCMNP13", "MAG":
      "2251939518", "CorpusId": 990233}, "corpusId": 990233, "publicationVenue": {"id":
      "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods
      in Natural Language Processing", "type": "conference", "alternate_names": ["Empir
      Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
      "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
      "url": "https://www.semanticscholar.org/paper/687bac2d3320083eb4530bf18bb8f8f721477600",
      "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment
      Treebank", "abstract": "Semantic word spaces have been very useful but cannot
      express the meaning of longer phrases in a principled way. Further progress
      towards understanding compositionality in tasks such as sentiment detection
      requires richer supervised training and evaluation resources and more powerful
      models of composition. To remedy this, we introduce a Sentiment Treebank. It
      includes fine grained sentiment labels for 215,154 phrases in the parse trees
      of 11,855 sentences and presents new challenges for sentiment compositionality.
      To address them, we introduce the Recursive Neural Tensor Network. When trained
      on the new treebank, this model outperforms all previous methods on several
      metrics. It pushes the state of the art in single sentence positive/negative
      classification from 80% up to 85.4%. The accuracy of predicting fine-grained
      sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over
      bag of features baselines. Lastly, it is the only model that can accurately
      capture the effects of negation and its scope at various tree levels for both
      positive and negative phrases.", "venue": "Conference on Empirical Methods in
      Natural Language Processing", "year": 2013, "referenceCount": 49, "citationCount":
      6967, "influentialCitationCount": 1218, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-10-01", "journal": {"pages": "1631-1642"}, "citationStyles":
      {"bibtex": "@Article{Socher2013RecursiveDM,\n author = {R. Socher and Alex Perelygin
      and Jean Wu and Jason Chuang and Christopher D. Manning and A. Ng and Christopher
      Potts},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n
      pages = {1631-1642},\n title = {Recursive Deep Models for Semantic Compositionality
      Over a Sentiment Treebank},\n year = {2013}\n}\n"}, "authors": [{"authorId":
      "2166511", "name": "R. Socher"}, {"authorId": "24590005", "name": "Alex Perelygin"},
      {"authorId": "2110402830", "name": "Jean Wu"}, {"authorId": "1964541", "name":
      "Jason Chuang"}, {"authorId": "144783904", "name": "Christopher D. Manning"},
      {"authorId": "34699434", "name": "A. Ng"}, {"authorId": "144922861", "name":
      "Christopher Potts"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "In recent work, the method has been used to help train deep neural networks
      on various tasks like image classification [69], speech recognition [68], entity
      disambiguation [17] and machine translation [48].", "intents": ["methodology"]}],
      "intents": ["methodology"], "contexts": ["In recent work, the method has been
      used to help train deep neural networks on various tasks like image classification
      [69], speech recognition [68], entity disambiguation [17] and machine translation
      [48]."], "citedPaper": {"paperId": "10ce58375314f90f83ca3bb88840cbc67bf8050f",
      "externalIds": {"DBLP": "conf/cncl/CaiWZ15", "MAG": "2157191138", "ACL": "P13-2006",
      "DOI": "10.1007/978-3-319-25816-4_22", "CorpusId": 7512771}, "corpusId": 7512771,
      "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual
      Meeting of the Association for Computational Linguistics", "type": "conference",
      "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association
      for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url":
      "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/10ce58375314f90f83ca3bb88840cbc67bf8050f",
      "title": "Learning Entity Representation for Entity Disambiguation", "abstract":
      null, "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": 2013, "referenceCount": 38, "citationCount": 153, "influentialCitationCount":
      18, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2013-08-01", "journal":
      {"pages": "267-278"}, "citationStyles": {"bibtex": "@Article{He2013LearningER,\n
      author = {Zhengyan He and Shujie Liu and Mu Li and Ming Zhou and Longkai Zhang
      and Houfeng Wang},\n booktitle = {Annual Meeting of the Association for Computational
      Linguistics},\n pages = {267-278},\n title = {Learning Entity Representation
      for Entity Disambiguation},\n year = {2013}\n}\n"}, "authors": [{"authorId":
      "2259453", "name": "Zhengyan He"}, {"authorId": "1803054", "name": "Shujie Liu"},
      {"authorId": "2112143809", "name": "Mu Li"}, {"authorId": "92660691", "name":
      "Ming Zhou"}, {"authorId": "1817730", "name": "Longkai Zhang"}, {"authorId":
      "1781885", "name": "Houfeng Wang"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "For DPRD [46] (winograd schemas), we replace the de\ufb01nite
      pronoun with the two possible referrents and predict the resolution that the
      generative model assigns higher average token log-probability to the rest of
      the sequence after the substitution.", "intents": []}, {"context": "For DPRD
      [46] (winograd schemas), we replace the definite pronoun with the two possible
      referrents and predict the resolution that the generative model assigns higher
      average token log-probability to the rest of the sequence after the substitution.",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["For
      DPRD [46] (winograd schemas), we replace the de\ufb01nite pronoun with the two
      possible referrents and predict the resolution that the generative model assigns
      higher average token log-probability to the rest of the sequence after the substitution.",
      "For DPRD [46] (winograd schemas), we replace the definite pronoun with the
      two possible referrents and predict the resolution that the generative model
      assigns higher average token log-probability to the rest of the sequence after
      the substitution."], "citedPaper": {"paperId": "b0c5f72790cca220541ea4809d1e43b4bdad1124",
      "externalIds": {"DBLP": "conf/emnlp/RahmanN12", "MAG": "1752492850", "ACL":
      "D12-1071", "CorpusId": 15274877}, "corpusId": 15274877, "publicationVenue":
      {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical
      Methods in Natural Language Processing", "type": "conference", "alternate_names":
      ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
      "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
      "url": "https://www.semanticscholar.org/paper/b0c5f72790cca220541ea4809d1e43b4bdad1124",
      "title": "Resolving Complex Cases of Definite Pronouns: The Winograd Schema
      Challenge", "abstract": "We examine the task of resolving complex cases of definite
      pronouns, specifically those for which traditional linguistic constraints on
      coreference (e.g., Binding Constraints, gender and number agreement) as well
      as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic
      salience) are not useful. Being able to solve this task has broader implications
      in artificial intelligence: a restricted version of it, sometimes referred to
      as the Winograd Schema Challenge, has been suggested as a conceptually and practically
      appealing alternative to the Turing Test. We employ a knowledge-rich approach
      to this task, which yields a pronoun resolver that outperforms state-of-the-art
      resolvers by nearly 18 points in accuracy on our dataset.", "venue": "Conference
      on Empirical Methods in Natural Language Processing", "year": 2012, "referenceCount":
      47, "citationCount": 182, "influentialCitationCount": 40, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2012-07-12",
      "journal": {"pages": "777-789"}, "citationStyles": {"bibtex": "@Article{Rahman2012ResolvingCC,\n
      author = {Altaf Rahman and Vincent Ng},\n booktitle = {Conference on Empirical
      Methods in Natural Language Processing},\n pages = {777-789},\n title = {Resolving
      Complex Cases of Definite Pronouns: The Winograd Schema Challenge},\n year =
      {2012}\n}\n"}, "authors": [{"authorId": "38999890", "name": "Altaf Rahman"},
      {"authorId": "145106110", "name": "Vincent Ng"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "The most compelling evidence for this so
      far has been the extensive use of pre-trained word embeddings [10, 39, 42] to
      improve performance on a range of NLP tasks [8, 11, 26, 45].", "intents": ["background"]},
      {"context": "Over the last few years, researchers have demonstrated the bene\ufb01ts
      of using word embeddings [11, 39, 42], which are trained on unlabeled corpora,
      to improve performance on a variety of tasks [8, 11, 26, 45].", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["The most compelling evidence for this
      so far has been the extensive use of pre-trained word embeddings [10, 39, 42]
      to improve performance on a range of NLP tasks [8, 11, 26, 45].", "Over the
      last few years, researchers have demonstrated the bene\ufb01ts of using word
      embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve
      performance on a variety of tasks [8, 11, 26, 45]."], "citedPaper": {"paperId":
      "bc1022b031dc6c7019696492e8116598097a8c12", "externalIds": {"MAG": "2158899491",
      "DBLP": "journals/jmlr/CollobertWBKKK11", "ArXiv": "1103.0398", "DOI": "10.5555/1953048.2078186",
      "CorpusId": 351666}, "corpusId": 351666, "publicationVenue": {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
      "name": "Journal of machine learning research", "type": "journal", "alternate_names":
      ["Journal of Machine Learning Research", "J mach learn res", "J Mach Learn Res"],
      "issn": "1532-4435", "alternate_issns": ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/",
      "alternate_urls": ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/bc1022b031dc6c7019696492e8116598097a8c12",
      "title": "Natural Language Processing (Almost) from Scratch", "abstract": "We
      propose a unified neural network architecture and learning algorithm that can
      be applied to various natural language processing tasks including part-of-speech
      tagging, chunking, named entity recognition, and semantic role labeling. This
      versatility is achieved by trying to avoid task-specific engineering and therefore
      disregarding a lot of prior knowledge. Instead of exploiting man-made input
      features carefully optimized for each task, our system learns internal representations
      on the basis of vast amounts of mostly unlabeled training data. This work is
      then used as a basis for building a freely available tagging system with good
      performance and minimal computational requirements.", "venue": "Journal of machine
      learning research", "year": 2011, "referenceCount": 103, "citationCount": 7387,
      "influentialCitationCount": 731, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2011-02-01", "journal": {"volume": "abs/1103.0398",
      "name": "ArXiv"}, "citationStyles": {"bibtex": "@Article{Collobert2011NaturalLP,\n
      author = {Ronan Collobert and J. Weston and L. Bottou and Michael Karlen and
      K. Kavukcuoglu and P. Kuksa},\n booktitle = {Journal of machine learning research},\n
      journal = {ArXiv},\n title = {Natural Language Processing (Almost) from Scratch},\n
      volume = {abs/1103.0398},\n year = {2011}\n}\n"}, "authors": [{"authorId": "2939803",
      "name": "Ronan Collobert"}, {"authorId": "145183709", "name": "J. Weston"},
      {"authorId": "52184096", "name": "L. Bottou"}, {"authorId": "21432929", "name":
      "Michael Karlen"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"}, {"authorId":
      "46283650", "name": "P. Kuksa"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "In recent work, the method has been used to help train deep neural
      networks on various tasks like image classification [69], speech recognition
      [68], entity disambiguation [17] and machine translation [48].", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["In recent work,
      the method has been used to help train deep neural networks on various tasks
      like image classification [69], speech recognition [68], entity disambiguation
      [17] and machine translation [48]."], "citedPaper": {"paperId": "ecd4bc32bb2717c96f76dd100fcd1255a07bd656",
      "externalIds": {"MAG": "217970951", "CorpusId": 18141431}, "corpusId": 18141431,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ecd4bc32bb2717c96f76dd100fcd1255a07bd656",
      "title": "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs
      for Real-World Speech Recognition", "abstract": "Recently, deep learning techniques
      have been successfully applied to automatic speech recognition tasks -first
      to phonetic recognition with context-independent deep belief network (DBN) hidden
      Markov models (HMMs) and later to large vocabulary continuous speech recognition
      using context-dependent (CD) DBN-HMMs. In this paper, we report our most recent
      experiments designed to understand the roles of the two main phases of the DBN
      learning -pre-training and fine tuning -in the recognition performance of a
      CD-DBN-HMM based large-vocabulary speech recognizer. As expected, we show that
      pre-training can initialize weights to a point in the space where fine-tuning
      can be effective and thus is crucial in training deep structured models. However,
      a moderate increase of the amount of unlabeled pre-training data has an insignificant
      effect on the final recognition results as long as the original training size
      is sufficiently large to initialize the DBN weights. On the other hand, with
      additional labeled training data, the fine-tuning phase of DBN training can
      significantly improve the recognition accuracy.", "venue": "", "year": 2010,
      "referenceCount": 27, "citationCount": 216, "influentialCitationCount": 4, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      "2010-12-01", "journal": {"volume": "", "name": ""}, "citationStyles": {"bibtex":
      "@Inproceedings{Yu2010RolesOP,\n author = {Dong Yu and L. Deng and George E.
      Dahl},\n title = {Roles of Pre-Training and Fine-Tuning in Context-Dependent
      DBN-HMMs for Real-World Speech Recognition},\n year = {2010}\n}\n"}, "authors":
      [{"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "144718788", "name":
      "L. Deng"}, {"authorId": "35188630", "name": "George E. Dahl"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Subsequent research [15] demonstrated
      that pre-training acts as a regularization scheme, enabling better generalization
      in deep neural networks.", "intents": ["background"]}], "intents": ["background"],
      "contexts": ["Subsequent research [15] demonstrated that pre-training acts as
      a regularization scheme, enabling better generalization in deep neural networks."],
      "citedPaper": {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "externalIds":
      {"MAG": "2964947096", "DBLP": "journals/jmlr/ErhanCBV10", "DOI": "10.5555/1756006.1756025",
      "CorpusId": 15796526}, "corpusId": 15796526, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract":
      "Much recent research has been devoted to learning algorithms for deep architectures
      such as Deep Belief Networks and stacks of auto-encoder variants, with impressive
      results obtained in several areas, mostly on vision and language data sets.
      The best results obtained on supervised learning tasks involve an unsupervised
      learning component, usually in an unsupervised pre-training phase. Even though
      these new algorithms have enabled training deep models, many questions remain
      as to the nature of this difficult learning problem. The main question investigated
      here is the following: how does unsupervised pre-training work? Answering this
      questions is important if learning in deep architectures is to be further improved.
      We propose several explanatory hypotheses and test them through extensive simulations.
      We empirically show the influence of pre-training with respect to architecture
      depth, model capacity, and number of training examples. The experiments confirm
      and clarify the advantage of unsupervised pre-training. The results suggest
      that unsupervised pre-training guides the learning towards basins of attraction
      of minima that support better generalization from the training data set; the
      evidence from these results supports a regularization explanation for the effect
      of pre-training.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 69, "citationCount": 2271,
      "influentialCitationCount": 77, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2010-03-01", "journal": {"pages": "201-208"}, "citationStyles": {"bibtex":
      "@Article{Erhan2010WhyDU,\n author = {D. Erhan and Aaron C. Courville and Yoshua
      Bengio and Pascal Vincent},\n booktitle = {International Conference on Artificial
      Intelligence and Statistics},\n pages = {201-208},\n title = {Why Does Unsupervised
      Pre-training Help Deep Learning?},\n year = {2010}\n}\n"}, "authors": [{"authorId":
      "1761978", "name": "D. Erhan"}, {"authorId": "1760871", "name": "Aaron C. Courville"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "120247189",
      "name": "Pascal Vincent"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "The most compelling evidence for this so far has been the extensive
      use of pretrained word embeddings [10, 39, 42] to improve performance on a range
      of NLP tasks [8, 11, 26, 45].", "intents": ["background"]}, {"context": "Early
      work by Collobert and Weston [10] used a wide variety of auxiliary NLP tasks
      such as POS tagging, chunking, named entity recognition, and language modeling
      to improve semantic role labeling.", "intents": ["background"]}], "intents":
      ["background"], "contexts": ["The most compelling evidence for this so far has
      been the extensive use of pretrained word embeddings [10, 39, 42] to improve
      performance on a range of NLP tasks [8, 11, 26, 45].", "Early work by Collobert
      and Weston [10] used a wide variety of auxiliary NLP tasks such as POS tagging,
      chunking, named entity recognition, and language modeling to improve semantic
      role labeling."], "citedPaper": {"paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
      "externalIds": {"MAG": "2117130368", "DBLP": "conf/icml/CollobertW08", "DOI":
      "10.1145/1390156.1390177", "CorpusId": 2617020}, "corpusId": 2617020, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/57458bc1cffe5caa45a885af986d70f723f406b4",
      "title": "A unified architecture for natural language processing: deep neural
      networks with multitask learning", "abstract": "We describe a single convolutional
      neural network architecture that, given a sentence, outputs a host of language
      processing predictions: part-of-speech tags, chunks, named entity tags, semantic
      roles, semantically similar words and the likelihood that the sentence makes
      sense (grammatically and semantically) using a language model. The entire network
      is trained jointly on all these tasks using weight-sharing, an instance of multitask
      learning. All the tasks use labeled data except the language model which is
      learnt from unlabeled text and represents a novel form of semi-supervised learning
      for the shared tasks. We show how both multitask learning and semi-supervised
      learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",
      "venue": "International Conference on Machine Learning", "year": 2008, "referenceCount":
      24, "citationCount": 5614, "influentialCitationCount": 301, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2008-07-05", "journal":
      {"pages": "160-167"}, "citationStyles": {"bibtex": "@Article{Collobert2008AUA,\n
      author = {Ronan Collobert and J. Weston},\n booktitle = {International Conference
      on Machine Learning},\n pages = {160-167},\n title = {A unified architecture
      for natural language processing: deep neural networks with multitask learning},\n
      year = {2008}\n}\n"}, "authors": [{"authorId": "2939803", "name": "Ronan Collobert"},
      {"authorId": "145183709", "name": "J. Weston"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "Early works explored the use of the technique
      in image classi\ufb01cation [20, 49, 63] and regression tasks [3].", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["Early works explored
      the use of the technique in image classi\ufb01cation [20, 49, 63] and regression
      tasks [3]."], "citedPaper": {"paperId": "843959ffdccf31c6694d135fad07425924f785b1",
      "externalIds": {"MAG": "2025768430", "DBLP": "conf/icml/VincentLBM08", "DOI":
      "10.1145/1390156.1390294", "CorpusId": 207168299}, "corpusId": 207168299, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": "Previous work has shown that the difficulties in learning deep
      generative or discriminative models can be overcome by an initial unsupervised
      learning step that maps inputs to useful intermediate representations. We introduce
      and motivate a new training principle for unsupervised learning of a representation
      based on the idea of making the learned representations robust to partial corruption
      of the input pattern. This approach can be used to train autoencoders, and these
      denoising autoencoders can be stacked to initialize deep architectures. The
      algorithm can be motivated from a manifold learning and information theoretic
      perspective or from a generative model perspective. Comparative experiments
      clearly show the surprising advantage of corrupting the input of autoencoders
      on a pattern classification benchmark suite.", "venue": "International Conference
      on Machine Learning", "year": 2008, "referenceCount": 30, "citationCount": 6684,
      "influentialCitationCount": 515, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2008-07-05", "journal": {"pages": "1096-1103"},
      "citationStyles": {"bibtex": "@Article{Vincent2008ExtractingAC,\n author = {Pascal
      Vincent and H. Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},\n
      booktitle = {International Conference on Machine Learning},\n pages = {1096-1103},\n
      title = {Extracting and composing robust features with denoising autoencoders},\n
      year = {2008}\n}\n"}, "authors": [{"authorId": "120247189", "name": "Pascal
      Vincent"}, {"authorId": "1777528", "name": "H. Larochelle"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "1798462", "name": "Pierre-Antoine Manzagol"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "This paradigm has
      attracted signi\ufb01cant interest, with applications to tasks like sequence
      labeling [24, 33, 57] or text classi\ufb01cation [41, 70].", "intents": ["methodology"]}],
      "intents": ["methodology"], "contexts": ["This paradigm has attracted signi\ufb01cant
      interest, with applications to tasks like sequence labeling [24, 33, 57] or
      text classi\ufb01cation [41, 70]."], "citedPaper": {"paperId": "7ece4e8d31f872d928369ac2cf58a616a7182112",
      "externalIds": {"DBLP": "conf/acl/SuzukiI08", "MAG": "2154326713", "ACL": "P08-1076",
      "CorpusId": 647664}, "corpusId": 647664, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
      "name": "Annual Meeting of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting
      of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput
      Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url":
      "https://www.semanticscholar.org/paper/7ece4e8d31f872d928369ac2cf58a616a7182112",
      "title": "Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word
      Scale Unlabeled Data", "abstract": "This paper provides evidence that the use
      of more unlabeled data in semi-supervised learning can improve the performance
      of Natural Language Processing (NLP) tasks, such as part-of-speech tagging,
      syntactic chunking, and named entity recognition. We first propose a simple
      yet powerful semi-supervised discriminative model appropriate for handling large
      scale unlabeled data. Then, we describe experiments performed on widely used
      test collections, namely, PTB III data, CoNLL\u201900 and \u201903 shared task
      data for the above three NLP tasks, respectively. We incorporate up to 1G-words
      (one billion tokens) of unlabeled data, which is the largest amount of unlabeled
      data ever used for these tasks, to investigate the performance improvement.
      In addition, our results are superior to the best reported results for all of
      the above test collections.", "venue": "Annual Meeting of the Association for
      Computational Linguistics", "year": 2008, "referenceCount": 19, "citationCount":
      156, "influentialCitationCount": 17, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2008-06-01", "journal": {"pages": "665-673"}, "citationStyles":
      {"bibtex": "@Article{Suzuki2008SemiSupervisedSL,\n author = {Jun Suzuki and
      Hideki Isozaki},\n booktitle = {Annual Meeting of the Association for Computational
      Linguistics},\n pages = {665-673},\n title = {Semi-Supervised Sequential Labeling
      and Segmentation Using Giga-Word Scale Unlabeled Data},\n year = {2008}\n}\n"},
      "authors": [{"authorId": "144042991", "name": "Jun Suzuki"}, {"authorId": "34789234",
      "name": "Hideki Isozaki"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Early works explored the use of the technique in image classification
      [20, 49, 63] and regression tasks [3].", "intents": []}], "intents": [], "contexts":
      ["Early works explored the use of the technique in image classification [20,
      49, 63] and regression tasks [3]."], "citedPaper": {"paperId": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
      "externalIds": {"DBLP": "conf/nips/RanzatoPCL06", "MAG": "2172174689", "DOI":
      "10.7551/mitpress/7503.003.0147", "CorpusId": 819006}, "corpusId": 819006, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4",
      "title": "Efficient Learning of Sparse Representations with an Energy-Based
      Model", "abstract": "We describe a novel unsupervised method for learning sparse,
      overcomplete features. The model uses a linear encoder, and a linear decoder
      preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary
      sparse code vector. Given an input, the optimal code minimizes the distance
      between the output of the decoder and the input patch while being as similar
      as possible to the encoder output. Learning proceeds in a two-phase EM-like
      fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters
      of the encoder and decoder so as to decrease the energy. The model produces
      \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters
      when trained on natural image patches. Inference and learning are very fast,
      requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised
      method to initialize the first layer of a convolutional network, we achieved
      an error rate slightly lower than the best reported result on the MNIST dataset.
      Finally, an extension of the method is described to learn topographical filter
      maps.", "venue": "Neural Information Processing Systems", "year": 2006, "referenceCount":
      16, "citationCount": 1299, "influentialCitationCount": 61, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2006-12-04", "journal": {"pages": "1137-1144"},
      "citationStyles": {"bibtex": "@Article{Ranzato2006EfficientLO,\n author = {Marc''Aurelio
      Ranzato and Christopher S. Poultney and S. Chopra and Yann LeCun},\n booktitle
      = {Neural Information Processing Systems},\n pages = {1137-1144},\n title =
      {Efficient Learning of Sparse Representations with an Energy-Based Model},\n
      year = {2006}\n}\n"}, "authors": [{"authorId": "1706809", "name": "Marc''Aurelio
      Ranzato"}, {"authorId": "2060013", "name": "Christopher S. Poultney"}, {"authorId":
      "3295092", "name": "S. Chopra"}, {"authorId": "1688882", "name": "Yann LeCun"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "Early works explored
      the use of the technique in image classi\ufb01cation [20, 49, 63] and regression
      tasks [3].", "intents": ["methodology"]}], "intents": ["methodology"], "contexts":
      ["Early works explored the use of the technique in image classi\ufb01cation
      [20, 49, 63] and regression tasks [3]."], "citedPaper": {"paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
      "externalIds": {"MAG": "2110798204", "DBLP": "conf/nips/BengioLPL06", "DOI":
      "10.7551/mitpress/7503.003.0024", "CorpusId": 14201947}, "corpusId": 14201947,
      "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural
      Information Processing Systems", "type": "conference", "alternate_names": ["Neural
      Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url":
      "https://www.semanticscholar.org/paper/355d44f53428b1ac4fb2ab468d593c720640e5bd",
      "title": "Greedy Layer-Wise Training of Deep Networks", "abstract": "Complexity
      theory of circuits strongly suggests that deep architectures can be much more
      efficient (sometimes exponentially) than shallow architectures, in terms of
      computational elements required to represent some functions. Deep multi-layer
      neural networks have many levels of non-linearities allowing them to compactly
      represent highly non-linear and highly-varying functions. However, until recently
      it was not clear how to train such deep networks, since gradient-based optimization
      starting from random initialization appears to often get stuck in poor solutions.
      Hinton et al. recently introduced a greedy layer-wise unsupervised learning
      algorithm for Deep Belief Networks (DBN), a generative model with many layers
      of hidden causal variables. In the context of the above optimization problem,
      we study this algorithm empirically and explore variants to better understand
      its success and extend it to cases where the inputs are continuous or where
      the structure of the input distribution is not revealing enough about the variable
      to be predicted in a supervised task. Our experiments also confirm the hypothesis
      that the greedy layer-wise unsupervised training strategy mostly helps the optimization,
      by initializing weights in a region near a good local minimum, giving rise to
      internal distributed representations that are high-level abstractions of the
      input, bringing better generalization.", "venue": "Neural Information Processing
      Systems", "year": 2006, "referenceCount": 18, "citationCount": 4379, "influentialCitationCount":
      239, "isOpenAccess": true, "openAccessPdf": {"url": "http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2006-12-04", "journal": {"pages": "153-160"}, "citationStyles":
      {"bibtex": "@Article{Bengio2006GreedyLT,\n author = {Yoshua Bengio and Pascal
      Lamblin and D. Popovici and H. Larochelle},\n booktitle = {Neural Information
      Processing Systems},\n pages = {153-160},\n title = {Greedy Layer-Wise Training
      of Deep Networks},\n year = {2006}\n}\n"}, "authors": [{"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "3087941", "name": "Pascal Lamblin"},
      {"authorId": "32384143", "name": "D. Popovici"}, {"authorId": "1777528", "name":
      "H. Larochelle"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "This paradigm has attracted significant interest, with applications to tasks
      like sequence labeling [24, 33, 57] or text classification [41, 70].", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["This paradigm has
      attracted significant interest, with applications to tasks like sequence labeling
      [24, 33, 57] or text classification [41, 70]."], "citedPaper": {"paperId": "d5f5110d65eda0d2df7329582a232a86bf9a3a65",
      "externalIds": {"MAG": "2150969560", "DBLP": "conf/acl/JiaoWLGS06", "ACL": "P06-1027",
      "DOI": "10.3115/1220175.1220202", "CorpusId": 89684}, "corpusId": 89684, "publicationVenue":
      {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the
      Association for Computational Linguistics", "type": "conference", "alternate_names":
      ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational
      Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"},
      "url": "https://www.semanticscholar.org/paper/d5f5110d65eda0d2df7329582a232a86bf9a3a65",
      "title": "Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation
      and Labeling", "abstract": "We present a new semi-supervised training procedure
      for conditional random fields (CRFs) that can be used to train sequence segmentors
      and labelers from a combination of labeled and unlabeled training data. Our
      approach is based on extending the minimum entropy regularization framework
      to the structured prediction case, yielding a training objective that combines
      unlabeled conditional entropy with labeled conditional likelihood. Although
      the training objective is no longer concave, it can still be used to improve
      an initial model (e.g. obtained from supervised training) by iterative ascent.
      We apply our new training algorithm to the problem of identifying gene and protein
      mentions in biological texts, and show that incorporating unlabeled data improves
      the performance of the supervised CRF in this case.", "venue": "Annual Meeting
      of the Association for Computational Linguistics", "year": 2006, "referenceCount":
      26, "citationCount": 150, "influentialCitationCount": 22, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.3115/1220175.1220202",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2006-07-17", "journal": {"volume": "", "pages": "209-216",
      "name": ""}, "citationStyles": {"bibtex": "@Article{Jiao2006SemiSupervisedCR,\n
      author = {Feng Jiao and Shaojun Wang and Chi-Hoon Lee and R. Greiner and Dale
      Schuurmans},\n booktitle = {Annual Meeting of the Association for Computational
      Linguistics},\n pages = {209-216},\n title = {Semi-Supervised Conditional Random
      Fields for Improved Sequence Segmentation and Labeling},\n year = {2006}\n}\n"},
      "authors": [{"authorId": "145644673", "name": "Feng Jiao"}, {"authorId": "1721114",
      "name": "Shaojun Wang"}, {"authorId": "2109465325", "name": "Chi-Hoon Lee"},
      {"authorId": "143686063", "name": "R. Greiner"}, {"authorId": "1714772", "name":
      "Dale Schuurmans"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "Early works explored the use of the technique in image classi\ufb01cation [20,
      49, 63] and regression tasks [3].", "intents": ["methodology"]}], "intents":
      ["methodology"], "contexts": ["Early works explored the use of the technique
      in image classi\ufb01cation [20, 49, 63] and regression tasks [3]."], "citedPaper":
      {"paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0", "externalIds": {"DBLP":
      "journals/neco/HintonOT06", "MAG": "2136922672", "DOI": "10.1162/neco.2006.18.7.1527",
      "CorpusId": 2309950, "PubMed": "16764513"}, "corpusId": 2309950, "publicationVenue":
      {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3", "name": "Neural Computation",
      "type": "journal", "alternate_names": ["Neural Comput"], "issn": "0899-7667",
      "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667", "alternate_urls":
      ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226", "http://www.mitpressjournals.org/loi/neco",
      "https://www.mitpressjournals.org/loi/neco"]}, "url": "https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0",
      "title": "A Fast Learning Algorithm for Deep Belief Nets", "abstract": "We show
      how to use complementary priors to eliminate the explaining-away effects that
      make inference difficult in densely connected belief nets that have many hidden
      layers. Using complementary priors, we derive a fast, greedy algorithm that
      can learn deep, directed belief networks one layer at a time, provided the top
      two layers form an undirected associative memory. The fast, greedy algorithm
      is used to initialize a slower learning procedure that fine-tunes the weights
      using a contrastive version of the wake-sleep algorithm. After fine-tuning,
      a network with three hidden layers forms a very good generative model of the
      joint distribution of handwritten digit images and their labels. This generative
      model gives better digit classification than the best discriminative learning
      algorithms. The low-dimensional manifolds on which the digits lie are modeled
      by long ravines in the free-energy landscape of the top-level associative memory,
      and it is easy to explore these ravines by using the directed connections to
      display what the associative memory has in mind.", "venue": "Neural Computation",
      "year": 2006, "referenceCount": 32, "citationCount": 15204, "influentialCitationCount":
      1271, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
      "Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2006-07-01", "journal": {"volume": "18", "pages": "1527-1554", "name": "Neural
      Computation"}, "citationStyles": {"bibtex": "@Article{Hinton2006AFL,\n author
      = {Geoffrey E. Hinton and Simon Osindero and Y. Teh},\n booktitle = {Neural
      Computation},\n journal = {Neural Computation},\n pages = {1527-1554},\n title
      = {A Fast Learning Algorithm for Deep Belief Nets},\n volume = {18},\n year
      = {2006}\n}\n"}, "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"},
      {"authorId": "2217144", "name": "Simon Osindero"}, {"authorId": "1725303", "name":
      "Y. Teh"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context": "These
      parameters are trained using stochastic gradient descent [51].", "intents":
      ["background"]}], "intents": ["background"], "contexts": ["These parameters
      are trained using stochastic gradient descent [51]."], "citedPaper": {"paperId":
      "34ddd8865569c2c32dec9bf7ffc817ff42faaa01", "externalIds": {"MAG": "1994616650",
      "DOI": "10.1214/AOMS/1177729586", "CorpusId": 16945044}, "corpusId": 16945044,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/34ddd8865569c2c32dec9bf7ffc817ff42faaa01",
      "title": "A Stochastic Approximation Method", "abstract": "Let M(x) denote the
      expected value at level x of the response to a certain experiment. M(x) is assumed
      to be a monotone function of x but is unknown tot he experiment, and it is desire
      to find the solution x=0 of the equation M(x) = a, where x is a given constant.
      we give a method for making successive experiments at levels x1, x2,... in such
      a way that x, will tend to 0 in probability.", "venue": "", "year": 1951, "referenceCount":
      0, "citationCount": 8006, "influentialCitationCount": 989, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.pdf",
      "status": null}, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": null, "publicationDate": "1951-09-01",
      "journal": {"volume": "22", "pages": "400-407", "name": "Annals of Mathematical
      Statistics"}, "citationStyles": {"bibtex": "@Article{Robbins1951ASA,\n author
      = {H. Robbins},\n journal = {Annals of Mathematical Statistics},\n pages = {400-407},\n
      title = {A Stochastic Approximation Method},\n volume = {22},\n year = {1951}\n}\n"},
      "authors": [{"authorId": "145648751", "name": "H. Robbins"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "\u2026[64], RTE [4], SciTail [25]
      Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase
      Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuf\ufb02ed at a sentence
      level - destroying long-range structure.", "intents": ["background"]}, {"context":
      "The Corpus of Linguistic Acceptability (CoLA) [65] contains expert judgements
      on whether a sentence is grammatical or not, and tests the innate linguistic
      bias of trained models.", "intents": ["background"]}], "intents": ["background"],
      "contexts": ["\u2026[64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is shuf\ufb02ed at a sentence level - destroying long-range
      structure.", "The Corpus of Linguistic Acceptability (CoLA) [65] contains expert
      judgements on whether a sentence is grammatical or not, and tests the innate
      linguistic bias of trained models."], "citedPaper": {"paperId": null, "externalIds":
      null, "corpusId": null, "publicationVenue": null, "url": null, "title": "Corpus
      of linguistic acceptability", "abstract": null, "venue": "http://nyu-mll.github.io/cola",
      "year": 2018, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "Method Classification Semantic Similarity GLUE CoLA SST2 MRPC STSB QQP (mc)
      (acc) (F1) (pc) (F1) Sparse byte mLSTM [16] - 93.", "intents": ["methodology"]}],
      "intents": ["methodology"], "contexts": ["Method Classification Semantic Similarity
      GLUE CoLA SST2 MRPC STSB QQP (mc) (acc) (F1) (pc) (F1) Sparse byte mLSTM [16]
      - 93."], "citedPaper": {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950",
      "externalIds": {"CorpusId": 52220661}, "corpusId": 52220661, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/a07609c2ed39d049d3e59b61408fb600c6ab0950",
      "title": "GPU Kernels for Block-Sparse Weights", "abstract": "We\u2019re releasing
      highly optimized GPU kernels for an underexplored class of neural network architectures:
      networks with block-sparse weights. The kernels allow for efficient evaluation
      and differentiation of linear layers, including convolutional layers, with flexibly
      configurable block-sparsity patterns in the weight matrix. We find that depending
      on the sparsity, these kernels can run orders of magnitude faster than the best
      available alternatives such as cuBLAS. Using the kernels we improve upon the
      state-of-the-art in text sentiment analysis and generative modeling of text
      and images. By releasing our kernels in the open we aim to spur further advancement
      in model and algorithm design.", "venue": "", "year": 2017, "referenceCount":
      28, "citationCount": 136, "influentialCitationCount": 13, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      null, "journal": null, "citationStyles": {"bibtex": "@Inproceedings{Gray2017GPUKF,\n
      author = {S. Gray and Alec Radford and Diederik P. Kingma},\n title = {GPU Kernels
      for Block-Sparse Weights},\n year = {2017}\n}\n"}, "authors": [{"authorId":
      "145565184", "name": "S. Gray"}, {"authorId": "38909097", "name": "Alec Radford"},
      {"authorId": "1726807", "name": "Diederik P. Kingma"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "We use three datasets for this task
      \u2013 the Microsoft Paraphrase corpus (MRPC) [14] (collected from news sources),
      the Quora Question Pairs (QQP) dataset [9], and the Semantic Textual Similarity
      benchmark (STS-B) [6].", "intents": ["methodology"]}, {"context": "\u2026NLI
      [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is
      shuf\ufb02ed at a sentence level - destroying long-range\u2026", "intents":
      ["background"]}], "intents": ["methodology", "background"], "contexts": ["We
      use three datasets for this task \u2013 the Microsoft Paraphrase corpus (MRPC)
      [14] (collected from news sources), the Quora Question Pairs (QQP) dataset [9],
      and the Semantic Textual Similarity benchmark (STS-B) [6].", "\u2026NLI [64],
      RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is
      shuf\ufb02ed at a sentence level - destroying long-range\u2026"], "citedPaper":
      {"paperId": "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "externalIds": {"CorpusId":
      233225749}, "corpusId": 233225749, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/8ff46c88964a36985f2b45933a3d47b81bd87bd0",
      "title": "Quora Question Pairs", "abstract": "Quora Question Pairs is an active
      Kaggle Competition, which challenges participants to tackle the natural language
      processing (NLP) problem of identifying duplicate questions [1]. The issue of
      duplicate questions stems from the enormous number of visitors on the Quora
      website (a platform for asking questions and connecting with people that contribute
      answers), making it hard to avoid having similar worded questions from different
      users. Effectively detecting duplicate questions not only saves time for seekers
      to find the best answer to their questions, but also reduces the effort of writers
      in terms of answering multiple versions of the same question[1].", "venue":
      "", "year": 2017, "referenceCount": 12, "citationCount": 153, "influentialCitationCount":
      23, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": null, "citationStyles": {"bibtex":
      "@Inproceedings{Chen2017QuoraQP,\n author = {Zihang Chen and Hongbo Zhang and
      Xiaoji Zhang and Leqi Zhao},\n title = {Quora Question Pairs},\n year = {2017}\n}\n"},
      "authors": [{"authorId": "2117098079", "name": "Zihang Chen"}, {"authorId":
      "2108838350", "name": "Hongbo Zhang"}, {"authorId": "2108198028", "name": "Xiaoji
      Zhang"}, {"authorId": "2111486985", "name": "Leqi Zhao"}]}}, {"isInfluential":
      false, "contextsWithIntent": [{"context": "Most deep learning methods require
      substantial amounts of manually labeled data, which restricts their applicability
      in many domains that suffer from a dearth of annotated resources [61].", "intents":
      ["background"]}], "intents": ["background"], "contexts": ["Most deep learning
      methods require substantial amounts of manually labeled data, which restricts
      their applicability in many domains that suffer from a dearth of annotated resources
      [61]."], "citedPaper": {"paperId": null, "externalIds": null, "corpusId": null,
      "publicationVenue": null, "url": null, "title": "Opportunities and challenges
      in working with low-resource languages", "abstract": null, "venue": "CMU", "year":
      2017, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}, {"isInfluential": false, "contextsWithIntent": [{"context":
      "The most compelling evidence for this so far has been the extensive use of
      pre-trained word embeddings [10, 39, 42] to improve performance on a range of
      NLP tasks [8, 11, 26, 45].", "intents": ["background"]}, {"context": "Over the
      last few years, researchers have demonstrated the bene\ufb01ts of using word
      embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve
      performance on a variety of tasks [8, 11, 26, 45].", "intents": ["background"]}],
      "intents": ["background"], "contexts": ["The most compelling evidence for this
      so far has been the extensive use of pre-trained word embeddings [10, 39, 42]
      to improve performance on a range of NLP tasks [8, 11, 26, 45].", "Over the
      last few years, researchers have demonstrated the bene\ufb01ts of using word
      embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve
      performance on a variety of tasks [8, 11, 26, 45]."], "citedPaper": {"paperId":
      "a14045a751f5d8ed387c8630a86a3a2861b90643", "externalIds": {"ACL": "D14-1082",
      "MAG": "2250861254", "DBLP": "conf/emnlp/ChenM14", "DOI": "10.3115/v1/D14-1082",
      "CorpusId": 11616343}, "corpusId": 11616343, "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253",
      "name": "Conference on Empirical Methods in Natural Language Processing", "type":
      "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical
      Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process",
      "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "url": "https://www.semanticscholar.org/paper/a14045a751f5d8ed387c8630a86a3a2861b90643",
      "title": "A Fast and Accurate Dependency Parser using Neural Networks", "abstract":
      "Almost all current dependency parsers classify based on millions of sparse
      indicator features. Not only do these features generalize poorly, but the cost
      of feature computation restricts parsing speed significantly. In this work,
      we propose a novel way of learning a neural network classifier for use in a
      greedy, transition-based dependency parser. Because this classifier learns and
      uses just a small number of dense features, it can work very fast, while achieving
      an about 2% improvement in unlabeled and labeled attachment scores on both English
      and Chinese datasets. Concretely, our parser is able to parse more than 1000
      sentences per second at 92.2% unlabeled attachment score on the English Penn
      Treebank.", "venue": "Conference on Empirical Methods in Natural Language Processing",
      "year": 2014, "referenceCount": 30, "citationCount": 1776, "influentialCitationCount":
      172, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages":
      "740-750"}, "citationStyles": {"bibtex": "@Article{Chen2014AFA,\n author = {Danqi
      Chen and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods
      in Natural Language Processing},\n pages = {740-750},\n title = {A Fast and
      Accurate Dependency Parser using Neural Networks},\n year = {2014}\n}\n"}, "authors":
      [{"authorId": "50536468", "name": "Danqi Chen"}, {"authorId": "144783904", "name":
      "Christopher D. Manning"}]}}, {"isInfluential": true, "contextsWithIntent":
      [{"context": "Task Datasets Natural language inference SNLI [5], MultiNLI [66],
      Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story
      Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs
      [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA
      [65]", "intents": ["methodology"]}, {"context": "Natural language inference
      SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering
      RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14],
      Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment
      Treebank-2 [54], CoLA [65] but is shuf\ufb02ed at a sentence level - destroying
      long-range structure.", "intents": []}, {"context": "We evaluate on \ufb01ve
      datasets with diverse sources, including image captions (SNLI), transcribed
      speech, popular \ufb01ction, and government reports (MNLI), Wikipedia articles
      (QNLI), science exams (SciTail) or news articles (RTE).", "intents": []}, {"context":
      "On RTE, one of the smaller datasets we evaluate on (2490 examples), we achieve
      an accuracy of 56%, which is below the 61.7% reported by a multi-task biLSTM
      model.", "intents": []}], "intents": ["methodology"], "contexts": ["Task Datasets
      Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4],
      SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity
      MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification
      Stanford Sentiment Treebank-2 [54], CoLA [65]", "Natural language inference
      SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering
      RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14],
      Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment
      Treebank-2 [54], CoLA [65] but is shuf\ufb02ed at a sentence level - destroying
      long-range structure.", "We evaluate on \ufb01ve datasets with diverse sources,
      including image captions (SNLI), transcribed speech, popular \ufb01ction, and
      government reports (MNLI), Wikipedia articles (QNLI), science exams (SciTail)
      or news articles (RTE).", "On RTE, one of the smaller datasets we evaluate on
      (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported
      by a multi-task biLSTM model."], "citedPaper": {"paperId": "db8885a0037fe47d973ade79d696586453710233",
      "externalIds": {"DBLP": "conf/tac/BentivogliCDG10", "CorpusId": 858065}, "corpusId":
      858065, "publicationVenue": {"id": "4e9e07f8-6008-4ffc-b3c0-92c327e36463", "name":
      "Text Analysis Conference", "type": "conference", "alternate_names": ["TAC",
      "Text Anal Conf"], "url": "https://tac.nist.gov/"}, "url": "https://www.semanticscholar.org/paper/db8885a0037fe47d973ade79d696586453710233",
      "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge", "abstract":
      "This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge.
      This year a major innovation was introduced, as the traditional Main Task was
      replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual
      Entailment is performed on a real corpus in the Update Summarization scenario.
      A subtask was also proposed, aimed at detecting novel information. To continue
      the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was
      set up, in which RTE systems had to validate the output of systems participating
      in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48
      submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs).
      As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory
      effort started in RTE-5 to perform resource evaluation through ablation tests
      was not only reiterated in RTE-6, but also extended to tools.", "venue": "Text
      Analysis Conference", "year": 2009, "referenceCount": 16, "citationCount": 646,
      "influentialCitationCount": 127, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": null, "citationStyles": {"bibtex": "@Article{Bentivogli2009TheSP,\n
      author = {L. Bentivogli and Peter Clark and Ido Dagan and Danilo Giampiccolo},\n
      booktitle = {Text Analysis Conference},\n title = {The Sixth PASCAL Recognizing
      Textual Entailment Challenge},\n year = {2009}\n}\n"}, "authors": [{"authorId":
      "2486762", "name": "L. Bentivogli"}, {"authorId": "48323507", "name": "Peter
      Clark"}, {"authorId": "7465342", "name": "Ido Dagan"}, {"authorId": "2380885",
      "name": "Danilo Giampiccolo"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "Natural language inference SNLI [5], MultiNLI [66], Question NLI
      [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence
      similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark
      [6] Classi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026",
      "intents": ["methodology"]}], "intents": ["methodology"], "contexts": ["Natural
      language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail
      [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR
      Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is\u2026"], "citedPaper":
      {"paperId": "351ec42df2b60c6042addf96e6b98673bbaf4dfd", "externalIds": {"DBLP":
      "conf/tac/GiampiccoloDMDC08", "CorpusId": 12381965}, "corpusId": 12381965, "publicationVenue":
      {"id": "4e9e07f8-6008-4ffc-b3c0-92c327e36463", "name": "Text Analysis Conference",
      "type": "conference", "alternate_names": ["TAC", "Text Anal Conf"], "url": "https://tac.nist.gov/"},
      "url": "https://www.semanticscholar.org/paper/351ec42df2b60c6042addf96e6b98673bbaf4dfd",
      "title": "The Fourth PASCAL Recognizing Textual Entailment Challenge", "abstract":
      "In 2008 the Recognizing Textual Entailment Challenge (RTE-4) was proposed for
      the first time as a track at the Text Analysis Conference (TAC). Another important
      innovation introduced in this campaign was a three-judgment task, which required
      the systems to make a further distinction between pairs where the entailment
      does not hold because the content of H is contradicted by the content of T,
      and pairs where the entailment cannot be determined because the truth of H cannot
      be verified on the basis of the content of T. A classic twoway task was also
      offered. RTE-4 attracted 26 teams, more than half of whom submitted runs for
      the new 3-way task. This paper describes the preparation of the dataset, and
      gives an overview of the results achieved by the participating systems.", "venue":
      "Text Analysis Conference", "year": 2008, "referenceCount": 11, "citationCount":
      386, "influentialCitationCount": 29, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": null, "journal": null, "citationStyles": {"bibtex": "@Article{Giampiccolo2008TheFP,\n
      author = {Danilo Giampiccolo and H. Dang and B. Magnini and Ido Dagan and Elena
      Cabrio and W. Dolan},\n booktitle = {Text Analysis Conference},\n title = {The
      Fourth PASCAL Recognizing Textual Entailment Challenge},\n year = {2008}\n}\n"},
      "authors": [{"authorId": "2380885", "name": "Danilo Giampiccolo"}, {"authorId":
      "2122755", "name": "H. Dang"}, {"authorId": "1712352", "name": "B. Magnini"},
      {"authorId": "7465342", "name": "Ido Dagan"}, {"authorId": "1772891", "name":
      "Elena Cabrio"}, {"authorId": "83415753", "name": "W. Dolan"}]}}, {"isInfluential":
      false, "contextsWithIntent": [], "intents": [], "contexts": [], "citedPaper":
      {"paperId": "8eefd28eb47e72794bb0355d8abcbebaac9d8ab1", "externalIds": {"MAG":
      "2486125749", "DBLP": "books/mit/06/NigamMM06", "DOI": "10.7551/mitpress/9780262033589.003.0003",
      "CorpusId": 256574}, "corpusId": 256574, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/8eefd28eb47e72794bb0355d8abcbebaac9d8ab1",
      "title": "Semi-Supervised Text Classification Using EM", "abstract": "For several
      decades, statisticians have advocated using a combination of labeled and unlabeled
      data to train classifiers by estimating parameters of a generative model through
      iterative Expectation-Maximization (EM) techniques. This chapter explores the
      effectiveness of this approach when applied to the domain of text classification.
      Text documents are represented here with a bag-of-words model, which leads to
      a generative classification model based on a mixture of multinomials. This model
      is an extremely simplistic representation of the complexities of written text.
      This chapter explains and illustrates three key points about semi-supervised
      learning for text classification with generative models. First, despite the
      simplistic representation, some text domains have a high positive correlation
      between generative model probability and classification accuracy. In these domains,
      a straightforward application of EM with the naive Bayes text model works well.
      Second, some text domains do not have this correlation. Here we can adopt a
      more expressive and appropriate generative model that does have a positive correlation.
      In these domains, semi-supervised learning again improves classification accuracy.
      Finally, EM suffers from the problem of local maxima, especially in high dimension
      domains such as text classification. We demonstrate that deterministic annealing,
      a variant of EM, can help overcome the problem of local maxima and increase
      classification accuracy further when the generative model is appropriate.",
      "venue": "Semi-Supervised Learning", "year": 2006, "referenceCount": 11, "citationCount":
      160, "influentialCitationCount": 11, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": null, "journal": {"pages": "32-55"},
      "citationStyles": {"bibtex": "@Inproceedings{Nigam2006SemiSupervisedTC,\n author
      = {K. Nigam and A. McCallum and Tom Michael Mitchell},\n booktitle = {Semi-Supervised
      Learning},\n pages = {32-55},\n title = {Semi-Supervised Text Classification
      Using EM},\n year = {2006}\n}\n"}, "authors": [{"authorId": "145172877", "name":
      "K. Nigam"}, {"authorId": "143753639", "name": "A. McCallum"}, {"authorId":
      "40975594", "name": "Tom Michael Mitchell"}]}}, {"isInfluential": false, "contextsWithIntent":
      [{"context": "This paradigm has attracted signi\ufb01cant interest, with applications
      to tasks like sequence labeling [24, 33, 57] or text classi\ufb01cation [41,
      70].", "intents": ["methodology"]}], "intents": ["methodology"], "contexts":
      ["This paradigm has attracted signi\ufb01cant interest, with applications to
      tasks like sequence labeling [24, 33, 57] or text classi\ufb01cation [41, 70]."],
      "citedPaper": {"paperId": "66b6b417184a19303a63029abe655dee3bdd4f74", "externalIds":
      {"CorpusId": 264806472}, "corpusId": 264806472, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/66b6b417184a19303a63029abe655dee3bdd4f74",
      "title": "--1 CONTENTS", "abstract": "Beech (Fagus sylvatica L.) is one of the
      economically most important tree species in Europe. It is often managed under
      silvicultural systems which include natural regeneration resulting in dense
      young stands. Many foresters consider pre-commercial thinning indispensable
      for the production of high quality timber and even for the maintenance of biodiversity
      and optimal opportunities for recreational activities. This study comprises
      four contrasting management regimes for young beech, initiated at an age of
      14 years: 1. no pre-commercial thinning, 2. stripwise thinning leaving alternate
      1-11\u20442 metre wide belts of beech, 21\u20442. stripwise thinning followed
      by heavy selective thinning, and 3. perpendicular stripwise thinning leaving
      a ''chessboard'' pattern of beech for subsequent selective thinning. In addition
      to these main treatments, the study includes effects of strip width, manual
      pruning, and stump height on tree and stand development. Until an age of 21
      years the height growth of potential crop trees was essentially unaffected by
      thinning practice, diameter growth was unaffected by treatment 2 but strongly
      promoted by treatment 21\u20442 and 3, and natural pruning was hampered by increasing
      thinning grade. The exterior wood quality of potential crop trees (excluding
      natural pruning) improved with increasing thinning grade, except for the less
      optimal treatment 3. For ground flora, cover as well as species diversity increased
      with increasing thinning grade at this stage of stand development. Additionally,
      a vigorous understorey of beech and other tree species developed with increasing
      thinning grade due to stump regrowth and additional natural regeneration. In
      conclusion, the study clearly indicates that the common practice of stripwise
      precommercial thinning is unjustified. The justification of heavy ''chessboard''
      thinning (with pruning) depends on whether the potential reduction in rotation
      length and the improvement in wood quality outweigh the discounted costs of
      pre-commercial thinning and selection and pruning of crop trees. Combined stripwise
      and early heavy thinning ranges between these extremes. However, biodiversity,
      forest recreation and other issues may override these conclusions.", "venue":
      "", "year": 2006, "referenceCount": 59, "citationCount": 643, "influentialCitationCount":
      42, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": null, "citationStyles": {"bibtex":
      "@Inproceedings{Bolte20061C,\n author = {A. Bolte and Asadollah Mataji and Ghavomoddin
      Zahedi Amiri},\n title = {--1 CONTENTS},\n year = {2006}\n}\n"}, "authors":
      [{"authorId": "93756545", "name": "A. Bolte"}, {"authorId": "2265851818", "name":
      "Asadollah Mataji"}, {"authorId": "2265855374", "name": "Ghavomoddin Zahedi
      Amiri"}]}}, {"isInfluential": false, "contextsWithIntent": [{"context": "\u2026MultiNLI
      [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30],
      Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question
      Pairs [9], STS Benchmark [6] Classi\ufb01cation Stanford Sentiment Treebank-2
      [54], CoLA [65] but is shuf\ufb02ed at a sentence level -\u2026", "intents":
      ["background"]}, {"context": "We use three datasets for this task \u2013 the
      Microsoft Paraphrase corpus (MRPC) [14] (collected from news sources), the Quora
      Question Pairs (QQP) dataset [9], and the Semantic Textual Similarity benchmark
      (STS-B) [6].", "intents": ["methodology"]}], "intents": ["methodology", "background"],
      "contexts": ["\u2026MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]
      Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase
      Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classi\ufb01cation
      Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuf\ufb02ed at a sentence
      level -\u2026", "We use three datasets for this task \u2013 the Microsoft Paraphrase
      corpus (MRPC) [14] (collected from news sources), the Quora Question Pairs (QQP)
      dataset [9], and the Semantic Textual Similarity benchmark (STS-B) [6]."], "citedPaper":
      {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "externalIds": {"DBLP":
      "conf/acl-iwp/DolanB05", "MAG": "131533222", "ACL": "I05-5002", "CorpusId":
      16639476}, "corpusId": 16639476, "publicationVenue": {"id": "e783305c-5d8a-44b9-b7a2-449d474a85b2",
      "name": "International Joint Conference on Natural Language Processing", "type":
      "conference", "alternate_names": ["IJCNLP", "Int Jt Conf Nat Lang Process"],
      "url": "https://www.aclweb.org/portal/ijcnlp"}, "url": "https://www.semanticscholar.org/paper/475354f10798f110d34792b6d88f31d6d5cb099e",
      "title": "Automatically Constructing a Corpus of Sentential Paraphrases", "abstract":
      "An obstacle to research in automatic paraphrase identification and generation
      is the lack of large-scale, publiclyavailable labeled corpora of sentential
      paraphrases. This paper describes the creation of the recently-released Microsoft
      Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled
      with a binary judgment as to whether the pair constitutes a paraphrase. The
      corpus was created using heuristic extraction techniques in conjunction with
      an SVM-based classifier to select likely sentence-level paraphrases from a large
      corpus of topicclustered news data. These pairs were then submitted to human
      judges, who confirmed that 67% were in fact semantically equivalent. In addition
      to describing the corpus itself, we explore a number of issues that arose in
      defining guidelines for the human raters.", "venue": "International Joint Conference
      on Natural Language Processing", "year": 2005, "referenceCount": 50, "citationCount":
      1292, "influentialCitationCount": 223, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": null, "journal": {"volume": "", "name": ""}, "citationStyles":
      {"bibtex": "@Article{Dolan2005AutomaticallyCA,\n author = {W. Dolan and Chris
      Brockett},\n booktitle = {International Joint Conference on Natural Language
      Processing},\n title = {Automatically Constructing a Corpus of Sentential Paraphrases},\n
      year = {2005}\n}\n"}, "authors": [{"authorId": "83415753", "name": "W. Dolan"},
      {"authorId": "3125776", "name": "Chris Brockett"}]}}, {"isInfluential": false,
      "contextsWithIntent": [{"context": "This paradigm has attracted signi\ufb01cant
      interest, with applications to tasks like sequence labeling [24, 33, 57] or
      text classi\ufb01cation [41, 70].", "intents": ["methodology"]}, {"context":
      "The earliest approaches used unlabeled data to compute word-level or phrase-level
      statistics, which were then used as features in a supervised model [33].", "intents":
      ["methodology"]}], "intents": ["methodology"], "contexts": ["This paradigm has
      attracted signi\ufb01cant interest, with applications to tasks like sequence
      labeling [24, 33, 57] or text classi\ufb01cation [41, 70].", "The earliest approaches
      used unlabeled data to compute word-level or phrase-level statistics, which
      were then used as features in a supervised model [33]."], "citedPaper": {"paperId":
      "31b4c03d721dc10b87c178277c1d369f91db8f0e", "externalIds": {"MAG": "2158049734",
      "CorpusId": 14740218}, "corpusId": 14740218, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/31b4c03d721dc10b87c178277c1d369f91db8f0e",
      "title": "Semi-Supervised Learning for Natural Language", "abstract": "Statistical
      supervised learning techniques have been successful for many natural language
      processing tasks, but they require labeled datasets, which can be expensive
      to obtain. On the other hand, unlabeled data (raw text) is often available \"for
      free\" in large quantities. Unlabeled data has shown promise in improving the
      performance of a number of tasks, e.g. word sense disambiguation, information
      extraction, and natural language parsing. In this thesis, we focus on two segmentation
      tasks, named-entity recognition and Chinese word segmentation. The goal of named-entity
      recognition is to detect and classify names of people, organizations, and locations
      in a sentence. The goal of Chinese word segmentation is to find the word boundaries
      in a sentence that has been written as a string of characters without spaces.
      Our approach is as follows: In a preprocessing step, we use raw text to cluster
      words and calculate mutual information statistics. The output of this step is
      then used as features in a supervised model, specifically a global linear model
      trained using the Perceptron algorithm. We also compare Markov and semi-Markov
      models on the two segmentation tasks. Our results show that features derived
      from unlabeled data substantially improves performance, both in terms of reducing
      the amount of labeled data needed to achieve a certain performance level and
      in terms of reducing the error using a fixed amount of labeled data. We find
      that sometimes semi-Markov models can also improve performance over Markov models.
      Thesis Supervisor: Michael Collins Title: Assistant Professor, CSAIL", "venue":
      "", "year": 2005, "referenceCount": 80, "citationCount": 366, "influentialCitationCount":
      34, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics",
      "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
      "journal": {"volume": "", "name": ""}, "citationStyles": {"bibtex": "@Inproceedings{Liang2005SemiSupervisedLF,\n
      author = {P. Liang},\n title = {Semi-Supervised Learning for Natural Language},\n
      year = {2005}\n}\n"}, "authors": [{"authorId": "2075292388", "name": "P. Liang"}]}},
      {"isInfluential": false, "contextsWithIntent": [{"context": "This corpus has
      been shown to contain more reasoning type questions that other datasets like
      CNN [19] or SQuaD [47], providing the perfect evaluation for our model which
      is trained to handle long-range contexts.", "intents": ["background"]}], "intents":
      ["background"], "contexts": ["This corpus has been shown to contain more reasoning
      type questions that other datasets like CNN [19] or SQuaD [47], providing the
      perfect evaluation for our model which is trained to handle long-range contexts."],
      "citedPaper": {"paperId": null, "externalIds": null, "corpusId": "236503748",
      "publicationVenue": null, "url": null, "title": "and P", "abstract": null, "venue":
      "", "year": null, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "citationStyles":
      null, "authors": []}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '235165'
      Content-Type:
      - application/json
      Date:
      - Wed, 27 Dec 2023 21:55:42 GMT
      Via:
      - 1.1 a5313514fbb2ef971d07fa7af0982586.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - JqEipgvwufnBtw0q-o_Gn66cxk9ApAVcMj8iKp_na4XRm8Vy1oXF1A==
      X-Amz-Cf-Pop:
      - GRU3-P4
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - Qn2GqE3QvHcEJsQ=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '235165'
      x-amzn-Remapped-Date:
      - Wed, 27 Dec 2023 21:55:42 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - e6a73fe5-aa14-4b1d-ab8b-dcc89b174dcc
    http_version: HTTP/1.1
    status_code: 200
version: 1
